[
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html",
    "href": "posts/2020-12-09-employee_skills_data/index.html",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "",
    "text": "Lachlan, a people analyst at a global company, sits down with the organisation’s Chief Strategy Officer (CSO). The CSO quickly gets to the point, stating: “Lachlan, I need your help finding a specialist to head up a new business. The opportunity is ripe—so we need to get started as soon as possible. By the end of the week, I need you to have identified a leader with the following skill set: experience in front- and back-end development, working knowledge of Amazon Web Services (AWS), knowledge of the South East Asian IT market, and fluent in Mandarin. We’ve got roughly 60,000 global employees, so surely there must be someone who fits the bill—right?”.\nLachlan pauses for a second before responding: “I honestly don’t know. I can certainly reach out to some contacts and look at the different datasets that we have internally, but I’m not 100% certain that I can find someone with the precise skill set that you’re looking for”.\nThe CSO frowns and purses her lips. “Okay, well how about externally? Can you find me someone from outside the company?”. Upon hearing this, Lachlan immediately looks relieved, replying: “Definitely—I can give you a detailed shortlist of specialists by close-of-business tomorrow. Would that be okay?”. The CSO smiles and nods, before adding: “How is it though that you can’t find me someone from our global internal population of 60,000, but you can provide me a list of people in 24 hours among a global population of approximately 8 billion people?”\nLachlan pauses, considering the problem at hand. After a few seconds, he responds simply: “Well, it’s all down to the quality of data at my disposal”.\nThis anecdote illustrates a common problem among companies of all sizes—most organisations are unaware of the skills residing within their employee population. The notion of employee skill data has garnered considerable interest of late, with interested parties ranging all the way from:\nSo what unites these disparate parties? Well, it’s ultimately the realisation that old skills management practices—specifically, an inability to catalogue the “gold” within the organisation—is untenable.\nThis article explores the two fundamental considerations when it comes to skills management. First, how do you acquire your employees’ relevant skill data? Second, what practical value can you then generate from this wealth of information?"
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html#how-to-acquire-employees-skill-data",
    "href": "posts/2020-12-09-employee_skills_data/index.html#how-to-acquire-employees-skill-data",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "How to Acquire Employees’ Skill Data",
    "text": "How to Acquire Employees’ Skill Data\nThe introductory anecdote highlights one key issue: an employee skills catalogue entirely depends upon quality data. So how about asking individual employees to complete and maintain their own employee skills profile (i.e. a company-specific profile similar to LinkedIn)?\nFor starters, this is unsustainable. Employees might forget to update their profile, feel like they’re too busy, or simply not see the benefits of this activity.\nAs a result, organisations must:\n\nMinimise the amount of effort needed from each individual employee.\nFully exploit existing data as intelligently as possible.\nGenerate clear-cut benefits for both individuals and wider organisations alike.\n\nAnd it appears that advancements in AI—specifically Natural Language Processing (NLP)—might hold the key. NLP has already been used widely within the HR industry, especially in the field of Talent Acquisition. This technology, when used correctly, connects and distils numerous data sources to make previously unknown connections.\nFor example, the following data sources—while often scattered across the HR / IT landscape and laying dormant—already contain a wealth of valuable information about individual employees.\n\nRecruitment data: Resumes, cover letters, assessments, etc.\nCareer history data: Roles previously held by individuals within an organisation, role titles, job descriptions, project descriptions, leadership roles, awards won, etc.\nLearning & Development data: Courses undertaken and completed, development objectives, mentoring, certifications, 360-degree feedback, psychometric assessments, etc.\nPerformance data: Performance objectives, performance appraisals, written feedback by peers and stakeholders, etc.\nExternal data: Professional networking sites, professional publications, patents, blogs, etc.\n\nAll of these sources (well, all aside from external data) have long been standard data collection mechanisms within the HR industry. If finance is considered the home of numbers-based insights, then HR is undoubtedly the home of text-based insights. Organisations can therefore automate the engineering of skill data by combining:\nThe various forms of employee data currently held in disparate organisational HR/IT systems.\nNext-generation NLP technologies that can perform realistic interpretations of text-based data.\n\n\n\n\n\nVisual representation of the process of automating 1. Ingestion of employee data currently in organisations, and 2. Interpretation of content via NLP to develop realistic skill profiles.\n\n\n\n\nThe above image visually demonstrates this process. By analysing and applying relevant skill data, organisations can achieve evidence-based decision-making when it comes to personnel matters. Needless to say, this brings considerable value to both individual employees and the broader organisation."
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html#the-practical-value-you-can-generate-with-skill-data-across-the-employee-lifecycle",
    "href": "posts/2020-12-09-employee_skills_data/index.html#the-practical-value-you-can-generate-with-skill-data-across-the-employee-lifecycle",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "The practical value you can generate with skill data across the employee lifecycle",
    "text": "The practical value you can generate with skill data across the employee lifecycle\nIt’s all well and good acquiring relevant skill data, but how can it be put to good use? If used correctly, skill data can guide a wide variety of employee touchpoints within an organisation. For instance, the very process of identifying the right person for a specific role (as per the introductory example) is a foundational application of employee skill data.\n\n\nFigure: Overview of where skills data can be used for both employee and organisation\nThe following diagram gives an overview of the areas where skills data can be put to good use for the advantage of the organisation and the advantage of the employee.\n\n\n\n\n\nOverview of the areas where skills data can be put to good use for the advantage of both the employee and organisation.\n\n\n\n\n\n\nTable: The benefits of skills data for both employee and organisation\nThe following table lists several other potential applications, both from the perspective of the employee and the organisation.\n\n\n\n\n\n\n\n\nEmployee Lifecycle Stage\nEmployee Value\nOrganisation Value\n\n\n\n\nRecruitment\nObjective recruiting decisions that provide personalised feedback: skills are inferred from application data sources (e.g. CV, LinkedIn, etc.) and skill-based matches are provided to the recruiter as an additional lens. As a result, objective and tailored feedback can be provided to unsuccessful candidates (i.e., no more generic “unsuccessful” emails from organisations), which can help them take future action, e.g. upskilling or tailoring their job search. For successful candidates, the inferred skills captured can be used to personalise the onboarding process.\nObjective skill-based recruiting decisions: skills can be inferred from a previous role incumbent and/or data sources that describe the role (e.g., job description, performance objectives, etc.). The vacancy is then more accurately described in terms of objectives and required skills. This provides greater accuracy for candidates and the organisation, thereby improving recruitment outcomes, saving time and money. Furthermore, the influx of skills into the organisation can be captured and subsequently analysed for trends and insights. Are critical skills being recruited at an adequate pace? Which skills are difficult to source and should be built?\n\n\nOnboarding\nExpediting employee integration: new employees are immediately included in the employee skill catalogue. This enables the immediate personalisation of learning recommendations, mentoring opportunities, and inclusion in organisational projects. Moreover, because expertise is immediately captured, employees can play an immediate role as SME’s where applicable. This enables value contribution and subsequently engagement and satisfaction, which may otherwise take months to establish.\nExpediting employee productivity: including employee skill details in the organisational catalogue expedites the speed with which new employees can be included in work deliverables, improving employee time to productivity. In addition, skill gaps between the vacancy and candidate are assessed automatically, thereby enabling faster closure of skill gaps through training and mentor recommendations.\n\n\nDevelopment\nPersonalised employability assessment and development recommendations: skills data helps to compare each employee to both the internal and external environment to determine their employability. Employees can undertand how desirable their skillset is, which in turn becomes a great motivator for learning. Organisations are better equipped to provide personalised learning recommendations to employees, based on their current skills and market skill trends. Finally, by tying learning recommendations to opportunities within the company (also known as talent markeplace), employees can explore new avenues for career growth.\nOptimising training offerings: Organisations can make more objective and intelligent decisions regarding their Learning System investments, by answering questions such as: How closely does a system mirror the skills an organisation currently has, and will need in future? Does a system mirror skill trends in the different geographic markets, or just one market? How are new training offerings prioritised by the provider and will this mirror future skill requirements of the organisation? Secondly, organisations can make more effective Strategic Workforce Planning decisions regarding which skills are “built” vs “bought”. Thirdly, organisations can compare employee skills to external market trends. This transparency, when shared with the workforce, can help to facilitate a culture of learning.\n\n\nTalent Identification\nObjective visibility and identification of talent (i.e., non-political) that enables better employee experiences: skill based identification surfaces better employee opportunities for awards and recognition, funding opportunities, project sponsorship, project involvement, skill and career growth, and learning programs (e.g., high performer investment, university course sponsorship, etc.).\nOptimising staffing decisions: rapid identification of resident skills facilitates more effective project staffing, team configuration and ultimately organisational design. An employee skills catalogue enables decision making at scale. Better staffing decisions can directly impact innovation and productivity outcomes. The employee skills catalogue enables greater ease and precision of talent (e.g., who has skills of greatest organisational value), internal job mobility and succession.\n\n\nCareer Growth\nGreater career transition opportunities: A skills-based talent marketplace enables employees to determine how transferable their skills are to other roles in their local or global organization. Which roles are currently open that an employee could transition into? Which roles are realistic career growth opportunities?\nLeveraging internal talent to maximum benefit: many organisations find it easier to identify and source talent externally than internally. By creating a skills-based talent marketplace, organisations can leverage the employee skills catalogue to identify and source talent with corporate knowledge, expediting onboarding. Moreover, enabling employees to easily identify opportunities internally may facilitate longer careers within organisations, thereby reducing turnover and the costs associated with recruitment, onboarding, etc.\n\n\nManaging Change\nIncreasingly objective, and transparent decisions with personalised feedback. Skill based decisions provide all employees with visibility during change, not just those who are well-networked. Decisions can be made in a way that systematically consider current state, SWP, employee skill utility, and thereby provide individual feedback in a transparent and valuable fashion.\nObjective and intelligent management of skill capital during change. A skills catalogue may inform the due diligence process (i.e., what are we acquiring), retention (i.e., which skill sets represent a unique value-add), and the post-sale integration process (i.e., configuring new organisations or project teams). Organisations can systematically determine the skills each employee provides the workplace as well as understanding, during times of restructure, which skills are being lost or gained during various change scenarios. This affords organisations an opportunity to remove some of the uncertainty associated with change.\n\n\nStrategic Workforce Planning\n\nConnecting the future skill state with the current skill state. It is impossible to effectively work toward a future state skill target, without knowing which skills are currently resident within the organisation. Such understanding will inform build or buy skill decisions and learning investments.\n\n\nOperational Efficiency / Knowledge Management\nFinding help (i.e., skills, corporate knowledge, resources, etc.) through an internal talent marketplace. Who has the skills related to my project(s) who may not be within my current network of contacts? Has something similar been done before? Having a transparent view of the skills across the workforce means that employees can connect with others who have expertise more effectively thus proactively catalyzing innovation and productivity. Employees no longer have to wait to “hear” about new communities of practice in an information-overload environment\nAn indirect approach to knowledge management and talent management can boost inclusivity across the workforce as more informal and formal network connections are facilitated. In turn, the impact of a more transparent, inclusive approach to the workforce could have significant impacts on developing the right culture at your organisation.\n\n\nOffboarding\nA transferable skills record that promotes employability: Leaving an organisation with a systematic, objective and comprehensive assessment of skills enables individual employability. Moreover, this benefit would ideally engender goodwill between ex-employee and organisation, encouraging advocacy and loyalty, and provide a skill passport that could readily enable future re-employment.\nIdentifying skills gaps before they are felt: The outflow of skills in the organisation can be captured and subsequently analysed for trends and insights. Are any critical skills leaving the company? How are departing skills being addressed by Talent Acquisition, Career Mobility and L&D activities?\n\n\n\nThe above list, while not exhaustive, illustrates the various ways in which skill data can underpin many major employee touch points within an organisation. The greatest benefit of skill based personnel decision making is that it provides an objective, comprehensive, and close to real time perspective of the organisation - an outcome previously beyond the reach of organisations."
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html#why-skills-could-be-the-new-business-currency",
    "href": "posts/2020-12-09-employee_skills_data/index.html#why-skills-could-be-the-new-business-currency",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "Why skills could be the new business currency",
    "text": "Why skills could be the new business currency\nSkills management has been around since the eighties. However, it has historically been complicated, highly resource-intensive, and, even worse, yielded questionable tangible value to the organisation as a whole.\nBut that’s all about to change. Technological advancements mean that organisations can identify and manage their employees’ skill sets more easily than ever before. The above discussion detailing prospective applications of skill data demonstrates the breadth of utility, and potential for objective enhancement of personnel decision making across the employee lifecycle. Moreover, skill data provides a globally universal currency for organisational decision making, thereby enabling personalised employee decision making at scale that transcends geographic boundaries.\nBy maintaining a continuous overview of the capabilities that lie within their organisation, they’ll be able to meet the future head-on—no matter what challenges they face."
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html#acknowledgments",
    "href": "posts/2020-12-09-employee_skills_data/index.html#acknowledgments",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the myHRfuture website under the title “The Value of Measuring Employee Skill Data” on November 25th, 2020."
  },
  {
    "objectID": "posts/2021-02-05-capturing_the_sun/index.html",
    "href": "posts/2021-02-05-capturing_the_sun/index.html",
    "title": "Capturing the Sun!",
    "section": "",
    "text": "Photo by Jude Beck on Unsplash.\n\n\n\n\n\n\nIntroduction\nCreative visualisations that help synthesize complex relationships into rapid understanding are priceless. As a consequence, I am always looking for new ways to convey meaning from data in accessible ways. To that end I thought I would have a play with Sunburst visualisations in R, using the sunburstR package. Below is a very quick walkthrough of my experience with this visualisation type.\nI’ve loaded five packages for this experiment. These packages are:\n\n\n\nPackage\nRequirement\n\n\n\n\nreadxl\nimporting the data\n\n\ndplyr\ndata wrangling\n\n\nd3r\nformatting the data for visualising as a sunburst\n\n\nRColorBrewer\npalette (i.e., colour) selection in the visualisation\n\n\nsunburstR\ncreating the interactive sunburst visualisation\n\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(d3r)\nlibrary(RColorBrewer)\nlibrary(sunburstR)\n\n\n\nLet’s begin…\n\n\n1. Read\nYou’ll note that I changed some of the response formats in the PerformanceRating and Department variables. This was to make the values shorter, so that they appeared better in the final visualisation. I added this step retrospectively, as I was finding that the text in the legend of the sunburst visualisation was too big and could not be read properly, despite altering the text size (more on this later). I felt this step was necessary for the visual appeal and understanding, and didn’t detract from the final outcome.\n\n\n\nCode\n# import the dataset\noriginal_tbl <- readxl::read_excel(path = \"datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.xlsx\")\n\n# do some basic formatting changes\noriginal_tbl <- original_tbl %>% \n  dplyr::mutate(PerformanceRating = dplyr::case_when(PerformanceRating == 3 ~ \"Achieving\",\n                                                     PerformanceRating == 4 ~ \"Excelling\",\n                                                     TRUE ~ \"Not Rated\"),\n                Department = dplyr::case_when(Department == \"Human Resources\" ~ \"HR\",\n                                              Department == \"Research & Development\" ~ \"R&D\",\n                                              TRUE ~ Department)\n                )\n\n\n\n\n\n2. Format\nTwo steps are present in the formatting of the data.\nThe first is to get a count of the variables. The two successive rings in my sunburst diagram, starting from the inside and heading outwards, were intended to be Department and then Performance Rating. Consequently, I grouped the data in this order and then added a count of the grouped variables.\nThe second step is to then get the data into a format suitable for visualising in a sunburst diagram. This can be achieved using the d3r package, specifically the d3_nest function. You simply pass the function the tibble previously created, which has my grouping variables in successive order, followed by the count. When calling the d3_nest function you need to specify which column has the values (i.e., count), the function does the rest. With the data formatted we are now ready to visualise.\n\n\n\nCode\n# format the tibble for visualising - Step 1\nshorter_data_tbl <- original_tbl %>% \n  dplyr::mutate(PerformanceRating = as.character(PerformanceRating)) %>% \n  #dplyr::group_by(Department, EducationField, PerformanceRating) %>% \n  dplyr::group_by(Department, PerformanceRating) %>% \n  dplyr::count() %>% \n  dplyr::ungroup()\n\n\n# format the shorter_data_tbl (Step 1) for visualising - Step 2\nsunburst_tree <- d3r::d3_nest(shorter_data_tbl, value_cols = \"n\")\n\n\n\n\n\n3. Visualise\nI started by fishing around for a good colour palette to use on the visualisation. This was an interative process that involved some trial and error. In the end I settled upon the Set1 palette from the RColorBrewer package. The next step was to creae the sunburst visualisation using the sunburstR package. The function is very straightforward and the arguements are pretty self-explanatory. I now had a nice, interactive sunburst diagram!\nI did notice on the first round through this process that I wanted to add a heading and change the format of text on the legend, as some of the department names were too long (e.g., “Research and Development”). Unfortunately, sunburstR doesn’t provide this functionality. However, upon exploring stackoverflow and GitHub it seemed that others had the same request and a workaround using the htmlwidgets and htmltools packages had been identified. As you can see from my code, I tinkered with some suggested work-arounds to create my own. Through a little subsequent trial and error I was able to decide upon some formatting that suited my taste.\nI really liked the interactivity of this visualisation. Highlighting my selection, by subduing the colours of other selections is attractive. This was coupled with the exlpanation at the top left of the visaulisation, and the count and proportion in the middle of the visualisation. In addition, I opted to include the interactive legend in the top right of the screen, which is toggled on and off through a checkbox selection. In hindsight, this inclusion probably wasn’t necessary in light of the other explanations. All in all, a very clean, interactive, and visually appealing visualisation.\n\n\nCode\n# I was fishing for a good colour palette for the data\n\n# display.brewer.all()\n# display.brewer.pal(n = 6, name = \"Set2\")\n\nhex_colours <- brewer.pal(n = 6, name = \"Set1\") \n\n\n# create the sunburst visualisation\nsb_plot <- sunburstR::sunburst(sunburst_tree,\n                               valueField = \"n\",\n                               count = TRUE, # adds both a count and proportion\n                               legend = TRUE,\n                               width=\"100%\", \n                               height=500,\n                               colors = hex_colours)\n\n\nhtmlwidgets::prependContent(\n   sb_plot,\n   htmltools::h2(\"Distribution of Performance Rating by Department\"),\n   htmltools::tags$style(\"\n   .sunburst-legend {\n     font-style: bold;\n     font-size: 0.65em;\n   }\n   .sunburst .sunburst-explanation {\n     font-style: bold;\n     font-size: 1.25em;\n   }\") \n)\n\n\nDistribution of Performance Rating by Department\n\n\n\n\n\n\n\n\n\n\nLegend\n\n\n\n\n\n\n\n \n\n\nFinal Thoughts\nI really like the ease with which you can create a very clean and more importantly, interactive sunburst visualisation using the sunburstR package. I used fairly simple HR data to try out the sunburst visaulisation. However, I feel the visualisation format would further shine (no pun intended) with more complex data (i.e., more variable layers), as it could facilitate rapid identification of population differences.\nThere are a number of functions in the package that facilitate the creation and use of these visualisations in Shiny Apps, which is also very attractive. This strength is also illustrative of a weakness of this visualisation type–sunburst diagrams are really best suited to interactive mediums. This is not a visualisation that readily lends itself to common static mediums (e.g., pptx, docx, pdf).\nI would welcome a little more flexibility in the package regarding the formatting of the visualisation, which I expect will likely be introduced in future updates. However, the current workarounds were sufficient for most tasks.\nFinal thought–I would definitely use the sunburst diagram, and specifically the sunburstR package, again!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dmckinnon2021,\n  author = {Adam D McKinnon},\n  title = {Capturing the {Sun!}},\n  date = {2021-02-05},\n  url = {https://www.adam-d-mckinnon.com//posts/2021-02-05-capturing_the_sun},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAdam D McKinnon. 2021. “Capturing the Sun!” February 5,\n2021. https://www.adam-d-mckinnon.com//posts/2021-02-05-capturing_the_sun."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html",
    "href": "posts/2020-03-16-hrtechanalysis/index.html",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "",
    "text": "Photo by JJ Ying on Unsplash."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#data-collection",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#data-collection",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Data collection",
    "text": "Data collection\nWe obtained data from the 2019 Unleash Conference in London, specifically the 35 start-ups and 64 sponsors that were present at the conference. Conference attendees were provided a list of company names and their “tags”, which were descriptors of the technology or HR domain that characterize the respective companies."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#network-analyses-how-we-did-what-we-did",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#network-analyses-how-we-did-what-we-did",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Network Analyses — How we did what we did?",
    "text": "Network Analyses — How we did what we did?\nWe examined the tags using network analysis, in a technology called Kenelyze (kudos to our partner André Vermeij).\nThe current network examines the “influence” of HR tech among those vendors present at the 2019 Unleash Conference in London. Network Nodes were the company Tags, and company name was included as a node attribute. The network was undirected and displayed using the Force Atlas 2 layout. Nodes (i.e. Tags) were sized according to the number of times the Tag was used to describe an organization, and coloured according to the PageRank score (i.e. influence measure). The network is presented below.\n \n\nNetwork Output of Analysis of HR Tech at 2019 Unleash Conference in London"
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#what-are-prominent-technologies-in-the-hr-tech-landscape",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#what-are-prominent-technologies-in-the-hr-tech-landscape",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "What are prominent technologies in the HR Tech landscape?",
    "text": "What are prominent technologies in the HR Tech landscape?\n\n\n1. Talent acquisition is a main influencer\nThe most frequent Tag, most connected to other Tags, and most influential was “Talent Acquisition (incl. Referrals)”. It is perhaps unsurprising that Talent Acquisition is both a common and influential domain in HR Technology. This may in part reflect the discrete nature of the recruiting process, characterized by a definitive start and end point, and a clear, binary success measure—hired or not hired. In addition, Talent Acquisition also lends itself to performance measurement, evidenced by metrics such as source of hire, time to hire, applicants per hire, cost per hire, offer acceptance per hire, etc. All discrete metrics that the technology domain has progressively digitized. Good news for Talent Acquisition functions, what about the rest of us?\n\n\n\n2. People Analytics and Employee Experience are “up and coming”\nFollowing Talent Acquisition the next most frequent and influential Tags included Employee Engagement, Learning & Development, People Analytics, Talent Management, and Employee Experience (EX). These Tags reflect what we interpret as a mix of established HR domains (e.g. Employee Engagement, Learning & Development, Talent Management, HR Transformation), and “up and coming” (e.g. People Analytics and EX). The latter two topics have experienced a meteoric rise in prominence in recent years. However, both People Analytics and EX we anecdotally know to be technology related, and both benefit from a historical connection to data collection network nodes such as “Employee Engagement” and “Surveys (incl. Employee Listening)”.\nWe note two points about People Analytics and EX—first, their collective network metrics and second, the various companies that identify themselves with these two capabilities (i.e. click on nodes to see company listings). To us the network metrics and network position suggest that People Analytics is currently more influential (e.g. frequency, degree, PageRank), more central to the network (i.e. position), and consequently likely to play a more significant role in connecting technologies in the HR Tech landscape in the immediate future. Longer term, only time and repeated assessment of the market will tell.\nSecondly, the companies identifying themselves with these two capabilities are varied! Our interpretation of this variation is that the terms People Analytics and EX (also applicable to “Workforce Planning”) mean many things to different people, perhaps driven by maturity of practice in the consumer population. Alternatively, it may also represent future use cases that aren’t currently mainstream, or the potential for partnership or acquisition among technology vendors. Finally, it may represent opportunistic thinking by marketing professionals!\n\n\n\n3. Skills are the new currency\nA special call out to the topic of Skills Matching, which we at Merck KGaA have invested in during 2019, and will further explore in 2020. As evidenced in the network, we believe there is real world connectivity between Skills Matching and domains such as People Analytics (our team currently owns the Skills Matching topic at Merck KGaA), Strategic Workforce Planning (i.e. bridging the current with the future), Talent Management, Learning & Development (i.e. to get from the present to the future what training interventions are required), Talent Sourcing (incl. Referrals), and Performance Management (i.e. among other things a useful source of data to inform Skills Matching!). As referenced in recent predictions for 2020 by a variety of authors (e.g. David Green, Visier, Bersin, etc.), we believe that “Skills are the new currency”! Kudos to Ian Bailie for one of our favourite stakeholder pitch lines.\n\n\n\n4. Social Networks appear niche\nTech Tags that appeared less frequently than expected, at least considering public hype, included Digital HR and Social Networks. This may simply reflect the companies present at Unleash 2019. However, it may also be suggestive of the niche nature of this technology / approach, and the slow rate of adoption among companies due to various reasons (e.g. data privacy implications, prevalence of skills, lack of available use cases, etc.)."
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html",
    "href": "posts/2020-08-27-network_analysis/index.html",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "",
    "text": "Photo by Matt Palmer on Unsplash.\nThis article explores:"
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html#combining-datasets-to-add-even-more-value",
    "href": "posts/2020-08-27-network_analysis/index.html#combining-datasets-to-add-even-more-value",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "Combining datasets to add even more value",
    "text": "Combining datasets to add even more value\nAnalyses based on open data can become particularly powerful when datasets are combined in a meaningful way. If you’re working with various types of documents (let’s say patents and journal articles), you can use metadata which is common to both datasets to link them together and generate combined network overviews. Document summaries/abstracts and author information are good examples of columns which can be used to link datasets."
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html#acknowledgments",
    "href": "posts/2020-08-27-network_analysis/index.html#acknowledgments",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the myHRfuture website under the title “How HR can Apply Network Analysis to Open Data” on August 13, 2020."
  },
  {
    "objectID": "posts/2022-07-21-isolation_forests/index.html",
    "href": "posts/2022-07-21-isolation_forests/index.html",
    "title": "Automated assessment of employee data quality using machine learning",
    "section": "",
    "text": "Photo by Mika Baumeister on Unsplash.\n\n\n\n\n\nIntroduction\nThe topic of data quality is like that of I.T. services generally… you only ever hear about it when there’s a problem! There’s an implicit assumption among stakeholders receiving the findings of people analytics initiatives that your data is “good”. Failure to observe this assumption of high data quality can significantly (and rapidly) undermine the credibility of findings, irrespective of how small the data quality deviation!\nFrom an analytical perspective, the quality of your insights can potentially be limited by flawed data. As the saying goes “garbage in, garbage out”. Coupled with stakeholder expectations, it becomes increasingly important that organisations invest time and energy in the ongoing assessment and curation of quality data to maximise the influence of data driven decision making in organisations.\nAt the same time, monitoring data quality in HR can be immensely time consuming, expensive, simplistic in execution (i.e., simple evaluation criteria such as age ranges), highly repetitive, and utterly devoid of ANY professional enjoyment! To overcome these shortcomings, we have developed an automated, multi-dimensional and scalable approach to data quality evaluation using unsupervised machine learning—Isolation Forests.\n\n\nThe Process\nWe used Isolation Forests, an unsupervised machine learning approach to scan employee data for potentially anomalous[1] records.\nOur code can be viewed at this GitHub account, which followed the following process steps:\n\nIngested personnel data and loaded the appropriate libraries. In our example, we used the IBM HR Attrition dataset, then installed the H2o and Pandas libraries.\nDefined the dataset columns to include in the data quality assessment. Trained an Isolation Forest algorithm on the data. The Isolation Forest algorithm partitioned data through a forest of decision trees. Each split of the data was made randomly and the number of splits required to isolate a record indicated whether or not the record was considered anomalous. When a forest of random trees collectively produced shorter path lengths for particular records, they were highly likely to be considered anomalies.\nUsing the trained Isolation Forest model, we then predicted which records we believed to be anomalous. Of those records predicted to have data quality issues we then narrowed down to those with the highest likelihood of being anomalous using characteristics from the trained model.\nThe model has two levels of output interpretation:\n\nGlobal – which dataset columns are most often related to data quality errors. At best this is interesting in identifying potentially systemic errors, which may be a catalyst for more controlled data entry parameters.\nLocal – for any given record why is it deemed an anomaly. It is this level that is most valuable in directly resolving data quality issues, and therefore, was the focus of our approach.\n\nFor those records deemed highly anomalous (our subset), we then trained a simple Random Forest model to predict the anomaly flag(s). This process was repeated multiple times for each record to increase our certainty in identifying those variables contributing to its anomaly status. Note: We kept the model simple to make identification (and interpretation) of the combination of variables contributing to each records anomaly status easy to perform. In short, we wanted the output interpretable by a wide audience, not just those familiar with machine learning outputs.\nThe generated output (see excerpt of output in image below) has two fields: 1. A unique identifier, and 2. A list of variables contributing to each records anomaly status. The unique identifier, in our case the unique Employee Number taken from the original dataset, is critical to enabling identification and access to the original record that requires review. The second column has a ranked list (ranked from most important to least) showing the variables “collectively” contributing to the anomaly status. For example, when inspecting EmployeeNumber 58’s record we see a Monthly Income only four times their DailyRate (possibly incorrect), and that the individual has been with the company 2 years, had their last promotion 2 years ago, and has 2 years in their current role, which collectively cannot all be correct–the algorithm suggests reviewing YearsInCurrentRole.\n\n\n\n\n\n\nExample Output.\n\n\n\n\n\n\nConclusion\nUnsupervised Machine Learning algorithms such as Isolation Forests can be an excellent way of automating and scaling the review of data to monitor for quality concerns. The approach lends itself to those datasets that have a reasonable level of data quality and that are looking to make further improvements.\nThe major advantage of this approach is that the Isolation Forest can identify a record as anomalous, despite no one variable in the record being out of acceptable limits. Instead, the algorithm assesses the combination of multiple variables to determine if the combination makes it seem anomalous. This is like having human intelligence review each record quickly, at scale, and without destroying anyone’s job satisfaction!\nWe propose that this approach has the potential to significantly reduce time spent on direct data quality evaluations, which has considerable direct benefits (i.e., better quality data, more representative analyses and interpretation, etc.) as well as indirect benefits (i.e., time can be spent on other value-add initiatives that can only be done by humans!). This is particularly true for smaller teams, where the need to do more with less is greatest.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{curioni2022,\n  author = {Martha Curioni and Adam D McKinnon},\n  title = {Automated Assessment of Employee Data Quality Using Machine\n    Learning},\n  date = {2022-07-21},\n  url = {https://www.adam-d-mckinnon.com//posts/2022-07-21-isolation_forests},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMartha Curioni, and Adam D McKinnon. 2022. “Automated Assessment\nof Employee Data Quality Using Machine Learning.” July 21, 2022.\nhttps://www.adam-d-mckinnon.com//posts/2022-07-21-isolation_forests."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html",
    "href": "posts/2021-02-05-group_impact/index.html",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "",
    "text": "Photo by patricia serna on Unsplash."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html#ingesting-data",
    "href": "posts/2021-02-05-group_impact/index.html#ingesting-data",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "1. Ingesting Data",
    "text": "1. Ingesting Data\nThe first code chunk is fairly simple. We load our packages, read in our assessment results, clean the variable names, and finally take a simple look at the structure of the data to see what we are working with.\n\n\nCode\n# load the packages we will use for this workflow\nlibrary(tidyverse) # workhorse\nlibrary(readxl) # ingesting our data\nlibrary(janitor) # cleaning variable names\nlibrary(rstatix) # performing t-tests\nlibrary(ggpubr) # simple boxplots\n\n\n# Ingest our training assessment results\nassessment_results_tbl <- readxl::read_excel(path = \"training_data.xlsx\") %>% \n                          janitor::clean_names()\n\n\n# Get a feel for the data and its structure\ndplyr::glimpse(assessment_results_tbl)\n\n\nRows: 27\nColumns: 2\n$ training <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ result   <dbl> 56, 50, 52, 44, 52, 47, 47, 53, 45, 48, 42, 51, 42, 43, 44, 5…\n\n\nA key point to note is that in the “training” variable a response of “A” equates to the traditional face-to-face delivery mode, while “B” equates to the new digital learning format."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html#analysing-data",
    "href": "posts/2021-02-05-group_impact/index.html#analysing-data",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "2. Analysing Data",
    "text": "2. Analysing Data\nThe analysis comprised the following steps:\n\nFirstly, we performed a quick exploration of the data by getting some summary statistics.\nSecondly, we performed our t-test to determine if statistically significant differences existed in assessment outcome scores achieved by the two training formats.\n\nThese two steps are performed sequentially.\n\nStep 1.\n\n\nCode\n# 2a. get some summary stats\ndescriptive_stats_tbl <- assessment_results_tbl %>% \n  dplyr::group_by(training) %>% \n  rstatix::get_summary_stats(result, type = \"mean_sd\")\n\n\ndescriptive_stats_tbl\n\n\n# A tibble: 2 × 5\n  training variable     n  mean    sd\n  <chr>    <chr>    <dbl> <dbl> <dbl>\n1 A        result      15  47.7  4.42\n2 B        result      12  56.5  4.28\n\n\n\n\nInterpretation\nOur descriptive statistics indicate an interesting outcome. It appears a notable difference exists between training formats in the mean score achieved on the assessment outcome, Training B (digital) seems to achieve on average a higher score (56.5) than Training A (face-to-face). However, the question remains–is this difference statistically significant?\n\n\n\nStep 2.\nBefore we can perform the t-test we must check several assumptions to ensure a t-test is appropriate. The assumptions are as follows:\n\n\n\n\n\n\n\nAssumption\nOutcome\n\n\n\n\n1. The data are continuous (not discrete).\nOur assessment outcomes scores are continuous - CHECK\n\n\n2. The data follow the normal probability distribution.\nWe will test for normality using a Q-Q plot (Quantile-Quantile plot) and the Shapiro-Wilk’s test.\n\n\n3. The variances of the two populations are equal.\nWe will test for equal variance using the Levene Test.\n\n\n4. The two samples are independent.\nOur training participants could only participate in one type of training (i.e., face-to-face or digital) so our samples are independent - CHECK\n\n\n5. Both samples are simple random samples from their respective populations.\nOur participant results were randomly selected for analysis - CHECK!\n\n\n\nIf the data is normally distributed and the variance across the two groups (e.g. Training Groups A & B) are equal then a t-test can be used effectively. Let’s check the last two assumptions and then analyse the data!\n\n\nCode\n# 2b. Normality Tests\nggpubr::ggqqplot(assessment_results_tbl$result,\n                 title = \"Normality Assessment\") %>% \n  plotly::plotly_build()\n\n\n\n\n\n\nCode\nrstatix::shapiro_test(assessment_results_tbl$result)\n\n\n# A tibble: 1 × 3\n  variable                      statistic p.value\n  <chr>                             <dbl>   <dbl>\n1 assessment_results_tbl$result     0.958   0.338\n\n\nCode\n# 2c. Equal Variance - levene's test\nlevene_test(result ~ training, data = assessment_results_tbl)\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     1    25     0.242 0.627\n\n\nCode\n# 2d. perform the t-test\n(t_test_tbl <- assessment_results_tbl %>% \n  rstatix::t_test(formula = result ~ training, var.equal = TRUE) %>% \n  rstatix::add_significance())\n\n\n# A tibble: 1 × 9\n  .y.    group1 group2    n1    n2 statistic    df         p p.signif\n  <chr>  <chr>  <chr>  <int> <int>     <dbl> <dbl>     <dbl> <chr>   \n1 result A      B         15    12     -5.20    25 0.0000223 ****    \n\n\n\n\n\nInterpretation\nNormality Our QQ plot suggests that we can assume normality. The distribution of results are falling approximately along the reference line and within the bounds. However, the QQ plot can be sensitive to small sample sizes. To further validate normality we performed the Shapiro Wilk’s test. This test again suggests that the data are normally distributed, as the p-value (0.338) is greater than 0.05. We can assume normality!\nEqual Variance We use the levene’s test to determine whether both the training methods have an approximately equal variance. Since our p-value (0.627) is greater than 0.05 we can assume that the variance in outcome assessments scores is not statistically significantly different for the two training methods. We can assume equal variance! All our assumptions are met and we can perform the t-test.\nt-test The t-test indicates that the difference is highly significant (p-value = 0.0002); well below a cut-off of 0.01. This difference is extremely unlikely to be by chance alone and suggests that the digital training yields a better outcome, at least as defined by the assessment outcomes."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html#visualising-results",
    "href": "posts/2021-02-05-group_impact/index.html#visualising-results",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "3. Visualising Results",
    "text": "3. Visualising Results\nOur next step is to visualise the differences achieved by the two training formats. The intent here is to ensure our stakeholders can clearly appreciate that a notable difference exists, and subsequently take an investment decision with confidence. To illustrate the difference observed, we opted to use a Box-Plot, which displays the distribution of scores in each group, coupled with some descriptive statistics. As with our QQ plot, we have used the Plotly package to make the plot interactive.\n\n\nCode\n# 3. visualise the results from the two training methods\n\ntraining_plot <- assessment_results_tbl %>%  \n                 ggpubr::ggboxplot(x     = \"training\", \n                                   y     = \"result\",\n                                   fill  = \"training\",\n                                   xlab  = \"Training Delivery Format\",\n                                   ylab  = \"Assessment Result\") \nt <- list(\n  family = \"arial\",\n  size = 12)\n\n\n# make the graph interactive\nplotly::plotly_build(training_plot) %>% \n  plotly::layout(title = \"<b>Difference in Assessment Results by Training Delivery Format</b>\",\n                 font = t,\n                 showlegend = FALSE) \n\n\n\n\n\n\n\n\nInterpretation\nOur visualisation seeks to clearly illustrate that employees who completed the digital training performed better on the end-of-course assessment. This may suggest that the digital training results in better knowledge retention (i.e., they are learning more).\nThe line crossing each box on the boxplot indicates the median score of each training format. The median is the middle value achieved on the assessment results for each training format. There is a clear difference between the two groups when comparing these lines. This is worth calling out when delivering the results to stakeholders."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html#quantifying-benefit",
    "href": "posts/2021-02-05-group_impact/index.html#quantifying-benefit",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "4. Quantifying Benefit",
    "text": "4. Quantifying Benefit\nThere are two key ways in which we can further quantify the differences of the training formats:\n\nThe percentage change in performance. We have identified the gain is statistically significant and visualised the difference above. We can make this increase in performance more accessible to a broader audience by indicating the performance gain as a percentage.\nThe cost difference of the delivery formats. By calculating the cost difference of delivering the training over a year or more for the two formats.\n\nOn the basis of these two metrics we are in a better position to recommend a future course of action.\nPerformance Efficiency\nUsing the mean score from each training group we can calculate the % gain in performance. The code is presented in the code chunk below.\n\n\nCode\n# 4. quantify the benefit  \n# mean assessment score for f2f training\ntrain_A_mean <- descriptive_stats_tbl %>% \n  dplyr::filter(training == \"A\") %>% \n  pull(mean)\n\n\n# mean assessment score for digital training\ntrain_B_mean <- descriptive_stats_tbl %>% \n  dplyr::filter(training == \"B\") %>% \n  pull(mean)\n\n\n# calculate % increase in performance\n(pct_increase = ((train_B_mean - train_A_mean)/ train_A_mean) %>% scales::percent())\n\n\n[1] \"18%\"\n\n\nCost Efficiency\nBelow is an except from an excel spreadsheet in which we have detailed costs associated with both training formats.\n\n\nCode\nknitr::include_graphics(\"training_costs_3.png\")\n\n\n\n\n\nCost of training offerings.\n\n\n\n\n\n\nInterpretation\nOur performance gains are straightforward–digital training results in end of course assessment scores that are 18% higher than traditional face-to-face delivery. This kind of result is accessible to all audiences, and is compelling! In addition, we are confident that the result is a function of the training delivery format based on the results of our t-test.\nOur simple cost assessment also creates a compelling picture of the financial savings associated with digital training. In one year we stand to create $8,400 in savings if we move to a digital format, and this increases to $21,800 at the end of two years."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "",
    "text": "Image by Pietro Jeng on Unsplash.\nWhen you hear Artificial Intelligence (AI) the first thing that comes to mind are robots; in particular, the Steven Spielberg movie titled A.I. where a robot child is built that can love and behave just like a real human. This idea appears to be closer to a dream than reality. Truth is, AI is more ubiquitous than we might think. It ranges from self-driving cars, movie recommendations on Netflix, e-mail spam detection to voice-controlled assistants such as Apple’s SIRI. The fact is that AI is already present across many businesses and various industries, as is shown in the figure below (Note the low adoption rate in Human Resources).\nStill, evidence suggests that HR departments remain unable to seize the multitude of opportunities associated with AI. In part, what may be required to accelerate the adoption of AI is educational content directed at HR Professionals, not data scientists. Thus, we offer this brief guide to Machine Learning (ML), an important subset of AI, with the intent to demystify ML and make it tangible."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-reinforcement-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-reinforcement-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "1. What is Reinforcement Learning?",
    "text": "1. What is Reinforcement Learning?\nReinforcement Learning is probably best known through IBM’s Deep Blue computer, a “robot” that learned how to play chess and beat the human world champion.\nReinforcement Learning is a type of technique that enables an algorithm to learn by trial and error, using feedback from its own actions and experiences. Much like Pavlov and his dog, Reinforcement Learning involves rewarding decisions that lead to success and penalizing decisions that lead to anything other than success—ultimately making the algorithm more intelligent in the process.\nExamples of reinforcement learning applied in HR are a bit lean, though are most prevalent in areas such as education (i.e., applying content based on the progress of the student), finance and investment (i.e., advanced forecasting), supply chain operations (i.e., robots fulfilling orders in a warehouse), traffic flow optimization, and healthcare (i.e., accurate classification of biopsy images)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-supervised-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-supervised-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "2. What is Supervised Learning?",
    "text": "2. What is Supervised Learning?\nThe most common forms of ML across industries, and specifically the HR domain, are Supervised Learning, followed by Unsupervised Learning.\nIn Supervised Learning, we try to predict an outcome, such as whether an employee will leave the company, the risk of an employee being injured, or the ideal starting salary of a new employee.\nTo make predictions we need different input variables (i.e., variables are called “Features” by data scientists). Our input features are only limited to our imagination (i.e., what we think will be important), what data we can get our hands-on, or what data we can create (e.g., by knowing where someone works and where they live we can create a variable focused on employee commute distance).  ### An example: Supervised Learning informing Employee Turnover\nLet’s examine a more detailed example of Supervised Learning—predicting who will leave an organization. Imagine that 1 in 5 new recruits leaves an organization in their first 12 months of tenure. To prevent such turnover, we could build a supervised learning model that predicts the likelihood of new starters leaving, so that our HR and managerial colleagues could intervene.\nIn this example, the model outcome being predicted is turnover risk, and the features used to predict turnover risk could include an employees’ demographic and employment characteristics (e.g., age, education level, role level, pay relative to market, month of employment, presence of development plans, and so on.).\nAssuming such a model was highly accurate, it would enable us to understand turnover among our new starter population from three angles.\n\nFirstly, what are the factors most influential in predicting turnover among our population. An example of such a model output is presented in the figure below, which illustrates whether a feature prevents turnover (green bars) or promotes turnover (red lines), and the relative importance of each feature in predicting turnover (i.e. longer lines denote more importance).\nSecondly, the model also rates the likelihood of each new starter leaving the company, enabling focused intervention (i.e. the risk that Adam will leave in his first 12 months).\nThirdly, the model identifies the features preventing or promoting turnover risk for each employee. This individualized output can enable HR professionals to take informed and personalized action, regardless of whether they personally know each employee.\n\n\n\n\n\n\nExample output from a model that predicts employee turnover risk. The plot describes the relative importance and directional influence of features in the model. Red bars represent features that contribute to employee turnover risk, while green bars represent features preventing turnover risk.\n\n\n\n\n\nA supervised learning model used to predict employee turnover among new starters has the potential to reduce notable costs, including financial (e.g., separation, vacancy, recruitment, training, and replacement) reputational (e.g., eroding an EVP and/or reducing candidate appeal) and productivity-related (e.g., on average organizations invest between four weeks and three months training new employees). Some of these costs can be readily quantified so that we can identify organizational savings based on prevented turnover (e.g., preventing 2 in 10 resignations saves $xxx)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-unsupervised-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-unsupervised-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "3. What is Unsupervised Learning?",
    "text": "3. What is Unsupervised Learning?\nUnlike Supervised Learning where we are trying to predict an outcome, Unsupervised Learning analyzes many variables simultaneously to identify similarities, patterns or relationships in the data. Unsupervised Learning is more about understanding what’s in the data. The two most common uses of unsupervised learning are focused on:\n\nClustering: automatically splitting the dataset into groups based on similarities among the features analyzed. Classically applied to consumers, but equally relevant to organizations, whereby we understand our employee segments (i.e., clusters) and determine whether our HR policies serve the segments.\nAssociation mining: identifies sets of variables that often occur together in your dataset. For example, identifying injury patterns among workers at specific sites.  ### An example: Unsupervised Learning informing Employee Turnover\n\nCluster Analysis, the most famous form of unsupervised learning, can also help us better understand employee attrition. This approach can help group employees based on similar features (e.g., location, tenure, nationality, education level, age, performance level, etc.).\nThe figure below depicts the results of an analysis of the employee’s demographic features. Multiple demographic features are first reduced to two dimensions using a method called Manifold Learning (another non-supervised method), and these two new dimensions are then clustered using a method called T-SNE. The figure below shows us how the employees can be grouped together, in this case, twelve clusters, based on their demographic features.\n\n\n\n\n\nAn example cluster analysis output when applied to employee features.\n\n\n\n\n Once grouped into clusters, the next step is to determine the risk of turnover for each group. Moreover, it is interesting to identify if there are some shared risk factors, practically indicating that employees within a cluster are experiencing the workplace in a similar way.\nThis last insight is of considerable practical significance, as it may help us tailor interventions that target specific employee clusters, thereby delivering maximum impact (i.e., retaining employees and reducing turnover costs) and return on our investment (i.e. for every $ spent we generated $xxx in savings from reduced turnover)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#acknowledgments",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#acknowledgments",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the Analytics In HR (AIHR) website under the title “A Beginner’s Guide to Machine Learning for HR Practitioners” on June 8th, 2020."
  },
  {
    "objectID": "posts/2023-01-03-exploratory_analysis/index.html",
    "href": "posts/2023-01-03-exploratory_analysis/index.html",
    "title": "Expediting Exploratory Data Analysis",
    "section": "",
    "text": "Photo by Possessed Photography on Unsplash.\n\n\n\n\n\nStart with the required libraries, and then load some data.\n\n\nCode\n# data manipulation\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Data Exploration\nlibrary(ppsr)\nlibrary(correlationfunnel)\nlibrary(DataExplorer)\n\n# Visualisation addition\nlibrary(plotly)\n\n\n# Load Data ----\npromotions_tbl <- readr::read_csv(file = \"train.csv\") %>% \n    janitor::clean_names()\n\n# reduce the dataset size\ncleaned_promotions_tbl <-\npromotions_tbl %>%\n    tidyr::drop_na() %>% \n    dplyr::mutate(\n        is_promoted = as.character(is_promoted),\n        is_promoted = if_else(is_promoted==1, \"Yes\", \"No\") %>% as.factor()\n    )\n\n\n\n\n1. Predictive Power Score\n\n\nCode\ncleaned_promotions_tbl %>%\n    select(-employee_id) %>%\n    visualize_pps(\n        y = 'is_promoted', \n        do_parallel = FALSE\n        )\n\n\n\n\n\n\n\n\n\n\n2. Correlation Funnel\n\n\nCode\ncleaned_promotions_tbl %>% \n    select(-employee_id) %>% \n    binarize() %>% \n    correlate(target = is_promoted__Yes) %>% \n    plot_correlation_funnel(interactive = TRUE) %>% \n    plotly::config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n3. DataExplorer\n\n\n\n\n\n\n\nDefault Heatmap\n\n\nCode\ncorr_plot <- cleaned_promotions_tbl %>% \n    select(-employee_id) %>% \n    DataExplorer::plot_correlation(\n        theme_config = list(\n            legend.position = \"none\",\n            axis.text.x     = element_text(angle = 90)\n                )\n    )\n\n\n\n\n\n\n\nInteractive Heatmap\n\n\nCode\ncorr_plot$data$value <- round(corr_plot$data$value, digits = 2)\n\nplotly::plotly_build(corr_plot) %>% \n    plotly::layout(width = 700, height = 700) %>% \n    plotly::config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dmckinnon2023,\n  author = {Adam D McKinnon},\n  title = {Expediting {Exploratory} {Data} {Analysis}},\n  date = {2023-01-03},\n  url = {https://www.adam-d-mckinnon.com//posts/2023-01-03-exploratory_analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAdam D McKinnon. 2023. “Expediting Exploratory Data\nAnalysis.” January 3, 2023. https://www.adam-d-mckinnon.com//posts/2023-01-03-exploratory_analysis."
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html",
    "href": "posts/2021-06-06-going_the_distance/index.html",
    "title": "Going the Distance!",
    "section": "",
    "text": "Photo by Émile Séguin on Unsplash."
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html#calling-google-maps",
    "href": "posts/2021-06-06-going_the_distance/index.html#calling-google-maps",
    "title": "Going the Distance!",
    "section": "1. Calling Google Maps",
    "text": "1. Calling Google Maps\nTo use Google Maps you will need three things:\n\nAddress data. We begin by loading some fictitious address data provided by the Victorian State Government – School addresses from 2015.\nWorkplace addresses, both an old workplace address and a new workplace address. For this example, I am using the following two addresses from Victoria, Australia:\n\n\n\nOld workplace address: 154 High St, Ashburton VIC 3147, Australia (Ashburton Public Library); and\n\n\n\n\nNew workplace address: Spring St, East Melbourne VIC 3002, Australia (Victorian Parliament Building).\n\n\n\nA Google Maps API key, which can be set up on the Google Maps Developer Site. The Google Maps service has a free usage quota. To access Google Maps we will use the googleway library in R.\n\nWith all three pieces ready, we will then call Google Maps using the googleway::google_distance function. We will do this for two modes of transit:\n\nPublic Transport (called “Transit”) &\nCar.\n\n\n\nCode\n# Libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(rrapply)\n\nlibrary(googleway)\nlibrary(purrr)\nlibrary(echarts4r)\nlibrary(reactable)\n\n\n\n# # Set API key ----\nkey = my_api_key # enter your API key here\n  \n# Import Data ----\noriginal_locations_tbl <- readr::read_csv(file =\n\"http://www.education.vic.gov.au/Documents/about/research/datavic/dv165-allschoolslocationlist2015.csv\") %>% \n                          janitor::clean_names()\n\n\n\n# limit the address data to schools in the Greater Melbourne local government area's\ncouncils <- c(\"Bayside (C)\", \"Port Phillip (C)\", \"Stonnington (C)\", \"Casey (C)\", \"Melbourne (C)\", \n              \"Frankston (C)\", \"Glen Eira (C)\", \"Monash (C)\", \"Yarra (C)\", \"Moonee Valley (C)\")\n\n\n# create an address dataset\naddresses_tbl <- original_locations_tbl %>% \n  \n  # create and format the home address field, and create the old and new workplace addresses\n  dplyr::mutate(\n    home_address = base::paste0(address_line_1, \", \", address_town, \" \", address_state, \" \",address_postcode),\n    old_work_address = \"154 High St, Ashburton VIC 3147\",\n    new_work_address = \"Spring St, East Melbourne VIC 3002\",\n    employee         = paste0(\"Employee \", row_number())  \n    ) %>% \n  \n  # only include addresses from areas around Melbourne\n  dplyr::filter(lga_name %in% councils) %>% \n  \n  # randomly select 100 records\n  dplyr::sample_n(100) %>% \n  \n  dplyr::select(employee, home_address, old_work_address, new_work_address)\n  \n\n\n# check the dataset\n# head(addresses_tbl)\n\n\n# call Google Maps using googleway to calculate the distance and time for the old and new workplace locations\n# the calculations are repeated for both public transport and car modes of transport\ncommute_tbl <- addresses_tbl %>%\n\n  dplyr::mutate(\n        old_transit_calculations = purrr::map2(.x = home_address,\n                                               .y = old_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"transit\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        new_transit_calculations = purrr::map2(.x = home_address,\n                                               .y = new_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"transit\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        old_car_calculations     = purrr::map2(.x = home_address,\n                                               .y = old_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"driving\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        new_car_calculations     = purrr::map2(.x = home_address,\n                                               .y = new_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"driving\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE))\n\n    )"
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html#unpacking-the-data",
    "href": "posts/2021-06-06-going_the_distance/index.html#unpacking-the-data",
    "title": "Going the Distance!",
    "section": "2. Unpacking the Data",
    "text": "2. Unpacking the Data\nThe data returned from the google_distance function is complicated! Consequently, it required some fiddling to unpack and format in a usable fashion. While the approach developed works, I strongly suspect it could be better. Searching online has yielded several alternatives. However, I decided to stay with my own and welcome suggested improvements.\n\n\nCode\n# cleaning function for results\nresults_cleaner <- function(data, old_or_new, car_or_transit){\n    \n    # for renaming multiple variables from the API call\n    var_string <- paste0(old_or_new, \"_\", car_or_transit, \"_\")\n    # for renaming the status field\n    new_name   <- paste0(var_string, \"status\")\n    \n    # receive the data\n    data |> \n        rrapply::rrapply(how = \"bind\") |> \n        janitor::clean_names() |> \n        dplyr::select(-1, -2, -7) |> # remove the last two columns\n        dplyr::rename_at(vars(starts_with(\"rows_elements_1_\")), ~ str_replace_all(., \n                                                                              pattern     = \"rows_elements_1_\",\n                                                                              replacement = var_string) \n                                                                              ) |> \n        dplyr::rename( !! quo_name(new_name) := status)\n        \n}\n\n\n# clean the final results for the old commute by public transport\nold_transit_commute_tbl <- commute_tbl %>%\n    dplyr::select(old_transit_calculations) %>%\n    results_cleaner(old_or_new = \"old\", car_or_transit = \"transit\")\n\n\n# clean the final results for the new commute by public transport\nnew_transit_commute_tbl <- commute_tbl %>%\n    dplyr::select(new_transit_calculations) %>%\n    results_cleaner(old_or_new = \"new\", car_or_transit = \"transit\")\n\n# clean the final results for the old commute by car\nold_car_commute_tbl <- commute_tbl %>%\n    dplyr::select(old_car_calculations) %>%\n    results_cleaner(old_or_new = \"old\", car_or_transit = \"car\")\n\n\n# clean the final results for the old commute by car\nnew_car_commute_tbl <- commute_tbl %>%\n    dplyr::select(new_car_calculations) %>%\n    results_cleaner(old_or_new = \"new\", car_or_transit = \"car\")\n\n\n\n\n# connect the original data with the new clean distance and time results\ntotal_commute_tbl <- addresses_tbl %>%\n    dplyr::bind_cols(old_transit_commute_tbl) %>% # add the columns of the old commute by public transport\n    dplyr::bind_cols(new_transit_commute_tbl) %>% # add the columns of the new commute by public transport\n    dplyr::bind_cols(old_car_commute_tbl) %>% # add the columns of the old commute by car\n    dplyr::bind_cols(new_car_commute_tbl) %>% # add the columns of the new commute by car\n\n    # filter out any results that were not OK\n    dplyr::filter(old_transit_status == \"OK\" | \n                  new_transit_status == \"OK\" | \n                  old_car_status     == \"OK\" | \n                  new_car_status     == \"OK\")"
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html#creating-insights",
    "href": "posts/2021-06-06-going_the_distance/index.html#creating-insights",
    "title": "Going the Distance!",
    "section": "3. Creating Insights",
    "text": "3. Creating Insights\nThe data returned from googleway provides both the travel distance between the two points, and time required to complete the trip for the selected mode of transit. From here it is important to gauge the following:\n\n\n1. Are people generally better off travelling to the new office location?\nThis can be examined by both distance and time. However, in a city a short trip can still take considerable time. Therefore, examining the difference in time taken for the shortest option (i.e., car or public transport) appears more realistic / accurate. If the time decreased by more than 5 minutes, an improvement in commute is assumed.\n\n\nCode\n# calculate value in Minutes (default results are in seconds)\nscale_mins <- function(x){\n  x <- x / 60 \n  round(x, digits = 0)\n}\n\n\n# calculate value in Kilometres (default results are in metres)\nscale_kms <- function(x){\n  x <- x / 1000\n  round(x, digits = 1)\n} \n\n\n\n\nfinal_commute_details_tbl <- total_commute_tbl |> \n    \n    # covert all duration variable values to minutes\n    dplyr::mutate_at(vars(ends_with(\"duration_value\")), scale_mins) %>% \n    \n      \n    # covert all distance variable values to kilometres\n    dplyr::mutate_at(vars(ends_with(\"distance_value\")), scale_kms) %>% \n  \n    # calculate the difference in distance and time between the new and old locations\n    # NOTE: calculations are based on new - old. Therefore, negative numbers indicate less distance and time, \n    # whereas positive numbers indicate an increase in distance and time\n    dplyr::mutate(\n        diff_transit_commute_distance = new_transit_distance_value - old_transit_distance_value,\n        diff_transit_commute_time     = new_transit_duration_value - old_transit_duration_value,\n        diff_car_commute_distance     = new_car_distance_value - old_car_distance_value,\n        diff_car_commute_time         = new_car_duration_value - old_car_duration_value,\n\n        # find the shortest time (i.e., transit or car) for both the old and new offices\n        old_office_min_mins = base::ifelse(old_transit_duration_value < old_car_duration_value, \n                                           old_transit_duration_value, old_car_duration_value),\n        new_office_min_mins = base::ifelse(new_transit_duration_value < new_car_duration_value, \n                                           new_transit_duration_value, new_car_duration_value),\n        \n        # find the differences in the best commute times between the two locations\n        new_office_best_commute_diff = new_office_min_mins - old_office_min_mins,\n        \n        # find which is the better commute for each person (new office, old office, or similar). This assumes \n        # that a commute is better if the commute time is reduced by more than 5 minutes\n        preferred_commute_location   = dplyr::case_when((old_office_min_mins - new_office_min_mins) < -5 ~ \"Old Office\",\n                                                        (new_office_min_mins - old_office_min_mins) < -5 ~ \"New Office\",\n                                                        TRUE ~ \"Similar Commute\"),\n        \n        # determine the best mode of transport for the new location\n        preferred_mode_transport     = base::ifelse(new_transit_duration_value <= new_car_duration_value, \"Public Transport\", \"Car\")\n                                        \n        )\n\n\n# visually represent the impact of the new location to travel time\nfinal_commute_details_tbl %>% \n  dplyr::count(preferred_commute_location) %>% \n  echarts4r::e_chart(preferred_commute_location) %>% \n  echarts4r::e_bar(n) %>% \n  echarts4r::e_labels(position = \"insideTop\") %>% \n  echarts4r::e_legend(show = FALSE) %>% \n  echarts4r::e_title(text = \"Preferred Office Location Based on Travel Time\",\n                     \"Preferred Location Is Defined by A Reduction In Travel Time Of More Than 5 Minutes\",\n                     left = \"10%\") %>% \n  echarts4r::e_tooltip() \n\n\n\n\n\n\nThe visualisation above depicts how the location change will impact employee travel, and it appears the majority of individuals will be worse off when commuting to the new location.\n\n\n\n2. Which individuals will have shorter or longer travel times?\n\n\nCode\nfinal_commute_details_tbl %>% \n  \n  # select the preferred variables\n  dplyr::select(employee, home_address, preferred_commute_location, new_office_best_commute_diff,preferred_mode_transport, \n                new_transit_duration_value, diff_transit_commute_time, new_car_duration_value, \n                diff_car_commute_time) %>% \n  \n    dplyr::arrange(new_office_best_commute_diff) %>% \n  \n  # rename the variables for more readable output\n  dplyr::rename(\n                Employee = employee,\n                `Home Address`= home_address,\n                `Preferred Office` = preferred_commute_location,\n                `Commute Change (Mins)` = new_office_best_commute_diff,\n                `Preferred Transport for New Office` = preferred_mode_transport,\n                `Time via Public Transport (Mins)` = new_transit_duration_value,\n                `Public Transport Time Saving (Mins)` = diff_transit_commute_time,\n                `Time via Car (Mins)` = new_car_duration_value,\n                `Car Time Saving (Mins)` = diff_car_commute_time) %>% \n  \n  reactable::reactable(\n    # pagination and searching options\n    showPageSizeOptions = TRUE,\n    pageSizeOptions = c(5, 10, 25, 50, 100),\n    defaultPageSize = 5,\n    \n    # table formatting options\n    outlined = TRUE,\n    borderless = TRUE,\n    striped = TRUE,\n    resizable = TRUE,\n\n    # column width\n    columns = list(\n      Employee = reactable::colDef(minWidth = 150),\n      `Home Address` = reactable::colDef(minWidth = 175),\n      `Preferred Office` = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Commute Change (Mins)` = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Preferred Transport for New Office`  = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Time via Public Transport (Mins)`  = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Public Transport Time Saving (Mins)`  = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Time via Car (Mins)` = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Car Time Saving (Mins)`= reactable::colDef(minWidth = 150, align = \"center\")\n    ),\n    \n    # theming changes    \n    theme = reactable::reactableTheme(\n      highlightColor = \"#f0f5f9\",\n      cellPadding = \"8px 12px\",\n      style = list(fontFamily = \"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, Arial, sans-serif\"))\n  )\n\n\n\n\n\n\n\n\nThe above table could readily be exported to Excel and delivered to Leaders, Managers, Transition Teams, HRBP’s, etc., to help inform the delivery of personalised discussions with staff regarding proposed or upcoming changes. In addition, it could also help to inform the identification of voluntary flight risks, based on negative travel experiences associated with the move. Moreover, additional analyses could be performed on this data to help inform communication efforts (e.g., clustering to develop employee personas)."
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html",
    "href": "posts/2021-11-13-personnel_forecasting/index.html",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "",
    "text": "Photo by LYCS Architecture on Unsplash."
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#business-context",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#business-context",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Business Context",
    "text": "Business Context\nThe Brisbane City Council (Australia) resolves hundreds of customer contact enquiries by residents each day. In a bid to ensure ongoing customer centricity, the council offers multiple channels through which residents can contact the council. It is expected that council staff will resolve a particular number of customer contact enquiries each day. The rate of resolution varies by channel and is measured as follows (these rates are fictitious):\n\nEmail: 175 / employee / day,\nFace to Face: 75 / employee / day,\nMail: 75 / employee / day,\nOnline (e.g., Twitter, LinkedIn, etc.): 150 / employee / day,\nVoice: 200 / employee / day, and\nWeb (e.g., council website contact forms): 150 / employee / day."
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#business-challenge",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#business-challenge",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Business Challenge",
    "text": "Business Challenge\nThe council would like to accurately roster sufficient staff to resolve contact volumes, while not “over rostering” staff to minimise costs. To do this we will train several Machine Learning Models using historic rates of contact for each of the various contact channels, and then forecast customer contact volumes by channel over the next month. With the forecasted volumes, we can then calculate the required number of staff based on expected resolution rates (see above)."
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#workflow",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#workflow",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Workflow",
    "text": "Workflow\nThe workflow for addressing this business challenge is as follows:\n\nSetup & Data\n\nlibraries\ningest the data\ndata wrangling / feature engineering & dataset creation\n\nModeling\n\ndata preprocessing\ncreate the ML models\nevaluate model performance\ndetermine the best model for each customer contact channel\n\nForecasting & Rostering\n\ngenerate future channel forecasts & determine staff requirements"
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#libraries",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#libraries",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Libraries",
    "text": "Libraries\nWe begin by bringing in the required libraries for the workflow. The most notable call-outs are the libraries related to the ModelTime ecosystem for time series analysis.\n\n\nCode\n# data wrangling\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(flipTime)\nlibrary(tsibble)\n\n# ModelTime Ecosystem & Modeling\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(tidymodels)\n\n# graphing\nlibrary(echarts4r)\nlibrary(sknifedatar)\nlibrary(xaringanExtra)"
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#ingest-data",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#ingest-data",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Ingest Data",
    "text": "Ingest Data\nThe data comes from the following site. In preparation of this work I had joined the files beginning January 2020 through to March 2021. This joined file is read here, as are Queensland Public Holidays between 2020 to 2021 (from the tsibble package). I felt public holidays may influence contact enquiry volumes. In addition, we define our forecast window (28 days), and then visualise the data for each of the customer contact channels to see what we are working with.\n\n\nCode\n# ingest the customer contact time series\ncustomer_contact_ts_tbl <- readxl::read_excel(path = \"contact_customer_enquiries_jan2020_mar2021.xlsx\") %>% \n                           janitor::clean_names() %>% \n                           dplyr::rename(id = channel, value = volume) \n\n\n# create a tibble of public holidays in QLD\nqld_public_holidays_tbl <- tsibble::holiday_aus(year = 2020:2021, state = \"QLD\")\n\n# define our forecast horizon\nforecast_horizon <- 28\n\n\n# plot the time series\ngraphs_tbl <- customer_contact_ts_tbl %>% \n    dplyr::group_by(id) %>% \n    tidyr::nest() %>% \n    dplyr::mutate(\n        .plot = purrr::map(data, ~ timetk::plot_time_series(\n                                                          .data = .x,\n                                                          .date_var = date,\n                                                          .value = value,\n                                                          .smooth = TRUE))\n                  ) %>% \n    ungroup()\n\n\nxaringanExtra::use_panelset()\n\n\n\n\n\n\n\nEmail\n\n\n\n\n\n\n\n\nFace2Face\n\n\n\n\n\n\n\n\nMail\n\n\n\n\n\n\n\n\nOnline\n\n\n\n\n\n\n\n\nVoice\n\n\n\n\n\n\n\n\nWeb"
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#data-wrangling-feature-engineering",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#data-wrangling-feature-engineering",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Data Wrangling & Feature Engineering",
    "text": "Data Wrangling & Feature Engineering\nThe process of creating the full dataset is comprehensive and will therefore be broken down into the following bullet-points:\n\nWe begin by ensuring that the dataset is complete and that there are no missing dates and customer contact volumes. Where dates are missing the date is added and a value of zero is included.\nThe data is then transformed using a log transformation. Transformations help to minimise the influence of trends or cycles in the data, and enables the forecasting of what is left after these transformations. It is important to apply the inverse of the transformation to the final model forecasts in order get to back to the original scale (i.e., actual customer contact volumes).\nThe data is joined with the Queensland public holiday data.\nWe extend the dataframe out by our forecast horizon of 28 days.\nNew features are created including Fourier Features, Lags and Rolling Means of our new Lag Features.\nThe timeseries is nested by the id (i.e., customer contact channel). and\nThe data for each channel is split into Training and Test datasets for modelling.\n\n\n\nCode\nnested_data_tbl <- customer_contact_ts_tbl %>%\n\n    # pad out the date range and add zero's where needed\n    dplyr::group_by(id) %>%\n    timetk::pad_by_time(\n        .date_var = date,\n        .by = \"day\",\n        .pad_value = 0,\n        .start_date = \"2020-01-01\",\n        .end_date   = \"2021-03-31\"\n    ) %>%\n    \n    dplyr::ungroup() %>%\n    \n    # log transform the data\n    dplyr::mutate(value = log1p(value)) %>% \n\n    # extend the timeseries for nested modelling\n    extend_timeseries(\n        .id_var        = id,\n        .date_var      = date,\n        .length_future = forecast_horizon\n    ) %>%\n\n    # add public holiday data - holidays may impact customer contact\n    dplyr::left_join(qld_public_holidays_tbl, by = c(\"date\" = \"date\")) %>%\n    \n    # change the public holidays data to a numeric variable\n    dplyr::mutate(\n        holiday = ifelse(is.na(holiday), 0 , 1)\n    ) %>%\n\n    # add some new features based on the date and volume\n    dplyr::group_by(id) %>%\n    timetk::tk_augment_fourier(date, .periods = c(7, 14, 28)) %>%\n    timetk::tk_augment_lags(.value = value, .lags = 28) %>%\n    tk_augment_slidify(\n        .value = value_lag28,\n        .f = ~ mean(., na.rm = TRUE),\n        .period = c(7, 14, 28, 28*2),\n        .align = \"center\",\n        .partial = TRUE\n    ) %>%\n\n    dplyr::filter(!is.na(value_lag28)) %>%\n    dplyr::ungroup() %>%\n\n    # nest the timeseries data and then split the dataset by actual and future\n    nest_timeseries(\n        .id_var        = id,\n        .length_future = forecast_horizon\n    ) %>% \n    \n    # split the nested timeseries data by channel (i.e., training and testing datasets)\n    split_nested_timeseries(\n        .length_test = forecast_horizon\n    )"
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#data-preprocessing",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#data-preprocessing",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThe data pre-processing involves use of the Recipes package. This recipe creates a number of new features based on the date variable, removes some unwanted features created, normalises some of the numeric features created so as to minimise undue influence on the models created, and then one-hot encodes all nominal variables.\n\n\nCode\n# define the recipe using the training data\nrecipe_spec <- recipes::recipe(value ~ ., data = extract_nested_train_split(nested_data_tbl)) %>%\n    step_timeseries_signature(date) %>% # create date features\n    recipes::step_rm(matches(\"(.xts$)|(.iso$)|(hour)|(minute)|(second)|(am.pm)\")) %>% # remove unwanted features\n    recipes::step_normalize(date_index.num, date_year) %>% # normalises two numeric variables\n    recipes::step_dummy(all_nominal(), one_hot = TRUE) # one hot encodes nominal variables"
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#create-the-machine-learning-models",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#create-the-machine-learning-models",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Create the Machine Learning Models",
    "text": "Create the Machine Learning Models\nIn this section we will create seven models in total. Four will be used for forecasting (XGboost model, Boosted Prophet Time Series model, Random Forest model, and a Neural Net model), while the fifth is used to prove how much better our models are than “guessing”. This comparison involves generating what is called a “Naive” forecast. The Naive forecast simply carries the last known value forward as a forecast. The accuracy metrics associated with the Naive forecast can then be compared with the metrics from our more sophisticated models to validate their accuracy, and ultimately their utility. This was something I started doing earlier this year on the back of advice from a colleague. The intent is to ensure your models are good (a simple litmus test), and more importantly, are an excellent way of illustrating the practical utility of the models to non-technical audiences.\n\n\nCode\n### Naive Model ----\nwkfl_fit_naive <- workflows::workflow() %>%\n    workflows::add_model(\n        modeltime::naive_reg(mode = \"regression\") %>% \n        parsnip::set_engine(\"naive\")\n    ) %>%\n    workflows::add_recipe(recipe_spec)\n\n\n\n# XGboost\nwkfl_fit_xgboost <- workflows::workflow() %>%\n    workflows::add_model(\n        spec = boost_tree(mode = \"regression\") %>% \n        parsnip::set_engine(\"xgboost\")\n    ) %>%\n    workflows::add_recipe(recipe_spec %>% \n                              recipes::update_role(date, new_role = \"indicator\")) \n\n\n# * Prophet Boost ----\nwkfl_fit_prophet_boost <- workflows::workflow() %>%\n    workflows::add_model(\n        spec = prophet_boost(\n            mode = \"regression\",\n            seasonality_yearly = TRUE\n        ) %>%\n            parsnip::set_engine(\"prophet_xgboost\")\n    ) %>%\n    workflows::add_recipe(recipe_spec)\n\n\n\n# Random Forest\nwkfl_fit_rf <- workflows::workflow() %>%\n    add_model(\n        spec = rand_forest(mode = \"regression\") %>% \n        parsnip::set_engine(\"ranger\")\n    ) %>%\n    workflows::add_recipe(recipe_spec %>% \n                              recipes::update_role(date, new_role = \"indicator\")) \n\n\n\n# Neural Net\nwkfl_fit_nnet <- workflows::workflow() %>%\n    workflows::add_model(\n        spec = mlp(mode = \"regression\") %>% \n        parsnip::set_engine(\"nnet\")\n    ) %>%\n    workflows::add_recipe(recipe_spec %>% \n                              recipes::update_role(date, new_role = \"indicator\"))"
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#evaluate-model-performance",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#evaluate-model-performance",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Evaluate Model Performance",
    "text": "Evaluate Model Performance\nAfter adding the ML models into a table, we determine their accuracy against the Test dataset. Below you can view a table indicating six accuracy metrics that come standard with the Modeltime package. With the exception of rsq the general rule of thumb is smaller numbers are better. It is important to keep in mind that we initially log transformed our customer contact volumes, which reduces their absolute value and the breadth of the range of values in the time series. Practically this means the accuracy metrics are artificially lower than if we had stayed with the actual volumes. That said, these default models are performing well, even without model tuning. This is further exemplified when comparing the models to the Naive Forecast. If we were to only rely on the rmse values and compare them (i.e., Naive forecast to six ML models) for each customer contact channel volume (e.g., web, face2face, etc.), we could say that our models are between three and five times more accurate than a Naive forecast.\n\n\nCode\n# Modeltime Table\nnested_modeltime_tbl <- modeltime::modeltime_nested_fit(\n    # Nested data\n    nested_data = nested_data_tbl,\n\n   # Add workflows\n    model_list = list(\n        wkfl_fit_naive,\n        wkfl_fit_xgboost,\n        wkfl_fit_prophet_boost,\n        wkfl_fit_rf,\n        wkfl_fit_nnet\n        )\n)\n\n\nnested_modeltime_tbl %>%\n    modeltime::extract_nested_test_accuracy() %>%\n    modeltime::table_modeltime_accuracy(\n        .round_digits = 2,\n        .show_sortable = TRUE,\n        .interactive = TRUE,\n        defaultPageSize = 5,\n        bordered = TRUE,\n        striped = TRUE,\n        compact = TRUE,\n        defaultColDef = reactable::colDef(width = 80),\n        style = list(fontFamily = \"Work Sans, sans-serif\", fontSize = \"13px\"),\n        columns = list(\n            .model_id   = reactable::colDef(show = FALSE),\n            .type = reactable::colDef(show = FALSE),\n            .model_desc = reactable::colDef(width = 240)\n                       )\n    )\n\n\n\n\n\n\n\n\nAs noted above the models are performing well, even without model tuning. This is further exemplified when comparing the models to the Naive Forecast. If we were to only rely on the rmse values and compare them (i.e., Naive forecast to four ML models) for each customer contact channel volume (e.g., web, face2face, etc.), we could say that our models are between three and five times more accurate than a Naive forecast.\n\n\n\nCode\ntest_forecasts_tbl <- nested_modeltime_tbl %>%\n    modeltime::extract_nested_test_forecast() %>%\n    dplyr::group_by(id) %>%\n    tidyr::nest() %>%\n    dplyr::mutate(\n                .plot_forecast = purrr::map(data, ~modeltime::plot_modeltime_forecast(.data = .x,\n                                                                                      .conf_interval_show = FALSE,\n                                                                                      .legend_max_width = 15,\n                                                                                      .plotly_slider = TRUE\n                                                                                      ))\n                )%>%\n    dplyr::ungroup()\n\n\n\nWe can also visualise the forecasts from each of the models for each customer contact channel. The visualisations produced are interactive (you can zoom by using the slider at bottom of the visualisations and click on the legend to add/remove models from the visualisation) and we can visually inspect how well the models perform relative to one another and the actual data (i.e., the Test dataset). This is another great way of showing to business stakeholders how well our models perform, thereby creating greater confidence, and ideally use, of the model forecasts.\n\n\n\nEmail\n\n\n\n\n\n\n\n\nFaceToFace\n\n\n\n\n\n\n\n\nMail\n\n\n\n\n\n\n\n\nOnline\n\n\n\n\n\n\n\n\nVoice\n\n\n\n\n\n\n\n\nWeb"
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#determine-the-best-models",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#determine-the-best-models",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Determine the Best Models",
    "text": "Determine the Best Models\nWe now determine which of our seven models is best for each of the customer contact channels. This is done using the Root Mean Square Error (RMSE) value. The lowest value is indicative of the least error, when compared to our Test dataset.\n\n\nCode\nbest_nested_modeltime_tbl <- nested_modeltime_tbl %>%\n    modeltime::modeltime_nested_select_best(\n        metric                = \"rmse\",\n        minimize              = TRUE,\n        filter_test_forecasts = TRUE\n    )\n\n\nbest_nested_modeltime_tbl %>%\n    modeltime::extract_nested_best_model_report() %>%\n    dplyr::mutate_if(is.double, ~ round(.x, digits = 2)) %>%\n    gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      id\n      .model_id\n      .model_desc\n      .type\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    email\n4\nRANGER\nTest\n0.24\n3.85\n0.62\n3.94\n0.27\n0.90\n    face2face\n3\nPROPHET W XGBOOST ERRORS\nTest\n0.37\nInf\n0.21\n66.68\n0.84\n0.88\n    mail\n4\nRANGER\nTest\n0.44\nInf\n0.44\n24.37\n0.58\n0.70\n    online\n2\nXGBOOST\nTest\n0.08\n1.65\n0.48\n1.65\n0.10\n0.76\n    voice\n4\nRANGER\nTest\n0.08\n1.09\n0.14\n1.08\n0.11\n0.98\n    web\n4\nRANGER\nTest\n0.13\n1.99\n0.31\n2.02\n0.17\n0.95\n  \n  \n  \n\n\n\n\n\n\nCode\nbest_test_forecasts_tbl <- best_nested_modeltime_tbl %>%\n    modeltime::extract_nested_test_forecast() %>%\n    dplyr::group_by(id) %>%\n    tidyr::nest() %>%\n    dplyr::mutate(\n                .plot_forecast = purrr::map(data, ~ modeltime::plot_modeltime_forecast(.data = .x,\n                                                                                       .conf_interval_show = FALSE,\n                                                                                       .legend_max_width = 15,\n                                                                                       .plotly_slider = TRUE\n                                                                                      ))\n                ) %>%\n    dplyr::ungroup()\n\n\nTo provide the best outcome we use the best model for each customer contact channel to make the future forecast. The best model, according to its performance on the Test dataset, is also visualised below for inspection.\n\n\n\nEmail\n\n\n\n\n\n\n\n\nFace2Face\n\n\n\n\n\n\n\n\nMail\n\n\n\n\n\n\n\n\nOnline\n\n\n\n\n\n\n\n\nVoice\n\n\n\n\n\n\n\n\nWeb"
  },
  {
    "objectID": "posts/2021-11-13-personnel_forecasting/index.html#generate-future-channel-forecasts-determine-staffing-requirements",
    "href": "posts/2021-11-13-personnel_forecasting/index.html#generate-future-channel-forecasts-determine-staffing-requirements",
    "title": "Forecasting Future Personnel Requirements Using Machine Learning",
    "section": "Generate Future Channel Forecasts & Determine Staffing Requirements",
    "text": "Generate Future Channel Forecasts & Determine Staffing Requirements\nThe final step is to take the best model for each of the customer contact channels to forecast 28-days into the future. Based on the forecasted volumes for each of the customer contact channels we can then determine the number of staff required to work on each day. To do this we simply take the forecasted daily volumes and divide by the number of contacts per day that an employee is expected to meet for each of the contact channels.\n\n\n\nCode\nstaff_required <- function(channel, volume){\n\n      dplyr::case_when(\n        channel == \"email\"     ~ volume / 175,\n        channel == \"face2face\" ~ volume / 75,\n        channel == \"mail\"      ~ volume / 75,\n        channel == \"online\"    ~ volume / 150,\n        channel == \"voice\"     ~ volume / 200,\n        channel == \"web\"       ~ volume / 150\n      )\n}\n\n\n# refit our best models to all our historical data\nnested_modeltime_refit_tbl <- best_nested_modeltime_tbl %>%\n    modeltime::modeltime_nested_refit(\n        control = control_nested_refit(verbose = FALSE)\n    )\n\n\n\n# final forecasts\nfinal_forecasted_volumes_tbl <- nested_modeltime_refit_tbl %>%\n    modeltime::extract_nested_future_forecast() %>%\n    dplyr::mutate(\n      .value         = expm1(.value),\n      known_staff    = map2_dbl(.x = id,\n                                .y = .value,\n                                .f = ~ staff_required(channel = .x, volume = .y)) %>% base::ceiling(),\n      known_staff    = if_else(.key == \"actual\", known_staff, NA_real_),\n\n      required_staff = map2_dbl(.x = id,\n                                .y = .value,\n                                .f = ~ staff_required(channel = .x, volume = .y)) %>% base::ceiling(),\n      required_staff = if_else(.key == \"prediction\", required_staff, NA_real_)\n      )\n\n\n# create a function to generate staff roster forecasts\nplot_staff_required <- function(df){\n\n    df %>%\n        echarts4r::e_charts(date) %>%\n        echarts4r::e_line(name = \"Historical Roster\", known_staff) %>%\n        echarts4r::e_line(name = \"Predicted Roster\", required_staff) %>%\n        echarts4r::e_datazoom(type = \"slider\", start = 90) %>% \n        echarts4r::e_tooltip(formatter = htmlwidgets::JS(\"\n        function(params){\n            return('<b>Date: </b>' + params.value[0] + '<br/><b>Staff Required: </b>' + params.value[1])\n            }\n          \")\n        )\n\n}\n\n\n# visualise the staff roster forecasts\nstaff_volumes_tbl <- final_forecasted_volumes_tbl %>%\n    dplyr::select(channel = id, date = .index, known_staff, required_staff) %>%\n    dplyr::group_by(channel) %>%\n    tidyr::nest() %>%\n    dplyr::mutate(\n        .plot_personnel_forecast = purrr::map(data, ~ plot_staff_required(df = .x))\n    )%>%\n    dplyr::ungroup()\n\n\n\n\n\nEmail\n\n\n\n\n\n\n\n\nFace2Face\n\n\n\n\n\n\n\n\nMail\n\n\n\n\n\n\n\n\nOnline\n\n\n\n\n\n\n\n\nVoice\n\n\n\n\n\n\n\n\nWeb"
  },
  {
    "objectID": "posts/2020-10-10-forecast_people_analytics/index.html",
    "href": "posts/2020-10-10-forecast_people_analytics/index.html",
    "title": "What is the Forecast for People Analytics?",
    "section": "",
    "text": "The last 9 months have, more than ever, emphasized the importance of knowing what is coming. In this article, we take a closer look at forecasting. Forecasting can be applied to a range of HR-related topics. We will specifically examine how forecasting models can be deployed in R, using an example analysis on the rise in popularity of “people analytics”."
  },
  {
    "objectID": "posts/2020-10-10-forecast_people_analytics/index.html#the-goal-is-to-know-whats-coming",
    "href": "posts/2020-10-10-forecast_people_analytics/index.html#the-goal-is-to-know-whats-coming",
    "title": "What is the Forecast for People Analytics?",
    "section": "The goal is to know what’s coming…",
    "text": "The goal is to know what’s coming…\nPredictions come in different shapes and sizes. There are many Supervised Machine Learning algorithms that can generate predictions of outcomes, such as flight risk, safety incidents, performance and engagement outcomes, and personnel selection. These examples represent the highly popular realm of “Predictive Analytics”.\nHowever, a less mainstream topic in the realm of prediction is that of “Forecasting” – often referred to as Time Series Analysis. In a nutshell, forecasting takes values over time (e.g., closing price of a stock over 120 days) to forecast the likely value in the future.\nThe main difference between supervised machine learning and forecasting is best characterized by the data used. Generally, forecasting relies upon historical data, and the patterns identified therein, to predict future values.\nAn HR-related example would be using historical rates of attrition in a business or geography to forecast future rates of attrition. In contrast, predictive analytics uses a variety of additional variables, such as company performance metrics, economic indicators, employment data, and so on, to predict future rates of turnover. Depending upon the use case, there is a time and a place for both approaches.\nIn the current article, we focus on forecasting and highlight a new library in the R ecosystem called ModelTime. ModelTime enables the application of multiple forecasting models quickly and easily while employing a tidy framework (for those not familiar with R don’t worry about this).\nTo illustrate the ease of using ModelTime we forecast the future level of interest in the domain of People Analytics using Google Trends data. From there we will discuss potential applications of forecasting supply and demand in the context of HR."
  },
  {
    "objectID": "posts/2020-10-10-forecast_people_analytics/index.html#data-collection",
    "href": "posts/2020-10-10-forecast_people_analytics/index.html#data-collection",
    "title": "What is the Forecast for People Analytics?",
    "section": "Data Collection",
    "text": "Data Collection\nThe time-series data we will use for our example comes directly from Google Trends. Google Trends is an online tool that enables users to discover trends in search behavior within Google Search, Google News, Google Images, Google Shopping, and YouTube.\nTo do so, users are required to specify the following:\n\nA search term (up to four additional comparison search terms are optional),\nA geography (i.e., where the Google Searches were performed),\nA time period, and\nGoogle source for searches (e.g., Web Search, Image Search, News Search, Google Shopping, or YouTube Search).\n\nIt is important to note that the search data returned does NOT represent the actual search volume in numbers, but rather a normalized index ranging from 0-100. The values returned represent the search interest relative to the highest search interest during the time period selected. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular at that point in time. A score of 0 means there was not enough data for this term.\n\n\nCode\n# Libraries\nlibrary(gtrendsR)\nlibrary(tidymodels)\nlibrary(modeltime)\nlibrary(tidyverse)\nlibrary(timetk)\nlibrary(lubridate)\nlibrary(flextable)\nlibrary(prophet)\n\n\n# Data - Google Trends Setup\nsearch_term   <- \"people analytics\"\nlocation      <- \"\" # global\ntime          <- \"2010-01-01 2020-08-01\" # format \"Y-m-d Y-m-d\"\ngprop         <- \"web\"\n\n\n# Get Google Trends Data\ngtrends_result_list <- gtrendsR::gtrends(\n    keyword = search_term, \n    geo     = location, \n    time    = time,\n    gprop   = gprop\n    )\n\n\n# Data Cleaning\ngtrends_search_tbl <- gtrends_result_list %>%\n    purrr::pluck(\"interest_over_time\") %>%\n    tibble::as_tibble() %>%\n    dplyr::select(date, hits) %>%\n    dplyr::mutate(date = ymd(date)) %>%\n    dplyr::rename(value = hits)\n\n\n# Visualise the Google Trends Data\ngtrends_search_tbl %>%\n  timetk::plot_time_series(date, value)\n\n\n\n\n\n\nWe can see from the visualisation that the term “people analytics” has trended upwards in Google web searches from January 2010 through to August 2020. The blue trend line, established using a LOESS smoother (i.e., a non-parametric technique that tries to find a curve of best fit without assuming the data adheres to a specific distribution) illustrates a continual rise in interest. The raw data also indicates that the Google search term of “people analytics”, perhaps unsurprisingly, peaked in June of 2020.\nThis peak may relate to the impact of COVID-19, specifically the requirement for organisations to deliver targeted ad-hoc reporting on personnel topics during this time. Irrespective, the future for People Analytics seems to be of increasing importance."
  },
  {
    "objectID": "posts/2020-10-10-forecast_people_analytics/index.html#modeling",
    "href": "posts/2020-10-10-forecast_people_analytics/index.html#modeling",
    "title": "What is the Forecast for People Analytics?",
    "section": "Modeling",
    "text": "Modeling\nLet’s move into some Forecasting! The process employed using ModelTime is as follows:\n\nWe separate our dataset into “Training” and “Test” datasets. The Training data represents that data from January 2010 to July 2019, while the Test data represents the last 12 months of data (i.e., August 2019 – August 2020). A visual representation of this split is presented in the image you see below.\nThe Training data is used to generate a 12-month forecast using several different models. In this article, we have chosen the following models: Exponential Smoothing, ARIMA, ARIMA Boost, Prophet, and Prophet Boost.\nThe forecasts generated are then compared to the Test data (i.e., actual data) to determine the accuracy of the different models.\nBased on the accuracy of the different models, one or more models are then applied to the entire dataset (i.e., Jan 2010 – August 2020) to provide a forecast into 2021.\n\n\n\n\n\n\n\n\nWe have presented the R code below, with supporting outputs, for steps 1 through to 4.\n\n\nStep 1.\n\n\nCode\n# Train/Test \n# hold out 12 months for test split\nmonths <- 12\n\n\n# calculate the total numnber of months in data\ntotal_months <- lubridate::interval(base::min(gtrends_search_tbl$date),\n                                    base::max(gtrends_search_tbl$date)) %/%  \n                                    base::months(1)\n\n\n# determine the proportion for the test split\nprop <- (total_months - months) / total_months\n\n\n# Train/Test Split\nsplits <- rsample::initial_time_split(gtrends_search_tbl, prop = prop)\n\n\n# Plot splits as sanity check\nsplits %>%\n  timetk::tk_time_series_cv_plan() %>%  \n  timetk::plot_time_series_cv_plan(date, value) \n\n\n\n\n\n\n\nThe plot above visually depicts our Training and Testing splits in the data, specifically the time period represented by both.\n\n\n\nStep 2\nWe first generate a 12-month forecast using five models (Exponential Smoothing, ARIMA, ARIMA Boost, Prophet, and Prophet Boost) on the training data.\n\n\nCode\n# Modeling\n# Exponential Smoothing\nmodel_fit_ets <- modeltime::exp_smoothing() %>%\n    parsnip::set_engine(engine = \"ets\") %>%\n    parsnip::fit(value ~ date, data = training(splits))\n\n\n# ARIMA \nmodel_fit_arima <- modeltime::arima_reg() %>%\n    parsnip::set_engine(\"auto_arima\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\n\n# ARIMA Boost\nmodel_fit_arima_boost <- modeltime::arima_boost() %>%\n    parsnip::set_engine(\"auto_arima_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date, label = TRUE), \n        data = training(splits))\n\n\n# Prophet\nmodel_fit_prophet <- modeltime::prophet_reg() %>%\n    parsnip::set_engine(\"prophet\") %>%\n    parsnip::fit(\n        value ~ date, \n        data = training(splits))\n\n\n# Prophet Boost\nmodel_fit_prophet_boost <- modeltime::prophet_boost() %>%\n    parsnip::set_engine(\"prophet_xgboost\") %>%\n    parsnip::fit(\n        value ~ date + as.numeric(date) + month(date, label = TRUE), \n        data = training(splits))\n\n\n# Modeltime Table\nmodel_tbl <- modeltime::modeltime_table(\n    model_fit_ets,\n    model_fit_arima,\n    model_fit_arima_boost,\n    model_fit_prophet,\n    model_fit_prophet_boost)\n\n\n\n\n\nStep 3.\nThe forecasts generated above are now compared to the Test data (i.e., actual data) to determine the accuracy of the different models.\n\n\nCode\n# Calibrate the model accuracy using the test data\ncalibration_tbl <- model_tbl %>%\n    modeltime::modeltime_calibrate(testing(splits))  \n\n\n# create a table of the calibration results\ncalibration_tbl %>%\n    modeltime::modeltime_accuracy() %>%\n    dplyr::mutate_if(is.numeric, ~round(., digits = 2)) %>%\n    gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      .model_id\n      .model_desc\n      .type\n      mae\n      mape\n      mase\n      smape\n      rmse\n      rsq\n    \n  \n  \n    1\nETS(M,A,N)\nTest\n8.52\n9.61\n1.28\n10.33\n11.05\n0.31\n    2\nARIMA(0,1,1)(2,0,1)[12] WITH DRIFT\nTest\n7.51\n8.53\n1.13\n9.01\n9.77\n0.18\n    3\nARIMA(0,1,1)(2,0,1)[12] WITH DRIFT W/ XGBOOST ERRORS\nTest\n8.53\n9.98\n1.28\n10.47\n10.19\n0.21\n    4\nPROPHET\nTest\n7.59\n8.55\n1.14\n9.14\n10.09\n0.26\n    5\nPROPHET W/ XGBOOST ERRORS\nTest\n7.47\n8.47\n1.12\n8.99\n9.73\n0.25\n  \n  \n  \n\n\n\n\n\nThe table above illustrates the metrics derived when evaluating the accuracy of the respective models using the Test set. While it is beyond the scope of this article to explain the models and their metrics, a simple rule of thumb when looking at the below table is that smaller error numbers generally indicate a better model!\nThe graph below illustrates how the models performed relative to the actual data (i.e., our Test set).\n\n\nCode\n# plot the forecast\ncalibration_tbl %>%\n    modeltime::modeltime_forecast(new_data = testing(splits), \n                                  actual_data = gtrends_search_tbl,\n                                  conf_interval = 0.90) %>%\n    modeltime::plot_modeltime_forecast(.legend_show = TRUE, \n                                       .legend_max_width = 20)\n\n\n\n\n\n\n\n\n\nStep 4.\nBased on these metrics we decided to use all five models to forecast into 2021 (i.e., our final Step 4).\n\n\nCode\n# Refit the five models with the last 12 months of data (i.e., Test data)\nrefit_tbl <- calibration_tbl %>%\n    modeltime::modeltime_refit(data = gtrends_search_tbl) \n\n\n# forecast the next 12 months into 2021\nforecast_tbl <- refit_tbl %>%\n    modeltime::modeltime_forecast(\n        h = \"1 year\",\n        actual_data = gtrends_search_tbl,\n        conf_interval = 0.90\n    ) \n\n\n# 3 create an interactive visualization of the forecast\nforecast_tbl %>%\n    modeltime::plot_modeltime_forecast(.interactive = TRUE,\n                                       .legend_max_width = 20)\n\n\n\n\n\n\n\nTo enhance the accuracy and interpretability (particularly among stakeholders) of the forecast we will take an average of the five models to create an aggregate model. We can see below in both the code and the interactive visualization, that the ongoing trend for people analytics is one of increasing popularity over time!\n\n\nCode\n# Create an aggregated model based on our 5 models\nmean_forecast_tbl <- forecast_tbl %>%\n    dplyr::filter(.key != \"actual\") %>%\n    dplyr::group_by(.key, .index) %>%\n    dplyr::summarise(across(.value:.conf_hi, mean)) %>%\n    dplyr::mutate(\n        .model_id   = 6,\n        .model_desc = \"AVERAGE OF MODELS\"\n    )\n\n\n# Visualize aggregate model \nforecast_tbl %>%\n    dplyr::filter(.key == \"actual\") %>%\n    dplyr::bind_rows(mean_forecast_tbl) %>%\n    modeltime::plot_modeltime_forecast()  \n\n\n\n\n\n\n\nThe forecast of Google Search interest for the next 12 months appears to continue its trend of upward growth – the forecast for People Analytics seems bright!"
  },
  {
    "objectID": "posts/2020-10-10-forecast_people_analytics/index.html#acknowledgments",
    "href": "posts/2020-10-10-forecast_people_analytics/index.html#acknowledgments",
    "title": "What is the Forecast for People Analytics?",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThe authors would like to acknowledge the work of Matt Dancho, both in the development and maintenance of TimeTK and ModelTime, and the Learning Labs Pro Series ran by Matt, upon which this article is directly based.\nThis article was first published on the Analytics In HR (AIHR) website under the title “Forecasting in R: a People Analytics Tool” on August 31st, 2020."
  },
  {
    "objectID": "posts/2020-12-30-causal_impact/index.html",
    "href": "posts/2020-12-30-causal_impact/index.html",
    "title": "Show Me The Money! Measuring Impact Over Time",
    "section": "",
    "text": "Image source: Cubankite, Africa Studio, Dean Drobot - Shutterstock."
  },
  {
    "objectID": "posts/2020-12-30-causal_impact/index.html#create-time-series-data",
    "href": "posts/2020-12-30-causal_impact/index.html#create-time-series-data",
    "title": "Show Me The Money! Measuring Impact Over Time",
    "section": "1. Create Time Series Data",
    "text": "1. Create Time Series Data\nThe below code stub generates fictitious data for our example. Please note that you can replicate this example, however, the data generated may be different, which in turn may lead to different outcomes (i.e., both visualisations, summary statistics and benefit quantified).\n\n\nCode\n# load libraries\n\nlibrary(tidyverse) # workhorse\nlibrary(janitor) # better naming conventions\nlibrary(lubridate) # working with dates\nlibrary(zoo) # time series data format for causal impact\nlibrary(CausalImpact) # our method of interest\nlibrary(plotly) # making graphs attractive and interactive\n\n\nset.seed(1)\n\n# create a date sequence\n# takes default timezone setting from machine\nmydatetime <- as.POSIXct(\"2018-01-01\") \ndate <- base::seq.POSIXt(from = mydatetime, length.out = 365, by = \"day\")\n\n\n# control data - 4 countries (Italy, Sweden, Spain, Germany)\n# numbers reflect number of employees on leave each day\nit_numbers <- 100 + stats::arima.sim(model = list(ar = 0.999), n = 365) \nse_numbers <- 1.5 * it_numbers + rnorm(85)\nes_numbers <- 0.8 * it_numbers + rnorm (25)\nde_numbers <- 2.5 * it_numbers + rnorm (125)\n\n\n# case country data - UK\n# implementing a gradual increase to reflect realistic behaviour \n# (i.e., incremental absence/leave taking increase)\nuk_numbers          <- 1.2 * it_numbers + rnorm(100)\nuk_numbers[275:295] <- uk_numbers[275:365] + 2\nuk_numbers[296:316] <- uk_numbers[275:365] + 5\nuk_numbers[317:347] <- uk_numbers[275:365] + 12\nuk_numbers[348:365] <- uk_numbers[275:365] + 10\n\n\n# combine the data into one tibble\ndata_tbl <- bind_cols(date, uk_numbers, it_numbers, se_numbers, es_numbers, de_numbers) %>% \n            janitor::clean_names() %>% \n            dplyr::rename(date = x1,\n                   uk_numbers = x2,\n                   it_numbers = x3,\n                   se_numbers = x4,\n                   es_numbers = x5,\n                   de_numbers = x6) %>% \n            \n            # ONLY INCLUDE WEEKDAY DATA - no absences on weekend!\n            dplyr::mutate(date2 = date %>% lubridate::as_date() %>% lubridate::wday()) %>% \n\n            # 1 = sunday and 7 = saturday, 2 - 6 = weekdays\n            dplyr::filter(date2 >1 & date2 < 7) %>% \n\n            #  round numbers to whole numbers (no employee parts!)\n            dplyr::mutate_at(vars(dplyr::ends_with(\"_numbers\")), ~ base::round(., 0)) %>% \n            dplyr::select(-date2) \n\n\n# create UK only data\nuk_data <- data_tbl %>% \n            dplyr::select(1:2) %>% \n            base::as.data.frame() %>% \n            zoo::read.zoo()\n\n                              \n# create all countries data\nall_country_data <- data_tbl %>% \n                    base::as.data.frame() %>% \n                    zoo::read.zoo() \n\n\nWe now have our employee absence data for both the UK and our control countries of Italy, Sweden, Spain and Germany. Let’s begin by assessing the impact of the campaign when looking at the UK data in isolation."
  },
  {
    "objectID": "posts/2020-12-30-causal_impact/index.html#perform-causal-impact-analysis-using-uk-data-review-results",
    "href": "posts/2020-12-30-causal_impact/index.html#perform-causal-impact-analysis-using-uk-data-review-results",
    "title": "Show Me The Money! Measuring Impact Over Time",
    "section": "2. Perform Causal Impact Analysis Using UK Data & Review Results",
    "text": "2. Perform Causal Impact Analysis Using UK Data & Review Results\nWe begin by specifying the pre and post campaign periods. With this established we are able to perform the causal impact analysis and then both visualise results and provide a statistical summary of the model.\n\n\nCode\n# establish when the intervention occurred\npre.period <- as.POSIXct(c(\"2018-01-01\", \"2018-09-30\"))\npost.period <- as.POSIXct(c(\"2018-10-01\", \"2018-12-31\"))\n\n\n# perform the causal impact analysis with UK data only\nimpact_uk_only <- CausalImpact::CausalImpact(data         = uk_data, \n                                              pre.period  = pre.period, \n                                              post.period = post.period)\n\n\n# get a graphical summary of the UK only-causal impact model\ngraphics::plot(impact_uk_only) %>% \n    plotly::plotly_build()\n\n\n\n\n\n\n\nThe graphical visualisation of the model provides three key perspectives of the model. The top panel of the visualisation shows a blue confidence interval, in which we would expect our time series data to remain in the absence of the campaign. The second panel displays the difference between situation normal (i.e., no campaign), which is depicted by the the zero value line in the graph, and what happened in reality. In our example we see the pointwise values occur both above and below the zero line. The third panel provides a perspective on the cumulative benefit of the campaign. The visualisation adds the pointwise contributions from the second panel, showing the benefit was minimal and only realised at the end of the post intervention period. None of these visualisation panels inspire confidence in our campaign!\nThe summary results do not paint a compelling picture for our campaign either. I will call out a few key results and couple this with text lifted directly from the Summary Report (not included in this article due to its length, but a very useful explanation for Causal Impact Analysis users). The summary indicates that during the post-intervention period we had an actual average of 107 employees on leave per day. In the absence of the campaign we would have expected to have an average 106 employees on leave per day. This expectation is based on a time series model generated using pre-campaign UK data.\n\n\nCode\n# get a summary &/or report of the model\nbase::summary(impact_uk_only) # summary results\n\n\nPosterior inference {CausalImpact}\n\n                         Average         Cumulative     \nActual                   107             7039           \nPrediction (s.d.)        106 (0.69)      6988 (45.57)   \n95% CI                   [105, 107]      [6902, 7070]   \n                                                        \nAbsolute effect (s.d.)   0.77 (0.69)     50.94 (45.57)  \n95% CI                   [-0.47, 2.1]    [-30.93, 137.0]\n                                                        \nRelative effect (s.d.)   0.74% (0.66%)   0.74% (0.66%)  \n95% CI                   [-0.44%, 2%]    [-0.44%, 2%]   \n\nPosterior tail-area probability p:   0.14257\nPosterior prob. of a causal effect:  86%\n\nFor more details, type: summary(impact, \"report\")\n\n\n\nOur 95% confidence interval tells us we would need the number to fall above 107 (or below 105 in case the campaign had an adverse impact) to be considered to have had an impact. The summary report goes into considerably more detail, and includes the following “although the intervention appears to have caused a positive effect, this effect is not statistically significant when considering the entire post-intervention period as a whole.” This interpretation is confirmed by our Posterior tail-area probability of p = 0.146. This means that the positive effect noted in the results may be by chance and is not considered statistically significant."
  },
  {
    "objectID": "posts/2020-12-30-causal_impact/index.html#perform-causal-impact-analysis-using-uk-and-control-data-review-results",
    "href": "posts/2020-12-30-causal_impact/index.html#perform-causal-impact-analysis-using-uk-and-control-data-review-results",
    "title": "Show Me The Money! Measuring Impact Over Time",
    "section": "3. Perform Causal Impact Analysis Using UK And Control Data & Review Results",
    "text": "3. Perform Causal Impact Analysis Using UK And Control Data & Review Results\nLet’s repeat the process, though this time including the data from the control countries (i.e., Italy, Sweden, Spain, and Germany. With our data, and pre and post campaign periods already established we can proceed directly to modelling and examining the output of the model.\n\n\nCode\n# perform the causal impact analysis with all data\nimpact_all_data <- CausalImpact::CausalImpact(data = all_country_data, \n                                              pre.period  = pre.period, \n                                              post.period = post.period)\n\n\n# get a graphical summary of the campaign impact\ngraphics::plot(impact_all_data) %>% \n    plotly::plotly_build()\n\n\n\n\n\n\nThis time the visualisation of the model results are more compelling! The top panel shows a blue line which is our baseline. The baseline indicates where we would expect UK data to be in the absence of the campaign (based on both the control countries and UK data). The actual data appears to exceed the baseline. The second panel, depicting the difference between no campaign (i.e., the zero line) and the actual data indicated the campaign had a positive impact. The third panel suggests that the cumulative benefit of the campaign, based on the pointwise contributions from the second panel, is considerable. The campaign appears to be working, but let’s get a statistical perspective.\nThe results of the model suggest that the campaign had a positive impact that is statistically significant! Our actual average of 107 employees being on leave each day is above that of the model prediction, which was 100. In addition, our actual average is well above our 95% Confidence Interval of 101. Finally the probability that this result was due to chance alone is 0.001, and we have achieved a 99.8997% probability that the campaign had a causal effect (i.e., worth betting on!).\n\n\nCode\n# get a summary &/or report of the model\nbase::summary(impact_all_data) # summary results\n\n\nPosterior inference {CausalImpact}\n\n                         Average        Cumulative    \nActual                   107            7039          \nPrediction (s.d.)        100 (0.37)     6600 (24.10)  \n95% CI                   [99, 101]      [6553, 6647]  \n                                                      \nAbsolute effect (s.d.)   6.6 (0.37)     438.6 (24.10) \n95% CI                   [5.9, 7.4]     [391.5, 486.0]\n                                                      \nRelative effect (s.d.)   6.6% (0.39%)   6.6% (0.39%)  \n95% CI                   [5.9%, 7.4%]   [5.9%, 7.4%]  \n\nPosterior tail-area probability p:   0.00102\nPosterior prob. of a causal effect:  99.89796%\n\nFor more details, type: summary(impact, \"report\")\n\n\n\nComparing the two models (i.e., UK data only vs. UK data and control countries) clearly illustrates the value of including control data in this instance."
  },
  {
    "objectID": "posts/2020-12-30-causal_impact/index.html#quantifying-the-financial-impact-of-the-campaign",
    "href": "posts/2020-12-30-causal_impact/index.html#quantifying-the-financial-impact-of-the-campaign",
    "title": "Show Me The Money! Measuring Impact Over Time",
    "section": "4. Quantifying The Financial Impact Of The Campaign",
    "text": "4. Quantifying The Financial Impact Of The Campaign\nBased on the period we analysed (i.e., 1/10/2018 - 31/12/2018) our second model indicates that under normal circumstances we would have expected 100 employees on average to take leave each day, while the campaign seems to have catalysed a daily average of 107 employees. The cumulative benefit of this campaign is an additional 439 UK employees taking leave during the post campaign period (i.e., subtracting our cumulative prediction (6599) from our cumulative actual (7036)).\nIf the average daily cost of an UK employee is $300 GBP we could simply determine the financial benefit of the campaign by multiplying 439 by our average daily cost of an employee (i.e., $300 GBP). The estimated gross financial benefit of the campaign would be $131,700 GBP, which can be narrowed to a net financial benefit of $101,700 GBP, after subtracting our campaign costs of $30,000 GBP. Not bad HR!"
  },
  {
    "objectID": "posts/2022-12-30-promotion_prediction/index.html",
    "href": "posts/2022-12-30-promotion_prediction/index.html",
    "title": "Predicting Promotions Through Machine Learning",
    "section": "",
    "text": "Photo by Possessed Photography on Unsplash.\n\n\n\n\n\n\nLibraries\n\n\nCode\n# data manipulation\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(janitor)\n\n# modelling\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(bundle)\nlibrary(plotly)\n\n# Processing power\nlibrary(doParallel)\nlibrary(parallelly)\n\n\ntidymodels_prefer()\n\n\n\n\n\nData\n\n\nCode\n# Load Data ----\npromotions_tbl <- readxl::read_excel(path = \"2022_12_15_promotions.xlsx\")\n\n\npromotions_tbl <- promotions_tbl %>% \n    mutate(\n        promoted  = forcats::as_factor(promoted) %>% forcats::fct_relevel(\"promoted\", \"not promoted\")\n    ) %>% \n    mutate_at(.vars = c(\"gender\", \"work_site\", \"management_level\"), .funs = ~ forcats::as_factor(.))\n\n\n\n\n\nBuilding an ML Model\n\n1. Splitting the data\n\n\nCode\n# Spending the dataset ----\n\nset.seed(836)\npromotion_split     <- initial_split(promotions_tbl, strata = promoted)\npromotion_train_tbl <- training(promotion_split)\npromotion_test_tbl  <- testing(promotion_split)\n\n\nset.seed(234)\npromotion_folds <- bootstraps(promotion_train_tbl, \n                              times = 75, # default is 25 - inflated to accommodate racing method of tuning \n                              strata = promoted)\n\n# check the promotion_folds \n# promotion_folds\n\n\n ### 2. Pre-processing the data\n\n\nCode\n# Data Pre-processing ----\nxgboost_recipe <- \n    recipe(formula = promoted ~ ., data = promotion_train_tbl) %>% \n    recipes::update_role(employee_id, new_role = \"id\") %>% \n    step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% \n    step_zv(all_predictors()) \n\n\n# check the recipe\n# xgboost_recipe\n\n\n ### 3. Create a model specification\n\n\nCode\n# Model Set-up ----\nxgboost_spec <- \n    boost_tree(trees = 1000, \n               tree_depth = tune(), min_n = tune(), \n               loss_reduction = tune(), \n               sample_size = tune(), mtry = tune(),\n               learn_rate = tune()) %>% \n    set_engine(\"xgboost\") %>% \n    set_mode(\"classification\")\n\n\n# check the model specification\n# xgboost_spec\n\n\n ### 4. Workflow setup\n\n\nCode\n# Workflow setup\nxgboost_workflow <- \n    workflow() %>% \n    add_recipe(xgboost_recipe) %>% \n    add_model(xgboost_spec) \n\n# Check the workflow\n# xgboost_workflow\n\n\n ### 5. Tuning the model\n\n\nCode\n# specify the metrics of interest\n# NOTE: The first metric listed will be used for tuning\npromotion_metrics <- metric_set(\n                            roc_auc, \n                            accuracy, \n                            sensitivity, \n                            specificity\n                            )\n\n\n# establish parallel processing based on the number of available cores\ndoParallel::registerDoParallel(cores = parallelly::availableCores())\n\n\nset.seed(826)\nracing_resamples <- finetune::tune_race_anova(\n    xgboost_workflow,\n    resamples = promotion_folds,\n    grid = 100, # cast a wide grid to optimise the results -\n                # works best with many resamples - set earlier to 75\n    metrics = promotion_metrics,\n    control = control_race(\n        verbose_elim = TRUE,\n        save_pred    = TRUE\n        )\n)\n\n\n# racing_resamples\n\n\n\n\n\n6. Assess model performance\n\n\nCode\nfirst_model_metrics_tbl <- collect_metrics(racing_resamples)\ntuning_plot <- plotly_build(plot_race(racing_resamples))\n\nxaringanExtra::use_panelset()\n\n\n\n\n\n\n\nPromotion Metrics\n\n\n\n\n\n\n  \n  \n    \n      mtry\n      min_n\n      tree_depth\n      learn_rate\n      loss_reduction\n      sample_size\n      .metric\n      .estimator\n      mean\n      n\n      std_err\n      .config\n    \n  \n  \n    3\n8\n13\n0.002555174\n0.0003583973\n0.8054182\naccuracy\nbinary\n0.7927507\n75\n0.002301022\nPreprocessor1_Model044\n    3\n8\n13\n0.002555174\n0.0003583973\n0.8054182\nroc_auc\nbinary\n0.8479777\n75\n0.002578232\nPreprocessor1_Model044\n    3\n8\n13\n0.002555174\n0.0003583973\n0.8054182\nsensitivity\nbinary\n0.6046792\n75\n0.006856507\nPreprocessor1_Model044\n    3\n8\n13\n0.002555174\n0.0003583973\n0.8054182\nspecificity\nbinary\n0.9112788\n75\n0.005088285\nPreprocessor1_Model044\n  \n  \n  \n\n\n\n\n\n\nModel Tuning Visualisation\n\n\n\n\n\n\n\n\n\n\n\n7. Finalise the workflow\n\n\nCode\nlast_fit_xgboost_workflow <- xgboost_workflow %>%\n    finalize_workflow(select_best(racing_resamples, \"roc_auc\")) %>%\n    last_fit(promotion_split)\n\n\n# last_fit_xgboost_workflow\n\n# test the fit\ncollect_metrics(last_fit_xgboost_workflow) %>% gt::gt()\n\n\n\n\n\n\n  \n  \n    \n      .metric\n      .estimator\n      .estimate\n      .config\n    \n  \n  \n    accuracy\nbinary\n0.8190045\nPreprocessor1_Model1\n    roc_auc\nbinary\n0.8768303\nPreprocessor1_Model1\n  \n  \n  \n\n\n\n\nCode\n# extract the model workflow for further testing & saving\nfinal_model_workflow <- last_fit_xgboost_workflow %>%\n    extract_workflow()\n\n\n\n\n\n8. Re-assess model performance\n\n\nCode\n# test the model\npred_test <- final_model_workflow %>% \n    predict(promotion_test_tbl) %>%\n    bind_cols(promotion_test_tbl)\n\n# Visualise the performance using a confusion matrix\ncm <- conf_mat(pred_test, promoted, .pred_class)\nautoplot(cm, type = \"heatmap\") %>% \n    plotly::plotly_build()\n\n\n\n\n\n\n\n\n\n\nSave the model\n\n\nCode\n# save the model for future use \nmodel_bundle <- bundle::bundle(final_model_workflow)\nreadr::write_rds(model_bundle, file = \"model_bundle.rds\")\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dmckinnon2022,\n  author = {Adam D McKinnon},\n  title = {Predicting {Promotions} {Through} {Machine} {Learning}},\n  date = {2022-12-30},\n  url = {https://www.adam-d-mckinnon.com//posts/2022-12-30-promotion_prediction},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAdam D McKinnon. 2022. “Predicting Promotions Through Machine\nLearning.” December 30, 2022. https://www.adam-d-mckinnon.com//posts/2022-12-30-promotion_prediction."
  },
  {
    "objectID": "posts/2020-08-04-cluster_analysis/index.html",
    "href": "posts/2020-08-04-cluster_analysis/index.html",
    "title": "A People Analytics Tutorial on Unsupervised Machine Learning",
    "section": "",
    "text": "Image source: Clustering: scikit-learn.org\nWe recently published an article titled “A Beginner’s Guide to Machine Learning for HR Practitioners” where we touched on the three broad types of Machine Learning (ML); reinforcement, supervised, and unsupervised learning. In this follow-up article we will explore unsupervised ML in more depth. We will demonstrate how we use cluster analysis, a form of unsupervised ML, to identify similarities, patterns, and relationships in datasets intelligently (like humans – but faster or more accurately). We have included some practical code written in R. Let’s get started!"
  },
  {
    "objectID": "posts/2020-08-04-cluster_analysis/index.html#cluster-analysis-in-hr",
    "href": "posts/2020-08-04-cluster_analysis/index.html#cluster-analysis-in-hr",
    "title": "A People Analytics Tutorial on Unsupervised Machine Learning",
    "section": "Cluster Analysis in HR",
    "text": "Cluster Analysis in HR\nThe objective we aim to achieve is an understanding of factors associated with employee turnover within our data. To do this, we form clusters based on a set of employee variables (i.e., Features) such as age, marital status, role level, and so on. The clusters help us better understand the many attributes that may be associated with turnover, and whether there are distinct clusters of employees that are more susceptible to turnover. This last insight can facilitate the personalising of the employee experience at scale by determining whether current HR policies are serving the employee clusters identified in the analysis, as opposed to using a one size fits all approach."
  },
  {
    "objectID": "posts/2020-08-04-cluster_analysis/index.html#acknowledgments",
    "href": "posts/2020-08-04-cluster_analysis/index.html#acknowledgments",
    "title": "A People Analytics Tutorial on Unsupervised Machine Learning",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the Analytics In HR (AIHR) website under the title “A Tutorial on People Analytics Using R – Clustering”on June 13, 2020. The original version can be viewed here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "People Analytics Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nExpediting Exploratory Data Analysis\n\n\nExperimenting with different methods to rapidly explore relationships within datasets prior to performing other analytic activities.\n\n\n\nAdam D McKinnon\n\n\nJan 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Promotions Through Machine Learning\n\n\nBuilding an XGBoost model in the Tidymodels ecosystem that predicts whether an employee should be promoted.\n\n\n\nAdam D McKinnon\n\n\nDec 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated assessment of employee data quality using machine learning\n\n\nAn automated, multi-dimensional and scalable approach to monitoring and improving employee data quality using unsupervised machine learning.\n\n\n\nMartha Curioni, Adam D McKinnon\n\n\nJul 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting Future Personnel Requirements Using Machine Learning\n\n\nA practical demonstration of using machine learning to predict personnel requirements based on customer volumes.\n\n\n\nAdam D McKinnon\n\n\nNov 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoing the Distance!\n\n\nA practical guide to measuring distance using Google Maps in R.\n\n\n\nAdam D McKinnon\n\n\nJun 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Impact in HR: A Practical Demonstration\n\n\nA ‘How To’ article on the measurment of differences between groups in HR settings.\n\n\n\nAdam D McKinnon, Sambit Das\n\n\nFeb 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCapturing the Sun!\n\n\nExloring Visualisations: Applying the Sunburst Visualisation in R to Synthesize Complex Relationships.\n\n\n\nAdam D McKinnon\n\n\nFeb 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow Me The Money! Measuring Impact Over Time\n\n\nA “How To” article on the use of Causal Impact Analysis for People Analytics Practitioners.\n\n\n\nAdam D McKinnon\n\n\nJan 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Value of Measuring Employee Skill Data\n\n\nThis article explores the two fundamental considerations when it comes to skills management. First, how do you acquire your employees’ skill data? Second, what practical…\n\n\n\nAdam D McKinnon, Mikaël Wornoo\n\n\nDec 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the Forecast for People Analytics?\n\n\nAn article on the use of Google Trends and the new TimeTK and ModelTime Libraries in R.\n\n\n\nAdam D McKinnon, Monica Ashton\n\n\nOct 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow HR can Apply Network Analysis to Open Data\n\n\nThis article explores how Network Analysis can be applied to a variety of Open data sources to inform people-related decisions inside organisations. Practical use cases…\n\n\n\nAdam D McKinnon, André Vermeij\n\n\nAug 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA People Analytics Tutorial on Unsupervised Machine Learning\n\n\nA “How To” article on the use of Cluster Analysis to understand phenomenon within Organisations.\n\n\n\nAdam D McKinnon, Monica Ashton\n\n\nAug 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Beginner’s Guide to Machine Learning for HR Practitioners\n\n\nAn introductory article intended to demistify Machine Learning (ML)–an important subset of AI–for HR Professionals.\n\n\n\nAdam D McKinnon, Monica Ashton\n\n\nJun 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019\n\n\nIn this article we take a data-driven look at HR Technology, using network analysis. Specifically, we objectively examine the question ’which technologies and vendors are…\n\n\n\nAdam D McKinnon, Leandra Griep\n\n\nMar 16, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my personal blog, coming to you from Melbourne, Australia. I write about the evolving field of People Analytics, mostly through the application of the R language. On this blog site you can find my posts which provide both a philosophical perspective regarding advancements in people analytics, along with practical “How To” projects applying different methods (please learn from my mistakes!)."
  }
]