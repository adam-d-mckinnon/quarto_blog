{"title":"Predicting Promotions Through Machine Learning","markdown":{"yaml":{"title":"Predicting Promotions Through Machine Learning","date":"2022-12-30","description":"Building an XGBoost model in the Tidymodels ecosystem that predicts whether an employee should be promoted.","author":[{"name":"Adam D McKinnon"}],"categories":["Tidymodels","XGBoost","R","Machine Learning","Employee Promotions"],"image":"markus-spiske-QozzJpFZ2lg-unsplash.jpg","title-block-banner":true},"headingText":"Libraries","containsRefs":false,"markdown":"\n\n```{r header, echo=FALSE, code_folding = FALSE, fig.cap=\"Photo by [Possessed Photography](https://unsplash.com/@markusspiske) on [Unsplash](https://unsplash.com/).\", out.width = '100%'}\nknitr::include_graphics(\"markus-spiske-QozzJpFZ2lg-unsplash.jpg\")\n\n```\n<br>\n\n\n```{r loading_libraries}\n\n# data manipulation\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(janitor)\n\n# modelling\nlibrary(tidymodels)\nlibrary(finetune)\nlibrary(bundle)\nlibrary(plotly)\n\n# Processing power\nlibrary(doParallel)\nlibrary(parallelly)\n\n\ntidymodels_prefer()\n\n```\n\n<br>\n\n# Data\n\n```{r loading_data}\n\n# Load Data ----\npromotions_tbl <- readxl::read_excel(path = \"2022_12_15_promotions.xlsx\")\n\n\npromotions_tbl <- promotions_tbl %>% \n    mutate(\n        promoted  = forcats::as_factor(promoted) %>% forcats::fct_relevel(\"promoted\", \"not promoted\")\n    ) %>% \n    mutate_at(.vars = c(\"gender\", \"work_site\", \"management_level\"), .funs = ~ forcats::as_factor(.))\n\n\n\n```\n\n<br>\n\n# Building an ML Model\n\n### 1. Splitting the data\n\n```{r splitting_data}\n\n# Spending the dataset ----\n\nset.seed(836)\npromotion_split     <- initial_split(promotions_tbl, strata = promoted)\npromotion_train_tbl <- training(promotion_split)\npromotion_test_tbl  <- testing(promotion_split)\n\n\nset.seed(234)\npromotion_folds <- bootstraps(promotion_train_tbl, \n                              times = 75, # default is 25 - inflated to accommodate racing method of tuning \n                              strata = promoted)\n\n# check the promotion_folds \n# promotion_folds\n\n\n\n```\n\n<br>\n### 2. Pre-processing the data\n\n```{r data_preprocessing}\n\n# Data Pre-processing ----\nxgboost_recipe <- \n    recipe(formula = promoted ~ ., data = promotion_train_tbl) %>% \n    recipes::update_role(employee_id, new_role = \"id\") %>% \n    step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% \n    step_zv(all_predictors()) \n\n\n# check the recipe\n# xgboost_recipe\n\n```\n\n<br>\n### 3. Create a model specification\n\n```{r model_spec}\n\n# Model Set-up ----\nxgboost_spec <- \n    boost_tree(trees = 1000, \n               tree_depth = tune(), min_n = tune(), \n               loss_reduction = tune(), \n               sample_size = tune(), mtry = tune(),\n               learn_rate = tune()) %>% \n    set_engine(\"xgboost\") %>% \n    set_mode(\"classification\")\n\n\n# check the model specification\n# xgboost_spec\n\n```\n\n<br>\n### 4. Workflow setup\n\n```{r workflow_setup}\n\n\n# Workflow setup\nxgboost_workflow <- \n    workflow() %>% \n    add_recipe(xgboost_recipe) %>% \n    add_model(xgboost_spec) \n\n# Check the workflow\n# xgboost_workflow\n\n\n```\n\n<br>\n### 5. Tuning the model\n\n```{r model_tuning}\n\n# specify the metrics of interest\n# NOTE: The first metric listed will be used for tuning\npromotion_metrics <- metric_set(\n                            roc_auc, \n                            accuracy, \n                            sensitivity, \n                            specificity\n                            )\n\n\n# establish parallel processing based on the number of available cores\ndoParallel::registerDoParallel(cores = parallelly::availableCores())\n\n\nset.seed(826)\nracing_resamples <- finetune::tune_race_anova(\n    xgboost_workflow,\n    resamples = promotion_folds,\n    grid = 100, # cast a wide grid to optimise the results -\n                # works best with many resamples - set earlier to 75\n    metrics = promotion_metrics,\n    control = control_race(\n        verbose_elim = TRUE,\n        save_pred    = TRUE\n        )\n)\n\n\n# racing_resamples\n\n\n\n```\n<br>\n\n### 6. Assess model performance\n\n```{r assess_model}\n\nfirst_model_metrics_tbl <- collect_metrics(racing_resamples)\ntuning_plot <- plotly_build(plot_race(racing_resamples))\n\nxaringanExtra::use_panelset()\n\n```\n\n::: panelset\n::: panel\n[Promotion Metrics]{.panel-name}\n\n```{r echo=FALSE, code_folding = FALSE}\n\nfirst_model_metrics_tbl %>% gt::gt()\n\n```\n:::\n\n::: panel\n[Model Tuning Visualisation]{.panel-name}\n\n```{r echo=FALSE, code_folding = FALSE}\n\ntuning_plot\n\n```\n:::\n:::\n\n<br>\n\n### 7. Finalise the workflow\n\n```{r finalise_workflow}\n\nlast_fit_xgboost_workflow <- xgboost_workflow %>%\n    finalize_workflow(select_best(racing_resamples, \"roc_auc\")) %>%\n    last_fit(promotion_split)\n\n\n# last_fit_xgboost_workflow\n\n# test the fit\ncollect_metrics(last_fit_xgboost_workflow) %>% gt::gt()\n\n# extract the model workflow for further testing & saving\nfinal_model_workflow <- last_fit_xgboost_workflow %>%\n    extract_workflow()\n\n\n```\n\n<br>\n\n### 8. Re-assess model performance\n\n```{r predictions}\n\n# test the model\npred_test <- final_model_workflow %>% \n    predict(promotion_test_tbl) %>%\n    bind_cols(promotion_test_tbl)\n\n# Visualise the performance using a confusion matrix\ncm <- conf_mat(pred_test, promoted, .pred_class)\nautoplot(cm, type = \"heatmap\") %>% \n    plotly::plotly_build()\n\n```\n\n<br>\n\n# Save the model\n\n```{r save_model}\n\n# save the model for future use \nmodel_bundle <- bundle::bundle(final_model_workflow)\nreadr::write_rds(model_bundle, file = \"model_bundle.rds\")\n\n\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../theme.scss"],"toc-depth":3,"toc":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","editor":"visual","theme":"lux","title-block-banner":true,"license":"CC BY","toc-title":"Table of contents","toc-location":"left","citation":true,"title":"Predicting Promotions Through Machine Learning","date":"2022-12-30","description":"Building an XGBoost model in the Tidymodels ecosystem that predicts whether an employee should be promoted.","author":[{"name":"Adam D McKinnon"}],"categories":["Tidymodels","XGBoost","R","Machine Learning","Employee Promotions"],"image":"markus-spiske-QozzJpFZ2lg-unsplash.jpg"},"extensions":{"book":{"multiFile":true}}}}}