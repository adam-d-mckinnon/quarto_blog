[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "People Analytics Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nExpediting Exploratory Data Analysis\n\n\n\n\n\n\n\nPPSR\n\n\nCorrelation Funnel\n\n\nDataExplorer\n\n\nR\n\n\n\n\nExperimenting with different methods to rapidly explore relationships within datasets prior to performing other analytic activities.\n\n\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Promotions Through Machine Learning\n\n\n\n\n\n\n\nTidymodels\n\n\nXGBoost\n\n\nR\n\n\nMachine Learning\n\n\nEmployee Promotions\n\n\n\n\nBuilding an XGBoost model in the Tidymodels ecosystem that predicts whether an employee should be promoted.\n\n\n\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAutomated assessment of employee data quality using machine learning\n\n\n\n\n\n\n\nPython\n\n\nH2o\n\n\nIsolation Forests\n\n\nMachine Learning\n\n\nData Quality\n\n\n\n\nAn automated, multi-dimensional and scalable approach to monitoring and improving employee data quality using unsupervised machine learning.\n\n\n\n\n\n\nJul 21, 2022\n\n\nMartha Curioni, Adam D McKinnon\n\n\n\n\n\n\n  \n\n\n\n\nGoing the Distance!\n\n\n\n\n\n\n\nGoogle Maps\n\n\ngoogleway\n\n\necharts4r\n\n\nR\n\n\n\n\nA practical guide to measuring distance using Google Maps in R.\n\n\n\n\n\n\nJun 6, 2021\n\n\nAdam D McKinnon\n\n\n\n\n\n\n  \n\n\n\n\nHow HR can Apply Network Analysis to Open Data\n\n\n\n\n\n\n\nNetwork Analysis\n\n\nOpen Data\n\n\nHR\n\n\n\n\nThis article explores how Network Analysis can be applied to a variety of Open data sources to inform people-related decisions inside organisations. Practical use cases applying network analysis to open data are provided.\n\n\n\n\n\n\nAug 27, 2020\n\n\nAdam D McKinnon, André Vermeij\n\n\n\n\n\n\n  \n\n\n\n\nA Beginner’s Guide to Machine Learning for HR Practitioners\n\n\n\n\n\n\n\nArtificial Intelligence\n\n\nMachine Learning\n\n\nHR\n\n\n\n\nAn introductory article intended to demistify Machine Learning (ML)–an important subset of AI–for HR Professionals.\n\n\n\n\n\n\nJun 19, 2020\n\n\nAdam D McKinnon, Monica Ashton\n\n\n\n\n\n\n  \n\n\n\n\n4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019\n\n\n\n\n\n\n\nHR Tech\n\n\nNetwork Analysis\n\n\nKenelyze\n\n\n\n\nIn this article we take a data-driven look at HR Technology, using network analysis. Specifically, we objectively examine the question ‘which technologies and vendors are influential in the HR Tech landscape?’\n\n\n\n\n\n\nMar 16, 2020\n\n\nAdam D McKinnon, Leandra Griep\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dmckinnon2023,\n  author = {Adam D McKinnon and Harlow Malloc},\n  title = {Post {With} {Code}},\n  date = {2023-01-08},\n  url = {https://www.adam-d-mckinnon.com//posts/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAdam D McKinnon, and Harlow Malloc. 2023. “Post With Code.”\nJanuary 8, 2023. https://www.adam-d-mckinnon.com//posts/post-with-code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my personal blog, coming to you from Melbourne, Australia. I write about the evolving field of People Analytics, mostly through the application of the R language. On this blog site you can find my posts which provide both a philosophical perspective regarding advancements in people analytics, along with practical “How To” projects applying different methods (please learn from my mistakes!)."
  },
  {
    "objectID": "posts/2023-01-03-exploratory_analysis/index.html",
    "href": "posts/2023-01-03-exploratory_analysis/index.html",
    "title": "Expediting Exploratory Data Analysis",
    "section": "",
    "text": "Start with the required libraries, and then load some data.\n\n\nCode\n# data manipulation\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Data Exploration\nlibrary(ppsr)\nlibrary(correlationfunnel)\nlibrary(DataExplorer)\n\n# Visualisation addition\nlibrary(plotly)\n\n\n# Load Data ----\npromotions_tbl <- readr::read_csv(file = \"train.csv\") %>% \n    janitor::clean_names()\n\n# reduce the dataset size\ncleaned_promotions_tbl <-\npromotions_tbl %>%\n    tidyr::drop_na() %>% \n    dplyr::mutate(\n        is_promoted = as.character(is_promoted),\n        is_promoted = if_else(is_promoted==1, \"Yes\", \"No\") %>% as.factor()\n    )\n\n\n\n\n1. Predictive Power Score\n\n\nCode\ncleaned_promotions_tbl %>%\n    select(-employee_id) %>%\n    visualize_pps(y = 'is_promoted', do_parallel = TRUE)\n\n\n\n\n\n\n\n\n\n\n2. Correlation Funnel\n\n\nCode\ncleaned_promotions_tbl %>% \n    select(-employee_id) %>% \n    binarize() %>% \n    correlate(target = is_promoted__Yes) %>% \n    plot_correlation_funnel(interactive = TRUE) %>% \n    plotly::config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n3. DataExplorer\n\n\n\n\n\n\n\nDefault Heatmap\n\n\nCode\ncorr_plot <- cleaned_promotions_tbl %>% \n    select(-employee_id) %>% \n    DataExplorer::plot_correlation(\n        theme_config = list(\n            legend.position = \"none\",\n            axis.text.x     = element_text(angle = 90)\n                )\n    )\n\n\n\n\n\n\n\nInteractive Heatmap\n\n\nCode\ncorr_plot$data$value <- round(corr_plot$data$value, digits = 2)\n\nplotly::plotly_build(corr_plot) %>% \n    plotly::layout(width = 700, height = 700) %>% \n    plotly::config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{2023,\n  author = {},\n  title = {Expediting {Exploratory} {Data} {Analysis}},\n  date = {2023-01-03},\n  url = {https://www.adam-d-mckinnon.com//posts/2023-01-03-exploratory_analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Expediting Exploratory Data Analysis.” 2023. January 3,\n2023. https://www.adam-d-mckinnon.com//posts/2023-01-03-exploratory_analysis."
  },
  {
    "objectID": "posts/2022-12-30-promotion_prediction/index.html",
    "href": "posts/2022-12-30-promotion_prediction/index.html",
    "title": "Predicting Promotions Through Machine Learning",
    "section": "",
    "text": "Photo by Possessed Photography on Unsplash.\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{2022,\n  author = {},\n  title = {Predicting {Promotions} {Through} {Machine} {Learning}},\n  date = {2022-12-30},\n  url = {https://www.adam-d-mckinnon.com//posts/2022-12-30-promotion_prediction},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Predicting Promotions Through Machine Learning.” 2022.\nDecember 30, 2022. https://www.adam-d-mckinnon.com//posts/2022-12-30-promotion_prediction."
  },
  {
    "objectID": "posts/2022-07-21-isolation_forests/index.html",
    "href": "posts/2022-07-21-isolation_forests/index.html",
    "title": "Automated assessment of employee data quality using machine learning",
    "section": "",
    "text": "Photo by Mika Baumeister on Unsplash.\n\n\n\n\n\nIntroduction\nThe topic of data quality is like that of I.T. services generally… you only ever hear about it when there’s a problem! There’s an implicit assumption among stakeholders receiving the findings of people analytics initiatives that your data is “good”. Failure to observe this assumption of high data quality can significantly (and rapidly) undermine the credibility of findings, irrespective of how small the data quality deviation!\nFrom an analytical perspective, the quality of your insights can potentially be limited by flawed data. As the saying goes “garbage in, garbage out”. Coupled with stakeholder expectations, it becomes increasingly important that organisations invest time and energy in the ongoing assessment and curation of quality data to maximise the influence of data driven decision making in organisations.\nAt the same time, monitoring data quality in HR can be immensely time consuming, expensive, simplistic in execution (i.e., simple evaluation criteria such as age ranges), highly repetitive, and utterly devoid of ANY professional enjoyment! To overcome these shortcomings, we have developed an automated, multi-dimensional and scalable approach to data quality evaluation using unsupervised machine learning—Isolation Forests.\n\n\nThe Process\nWe used Isolation Forests, an unsupervised machine learning approach to scan employee data for potentially anomalous[1] records.\nOur code can be viewed at this GitHub account, which followed the following process steps:\n\nIngested personnel data and loaded the appropriate libraries. In our example, we used the IBM HR Attrition dataset, then installed the H2o and Pandas libraries.\nDefined the dataset columns to include in the data quality assessment. Trained an Isolation Forest algorithm on the data. The Isolation Forest algorithm partitioned data through a forest of decision trees. Each split of the data was made randomly and the number of splits required to isolate a record indicated whether or not the record was considered anomalous. When a forest of random trees collectively produced shorter path lengths for particular records, they were highly likely to be considered anomalies.\nUsing the trained Isolation Forest model, we then predicted which records we believed to be anomalous. Of those records predicted to have data quality issues we then narrowed down to those with the highest likelihood of being anomalous using characteristics from the trained model.\nThe model has two levels of output interpretation:\n\nGlobal – which dataset columns are most often related to data quality errors. At best this is interesting in identifying potentially systemic errors, which may be a catalyst for more controlled data entry parameters.\nLocal – for any given record why is it deemed an anomaly. It is this level that is most valuable in directly resolving data quality issues, and therefore, was the focus of our approach.\n\nFor those records deemed highly anomalous (our subset), we then trained a simple Random Forest model to predict the anomaly flag(s). This process was repeated multiple times for each record to increase our certainty in identifying those variables contributing to its anomaly status. Note: We kept the model simple to make identification (and interpretation) of the combination of variables contributing to each records anomaly status easy to perform. In short, we wanted the output interpretable by a wide audience, not just those familiar with machine learning outputs.\nThe generated output (see excerpt of output in image below) has two fields: 1. A unique identifier, and 2. A list of variables contributing to each records anomaly status. The unique identifier, in our case the unique Employee Number taken from the original dataset, is critical to enabling identification and access to the original record that requires review. The second column has a ranked list (ranked from most important to least) showing the variables “collectively” contributing to the anomaly status. For example, when inspecting EmployeeNumber 58’s record we see a Monthly Income only four times their DailyRate (possibly incorrect), and that the individual has been with the company 2 years, had their last promotion 2 years ago, and has 2 years in their current role, which collectively cannot all be correct–the algorithm suggests reviewing YearsInCurrentRole.\n\n\n\n\n\n\nExample Output.\n\n\n\n\n\n\nConclusion\nUnsupervised Machine Learning algorithms such as Isolation Forests can be an excellent way of automating and scaling the review of data to monitor for quality concerns. The approach lends itself to those datasets that have a reasonable level of data quality and that are looking to make further improvements.\nThe major advantage of this approach is that the Isolation Forest can identify a record as anomalous, despite no one variable in the record being out of acceptable limits. Instead, the algorithm assesses the combination of multiple variables to determine if the combination makes it seem anomalous. This is like having human intelligence review each record quickly, at scale, and without destroying anyone’s job satisfaction!\nWe propose that this approach has the potential to significantly reduce time spent on direct data quality evaluations, which has considerable direct benefits (i.e., better quality data, more representative analyses and interpretation, etc.) as well as indirect benefits (i.e., time can be spent on other value-add initiatives that can only be done by humans!). This is particularly true for smaller teams, where the need to do more with less is greatest.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{curioni2022,\n  author = {Martha Curioni and Adam D McKinnon},\n  title = {Automated Assessment of Employee Data Quality Using Machine\n    Learning},\n  date = {2022-07-21},\n  url = {https://www.adam-d-mckinnon.com//posts/2022-07-21-isolation_forests},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMartha Curioni, and Adam D McKinnon. 2022. “Automated Assessment\nof Employee Data Quality Using Machine Learning.” July 21, 2022.\nhttps://www.adam-d-mckinnon.com//posts/2022-07-21-isolation_forests."
  },
  {
    "objectID": "posts/2020--8-13-network_analysis/index.html",
    "href": "posts/2020--8-13-network_analysis/index.html",
    "title": "Expediting Exploratory Data Analysis",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dmckinnon2023,\n  author = {Adam D McKinnon and Adam D McKinnon and André Vermeij},\n  title = {Expediting {Exploratory} {Data} {Analysis}},\n  date = {2023-01-03},\n  url = {https://www.adam-d-mckinnon.com//posts/2020--8-13-network_analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAdam D McKinnon, Adam D McKinnon, and André Vermeij. 2023.\n“Expediting Exploratory Data Analysis.” January 3, 2023. https://www.adam-d-mckinnon.com//posts/2020--8-13-network_analysis."
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html",
    "href": "posts/2020-08-27-network_analysis/index.html",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "",
    "text": "Photo by Matt Palmer on Unsplash.\nThis article explores:"
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html#combining-datasets-to-add-even-more-value",
    "href": "posts/2020-08-27-network_analysis/index.html#combining-datasets-to-add-even-more-value",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "Combining datasets to add even more value",
    "text": "Combining datasets to add even more value\nAnalyses based on open data can become particularly powerful when datasets are combined in a meaningful way. If you’re working with various types of documents (let’s say patents and journal articles), you can use metadata which is common to both datasets to link them together and generate combined network overviews. Document summaries/abstracts and author information are good examples of columns which can be used to link datasets."
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html#acknowledgments",
    "href": "posts/2020-08-27-network_analysis/index.html#acknowledgments",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the myHRfuture website under the title “How HR can Apply Network Analysis to Open Data” on August 13, 2020."
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html#why-network-analysis-could-be-more-valuable-than-organisational-network-analysis",
    "href": "posts/2020-08-27-network_analysis/index.html#why-network-analysis-could-be-more-valuable-than-organisational-network-analysis",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "Why “Network Analysis” could be more valuable than “Organisational Network Analysis”",
    "text": "Why “Network Analysis” could be more valuable than “Organisational Network Analysis”\nOrganisational Network Analysis (ONA) has received considerable attention in recent years within People Analytics circles globally, as illustrated by the public discussion and proliferation of companies specialising in this method. There is an appetite to learn and do more, which also speaks to the quality and actionability of insights generated. Typical ONA benefits include, for example, improved collaboration through a better view on intra- and inter-team collaboration and the identification of key individuals acting as hubs and connectors for efficient message broadcasting during change management initiatives.\nONA traditionally denotes the analysis of social interactions internally to inform organisational decision making. We have seen exceptional examples of this work published in the public domain from people like Michael Arena, Rob Cross, Manish Goel and many others. These are important examples; however, we could be imposing a false barrier upon practitioners by focusing too much on the ‘O’ for Organisational. While the term ONA is directionaly accurate, it limits our thinking and actions regarding the breadth of possible applications for network analysis. In other words, we are limiting the data sources we could use and the questions we could answer by only looking internally, within the organisation.\nIn academic circles, this work is often referred to as “Social Network Analysis” (SNA) and can be found in a body of literature that spans decades. SNA broadens the perspective of the discussion by using network analysis to understand social interactions. In doing so, it has generated a wealth of insights around social relationships in the fields of sociology, psychology and anthropology. There are however, once again, limitations. The field is limited to solely represent social connectedness (connections between individuals, teams, or entire companies), as opposed to simply connectedness (connections between any other type of data point – for example documents, projects, skills, knowledge areas, etc.). In a society of ever-increasing digitisation and data generation, insights into this broader concept of connectedness are of increasing value.\nNetwork analysis can be applied to a variety of data types and data sources in a bid to understand this kind of broad connectedness between data, as opposed to just connections between people. Since the early 2000s, the broader scientific fields of Network Science and Complex Networks have been focusing on applying large-scale network analysis in a wide variety of other domains as well, including, for example, biology (cellular networks), neurology (brain networks), economics (corporate governance, trade networks) and finance (transaction networks, fraud detection).\nThe People Analytics field should therefore embrace the name “Network Analysis” as opposed to “Organisational Network Analysis” in order to shed the limitations imposed by the domain specific names that have previously dominated public discussion in the field, and subsequently broaden the opportunity for People Analytics practitioners to generate unique insights."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "",
    "text": "Image by Pietro Jeng on Unsplash.\nWhen you hear Artificial Intelligence (AI) the first thing that comes to mind are robots; in particular, the Steven Spielberg movie titled A.I. where a robot child is built that can love and behave just like a real human. This idea appears to be closer to a dream than reality. Truth is, AI is more ubiquitous than we might think. It ranges from self-driving cars, movie recommendations on Netflix, e-mail spam detection to voice-controlled assistants such as Apple’s SIRI. The fact is that AI is already present across many businesses and various industries, as is shown in the figure below (Note the low adoption rate in Human Resources).\nStill, evidence suggests that HR departments remain unable to seize the multitude of opportunities associated with AI. In part, what may be required to accelerate the adoption of AI is educational content directed at HR Professionals, not data scientists. Thus, we offer this brief guide to Machine Learning (ML), an important subset of AI, with the intent to demystify ML and make it tangible."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-reinforcement-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-reinforcement-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "1. What is Reinforcement Learning?",
    "text": "1. What is Reinforcement Learning?\nReinforcement Learning is probably best known through IBM’s Deep Blue computer, a “robot” that learned how to play chess and beat the human world champion.\nReinforcement Learning is a type of technique that enables an algorithm to learn by trial and error, using feedback from its own actions and experiences. Much like Pavlov and his dog, Reinforcement Learning involves rewarding decisions that lead to success and penalizing decisions that lead to anything other than success—ultimately making the algorithm more intelligent in the process.\nExamples of reinforcement learning applied in HR are a bit lean, though are most prevalent in areas such as education (i.e., applying content based on the progress of the student), finance and investment (i.e., advanced forecasting), supply chain operations (i.e., robots fulfilling orders in a warehouse), traffic flow optimization, and healthcare (i.e., accurate classification of biopsy images)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-supervised-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-supervised-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "2. What is Supervised Learning?",
    "text": "2. What is Supervised Learning?\nThe most common forms of ML across industries, and specifically the HR domain, are Supervised Learning, followed by Unsupervised Learning.\nIn Supervised Learning, we try to predict an outcome, such as whether an employee will leave the company, the risk of an employee being injured, or the ideal starting salary of a new employee.\nTo make predictions we need different input variables (i.e., variables are called “Features” by data scientists). Our input features are only limited to our imagination (i.e., what we think will be important), what data we can get our hands-on, or what data we can create (e.g., by knowing where someone works and where they live we can create a variable focused on employee commute distance).  ### An example: Supervised Learning informing Employee Turnover\nLet’s examine a more detailed example of Supervised Learning—predicting who will leave an organization. Imagine that 1 in 5 new recruits leaves an organization in their first 12 months of tenure. To prevent such turnover, we could build a supervised learning model that predicts the likelihood of new starters leaving, so that our HR and managerial colleagues could intervene.\nIn this example, the model outcome being predicted is turnover risk, and the features used to predict turnover risk could include an employees’ demographic and employment characteristics (e.g., age, education level, role level, pay relative to market, month of employment, presence of development plans, and so on.).\nAssuming such a model was highly accurate, it would enable us to understand turnover among our new starter population from three angles.\n\nFirstly, what are the factors most influential in predicting turnover among our population. An example of such a model output is presented in the figure below, which illustrates whether a feature prevents turnover (green bars) or promotes turnover (red lines), and the relative importance of each feature in predicting turnover (i.e. longer lines denote more importance).\nSecondly, the model also rates the likelihood of each new starter leaving the company, enabling focused intervention (i.e. the risk that Adam will leave in his first 12 months).\nThirdly, the model identifies the features preventing or promoting turnover risk for each employee. This individualized output can enable HR professionals to take informed and personalized action, regardless of whether they personally know each employee.\n\n\n\n\n\n\nExample output from a model that predicts employee turnover risk. The plot describes the relative importance and directional influence of features in the model. Red bars represent features that contribute to employee turnover risk, while green bars represent features preventing turnover risk.\n\n\n\n\n\nA supervised learning model used to predict employee turnover among new starters has the potential to reduce notable costs, including financial (e.g., separation, vacancy, recruitment, training, and replacement) reputational (e.g., eroding an EVP and/or reducing candidate appeal) and productivity-related (e.g., on average organizations invest between four weeks and three months training new employees). Some of these costs can be readily quantified so that we can identify organizational savings based on prevented turnover (e.g., preventing 2 in 10 resignations saves $xxx)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-unsupervised-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-unsupervised-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "3. What is Unsupervised Learning?",
    "text": "3. What is Unsupervised Learning?\nUnlike Supervised Learning where we are trying to predict an outcome, Unsupervised Learning analyzes many variables simultaneously to identify similarities, patterns or relationships in the data. Unsupervised Learning is more about understanding what’s in the data. The two most common uses of unsupervised learning are focused on:\n\nClustering: automatically splitting the dataset into groups based on similarities among the features analyzed. Classically applied to consumers, but equally relevant to organizations, whereby we understand our employee segments (i.e., clusters) and determine whether our HR policies serve the segments.\nAssociation mining: identifies sets of variables that often occur together in your dataset. For example, identifying injury patterns among workers at specific sites.  ### An example: Unsupervised Learning informing Employee Turnover\n\nCluster Analysis, the most famous form of unsupervised learning, can also help us better understand employee attrition. This approach can help group employees based on similar features (e.g., location, tenure, nationality, education level, age, performance level, etc.).\nThe figure below depicts the results of an analysis of the employee’s demographic features. Multiple demographic features are first reduced to two dimensions using a method called Manifold Learning (another non-supervised method), and these two new dimensions are then clustered using a method called T-SNE. The figure below shows us how the employees can be grouped together, in this case, twelve clusters, based on their demographic features.\n\n\n\n\n\nAn example cluster analysis output when applied to employee features.\n\n\n\n\n Once grouped into clusters, the next step is to determine the risk of turnover for each group. Moreover, it is interesting to identify if there are some shared risk factors, practically indicating that employees within a cluster are experiencing the workplace in a similar way.\nThis last insight is of considerable practical significance, as it may help us tailor interventions that target specific employee clusters, thereby delivering maximum impact (i.e., retaining employees and reducing turnover costs) and return on our investment (i.e. for every $ spent we generated $xxx in savings from reduced turnover)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#acknowledgments",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#acknowledgments",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the Analytics In HR (AIHR) website under the title “A Beginner’s Guide to Machine Learning for HR Practitioners” on June 8th, 2020."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html",
    "href": "posts/2020-03-16-hrtechanalysis/index.html",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "",
    "text": "Photo by JJ Ying on Unsplash."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#data-collection",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#data-collection",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Data collection",
    "text": "Data collection\nWe obtained data from the 2019 Unleash Conference in London, specifically the 35 start-ups and 64 sponsors that were present at the conference. Conference attendees were provided a list of company names and their “tags”, which were descriptors of the technology or HR domain that characterize the respective companies."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#network-analyses-how-we-did-what-we-did",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#network-analyses-how-we-did-what-we-did",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Network Analyses — How we did what we did?",
    "text": "Network Analyses — How we did what we did?\nWe examined the tags using network analysis, in a technology called Kenelyze (kudos to our partner André Vermeij).\nThe current network examines the “influence” of HR tech among those vendors present at the 2019 Unleash Conference in London. Network Nodes were the company Tags, and company name was included as a node attribute. The network was undirected and displayed using the Force Atlas 2 layout. Nodes (i.e. Tags) were sized according to the number of times the Tag was used to describe an organization, and coloured according to the PageRank score (i.e. influence measure). The network is presented below.\n \n\nNetwork Output of Analysis of HR Tech at 2019 Unleash Conference in London"
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#data-interpretation-what-did-we-discover-what-are-main-insights",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#data-interpretation-what-did-we-discover-what-are-main-insights",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Data interpretation: What did we discover? / What are main insights?",
    "text": "Data interpretation: What did we discover? / What are main insights?"
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#next-steps",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#next-steps",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Next Steps",
    "text": "Next Steps\nThe analysis provides a data driven exploration of the HR Tech landscape present at Unleash London in 2019. While the dataset does not claim to capture the universe of HR Tech, it does provide an interesting perspective of available technologies and how these technologies relate to one another. We encourage further exploration of the interactive network—reflect on your own hypotheses, look at the edges for new trends, examine the network structure, and consider what it might mean for the future. To make the article digestible we intentionally did not address many relationships in the network.\nWe welcome your comments, particularly additional interpretation of the network, that may generate insights and practical value for other readers. Stay tuned! We may look to squeeze further insights from this analysis (e.g. different algorithms, node configurations, etc.)."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#what-are-prominent-technologies-in-the-hr-tech-landscape",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#what-are-prominent-technologies-in-the-hr-tech-landscape",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "What are prominent technologies in the HR Tech landscape?",
    "text": "What are prominent technologies in the HR Tech landscape?\n\n\n1. Talent acquisition is a main influencer\nThe most frequent Tag, most connected to other Tags, and most influential was “Talent Acquisition (incl. Referrals)”. It is perhaps unsurprising that Talent Acquisition is both a common and influential domain in HR Technology. This may in part reflect the discrete nature of the recruiting process, characterized by a definitive start and end point, and a clear, binary success measure—hired or not hired. In addition, Talent Acquisition also lends itself to performance measurement, evidenced by metrics such as source of hire, time to hire, applicants per hire, cost per hire, offer acceptance per hire, etc. All discrete metrics that the technology domain has progressively digitized. Good news for Talent Acquisition functions, what about the rest of us?\n\n\n\n2. People Analytics and Employee Experience are “up and coming”\nFollowing Talent Acquisition the next most frequent and influential Tags included Employee Engagement, Learning & Development, People Analytics, Talent Management, and Employee Experience (EX). These Tags reflect what we interpret as a mix of established HR domains (e.g. Employee Engagement, Learning & Development, Talent Management, HR Transformation), and “up and coming” (e.g. People Analytics and EX). The latter two topics have experienced a meteoric rise in prominence in recent years. However, both People Analytics and EX we anecdotally know to be technology related, and both benefit from a historical connection to data collection network nodes such as “Employee Engagement” and “Surveys (incl. Employee Listening)”.\nWe note two points about People Analytics and EX—first, their collective network metrics and second, the various companies that identify themselves with these two capabilities (i.e. click on nodes to see company listings). To us the network metrics and network position suggest that People Analytics is currently more influential (e.g. frequency, degree, PageRank), more central to the network (i.e. position), and consequently likely to play a more significant role in connecting technologies in the HR Tech landscape in the immediate future. Longer term, only time and repeated assessment of the market will tell.\nSecondly, the companies identifying themselves with these two capabilities are varied! Our interpretation of this variation is that the terms People Analytics and EX (also applicable to “Workforce Planning”) mean many things to different people, perhaps driven by maturity of practice in the consumer population. Alternatively, it may also represent future use cases that aren’t currently mainstream, or the potential for partnership or acquisition among technology vendors. Finally, it may represent opportunistic thinking by marketing professionals!\n\n\n\n3. Skills are the new currency\nA special call out to the topic of Skills Matching, which we at Merck KGaA have invested in during 2019, and will further explore in 2020. As evidenced in the network, we believe there is real world connectivity between Skills Matching and domains such as People Analytics (our team currently owns the Skills Matching topic at Merck KGaA), Strategic Workforce Planning (i.e. bridging the current with the future), Talent Management, Learning & Development (i.e. to get from the present to the future what training interventions are required), Talent Sourcing (incl. Referrals), and Performance Management (i.e. among other things a useful source of data to inform Skills Matching!). As referenced in recent predictions for 2020 by a variety of authors (e.g. David Green, Visier, Bersin, etc.), we believe that “Skills are the new currency”! Kudos to Ian Bailie for one of our favourite stakeholder pitch lines.\n\n\n\n4. Social Networks appear niche\nTech Tags that appeared less frequently than expected, at least considering public hype, included Digital HR and Social Networks. This may simply reflect the companies present at Unleash 2019. However, it may also be suggestive of the niche nature of this technology / approach, and the slow rate of adoption among companies due to various reasons (e.g. data privacy implications, prevalence of skills, lack of available use cases, etc.)."
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html",
    "href": "posts/2021-06-06-going_the_distance/index.html",
    "title": "Going the Distance!",
    "section": "",
    "text": "Photo by Émile Séguin on Unsplash."
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html#calling-google-maps",
    "href": "posts/2021-06-06-going_the_distance/index.html#calling-google-maps",
    "title": "Going the Distance!",
    "section": "1. Calling Google Maps",
    "text": "1. Calling Google Maps\nTo use Google Maps you will need three things:\n\nAddress data. We begin by loading some fictitious address data provided by the Victorian State Government – School addresses from 2015.\nWorkplace addresses, both an old workplace address and a new workplace address. For this example, I am using the following two addresses from Victoria, Australia:\n\n\n\nOld workplace address: 154 High St, Ashburton VIC 3147, Australia (Ashburton Public Library); and\n\n\n\n\nNew workplace address: Spring St, East Melbourne VIC 3002, Australia (Victorian Parliament Building).\n\n\n\nA Google Maps API key, which can be set up on the Google Maps Developer Site. The Google Maps service has a free usage quota. To access Google Maps we will use the googleway library in R.\n\nWith all three pieces ready, we will then call Google Maps using the googleway::google_distance function. We will do this for two modes of transit:\n\nPublic Transport (called “Transit”) &\nCar.\n\n\n\nCode\n# Libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(stringr)\n\nlibrary(googleway)\nlibrary(purrr)\nlibrary(data.table)\nlibrary(echarts4r)\nlibrary(reactable)\n\n\n\n# # Set API key ----\nkey = my_api_key # enter your API key here\n  \n# Import Data ----\noriginal_locations_tbl <- readr::read_csv(file =\n\"http://www.education.vic.gov.au/Documents/about/research/datavic/dv165-allschoolslocationlist2015.csv\") %>% \n                          janitor::clean_names()\n\n\n\n# limit the address data to schools in the Greater Melbourne local government area's\ncouncils <- c(\"Bayside (C)\", \"Port Phillip (C)\", \"Stonnington (C)\", \"Casey (C)\", \"Melbourne (C)\", \n              \"Frankston (C)\", \"Glen Eira (C)\", \"Monash (C)\", \"Yarra (C)\", \"Moonee Valley (C)\")\n\n\n# create an address dataset\naddresses_tbl <- original_locations_tbl %>% \n  \n  # create and format the home address field, and create the old and new workplace addresses\n  dplyr::mutate(\n    home_address = base::paste0(address_line_1, \", \", address_town, \" \", address_state, \" \",address_postcode),\n    old_work_address = \"154 High St, Ashburton VIC 3147\",\n    new_work_address = \"Spring St, East Melbourne VIC 3002\",\n    employee         = paste0(\"Employee \", row_number())  \n    ) %>% \n  \n  # only include addresses from areas around Melbourne\n  dplyr::filter(lga_name %in% councils) %>% \n  \n  # randomly select 100 records\n  dplyr::sample_n(100) %>% \n  \n  dplyr::select(employee, home_address, old_work_address, new_work_address)\n  \n\n\n# check the dataset\n# head(addresses_tbl)\n\n\n# call Google Maps using googleway to calculate the distance and time for the old and new workplace locations\n# the calculations are repeated for both public transport and car modes of transport\ncommute_tbl <- addresses_tbl %>%\n\n  dplyr::mutate(\n        old_transit_calculations = purrr::map2(.x = home_address,\n                                               .y = old_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"transit\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        new_transit_calculations = purrr::map2(.x = home_address,\n                                               .y = new_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"transit\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        old_car_calculations     = purrr::map2(.x = home_address,\n                                               .y = old_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"driving\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        new_car_calculations     = purrr::map2(.x = home_address,\n                                               .y = new_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"driving\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE))\n\n    )"
  }
]