---
title: 'Assessing Bias in ML Models'
date: 2023-04-17
description: "Assessing the presence of bias against minority groups in Machine Learning models to enhance their ethical strength and equality."
author: 
    - name: Adam D McKinnon
categories: [Bias, Tidymodels, Fairmodels, R]
image: "ai_models_stage.webp"
title-block-banner: true
draft: false
---

```{r header, echo=FALSE, code_folding = FALSE, out.width = '100%'}
knitr::include_graphics("ai_models_stage.webp")

```

<br>

# Introduction

Model explainability is a critical component of machine learning (ML) model building. [**Ludek Stehlik**](https://www.linkedin.com/in/ludekstehlik/), a great contributor in the people analytics community, recently published an excellent [article](https://blog-about-people-analytics.netlify.app/posts/2023-04-13-interpretable-ml/) on the use of the [**ModelStudio**](https://modelstudio.drwhy.ai) package (R ecosystem) for explaining ML models. Inspired by Ludek's blog, I've created the following blog on the use of the [**Fairmodels**](https://fairmodels.drwhy.ai) package in R.

<br>

[![](fairmodels_logo.png){fig-align="center" width="200"}](https://modeloriented.github.io/fairmodels/index.html)

<br>

Fairmodels, also built by the developers of ModelStudio, is intended to support assessing, visualising and mitigating the presence of bias in ML models.

<br>

## Setup

We'll begin by loading the required libraries and loading the dataset. The dataset is the Wage data from the ISLR package, and includes variables such as Age, Marital Status, Education, Region, Job Class, Health Rating, Health Insurance, Race and Wage. To provide context for the article, I've calculated the average Wage by Race (spoiler alert!). It's important to see the potential bias in Wage by Race prior to building the ML model that will predict wages.

<br>

```{r set_up}

# Load required libraries
library(tidyverse) # data manipulation
library(tidymodels) # model building

library(DALEX) # model explainer
library(DALEXtra) # model explainer with tidymodels functionality
library(fairmodels) # model fairness check


# Wage data from the ISLR package
wage_tbl <- readr::read_csv(file = "dataset-37830.csv") |> 
            janitor::clean_names()


# get the average wage by race
wage_tbl |> group_by(race) |> summarise(mean_wage = mean(wage))

```

<br><br>

## Model Preparation

The model preparation involves the following steps:

1.  Convert all character fields to factors and remove redundant fields;

2.  Split the cleaned dataset into Training & Test datasets;

3.  Create a simple recipe for the model building process. More could be done in terms of pre-processing and feature engineering. However, as the focus is on Fairness checks, a simple model will be built to expedite the process.

It is important to note that the Race variable was left in the dataset, but updated to be an "id" field. This ensures that the variable is accessible when needed in our next step, but not utilised in the model building process; and

4.  The values from the "Protected" variable (i.e., Race) are pulled out from the processed Training dataset and reserved for later use in the Fairness check. This is a critical component to enable the Fairness check.

<br>

```{r data_cleaning}


wage_cleaned_tbl <- wage_tbl %>%
    mutate_if(is.character, ~as.factor(.)) |> # update character fields to factors for modelling
    select(-c(logwage, sex)) # Remove unnecessary columns


# Split the dataset into training and testing sets
set.seed(123)
data_split <- initial_split(wage_cleaned_tbl, prop = 0.8)
wage_train_tbl <- training(data_split)
wage_test_tbl <- testing(data_split)


# Create a recipe for preprocessing
wage_recipe <- recipe(wage ~ ., data = wage_train_tbl) |> 
    recipes::update_role(race, new_role = "id")


# save out the protected variable ("Race") for later reference for Fairness checking
protected <- bake(wage_recipe |> prep(), new_data = wage_train_tbl) |> select(race) |> pull()

```

<br><br>

## Model Specification

A simple Random Forest model is built and then used, in combination with the recipe, in a Tidymodels Workflow, which is subsequently fit to the training dataset. The Workflow is then used to predict unseen values in the Test dataset, which can be compared to the actual values to "test" the quality of the model, in this case using the RMSE or Root Mean Square Error.

RMSE helps us measure the performance of a regression-based machine learning model by calculating the average difference between predicted and actual values. A lower RMSE means that the model's predictions are more accurate, just as closer dart throws to the bullseye mean you're a better dart player.

<br>

```{r modeling}

# Define the model specification
rf_spec <- rand_forest(trees = 1000) |> 
    set_engine("ranger") |> 
    set_mode("regression") 
    

# Create a workflow
wage_workflow <- workflow() |> 
    add_recipe(wage_recipe) |> 
    add_model(rf_spec)


# Train the model
wage_fit <- fit(wage_workflow, wage_train_tbl)


# Make predictions on the test set
wage_test_pred <- predict(wage_fit, wage_test_tbl) |> 
    bind_cols(wage_test_tbl)


# Evaluate the model performance
wage_metrics <- metric_set(rmse)
wage_results <- wage_metrics(wage_test_pred, truth = wage, estimate = .pred)


print(wage_results)


```

<br><br>

## Model Explainer & Fairness Check

First we create the DALEX-based explainer object, which draws upon the fitted Tidymodels Workflow, the training data and our predicted variable, in this instance, Wage. We then use the explainer object to perform a fairness check of the Wage predictions. As the prediction is a value, and not a probability of classification, we use the fairness_check_regression function from Fairmodels.

It is important to note that the creators of the Fairmodels package consider this funtion to be experimental. The documentation on this function states:

> "*the metrics in use are independence, separation, and sufficiency. The intuition behind this method is that the closer to 1 the metrics are the better. When all metrics are close to 1 then it means that from the perspective of a predictive model there are no meaningful differences between subgroups*."

Printing the fairness check object provides a summary. Plotting the fairness check object provides a detailed and self-explanatory visualisation of the summary. The visualisation clearly delineates those factor levels that pass the fairness check (i.e., values fall within the green zone), as opposed to those that don't (i.e., values falling within the red zone).

<br>

```{r explainer}


wage_explainer <- DALEXtra::explain_tidymodels(
    wage_fit,
    data    = wage_train_tbl |> select(-wage),
    y       = wage_train_tbl$wage,
    verbose = FALSE

)


model_performance(wage_explainer)


fairness_object <- fairness_check_regression(wage_explainer,
                          protected  = protected,
                          privileged = "1. White",
                          colorize = TRUE)


print(fairness_object)

plot(fairness_object)


```

<br>

Independence, Separation, and Sufficiency are essential metrics when assessing regression-based machine learning models for bias. These metrics help identify potential sources of bias and ensure the reliability and validity of the model's results.

<br>

#### Independence:

Independence is a metric that measures whether the residuals (the differences between predicted and actual values) are independent of one another. In a well-specified regression model, the residuals should not exhibit any patterns or correlations among themselves.

In our example the Fairness Check suggests that the residuals from the Wage predictions are correlated with the "4. Other" level of the Race variable. This indicates that the model has not adequately accounted for the relationship between the Race predictor and the Wage, which can lead to biased estimates of the effect of Race on Wage. In our example, the "4. Other" group appear to consistently receive lower wage predictions.

<br>

#### Separation:

Separation is a metric that measures whether there is complete separation between predictor variables and the outcome variable. Separation occurs when a certain variable, like the Race of an employee in this example, has an extremely high predictive value for the Wage predicted by the model. While it may be tempting to use this feature to make predictions, doing so could lead to biased outcomes. In our example the "4. Other" level in the Race feature nears being outside the bounds of being acceptable.

<br>

#### Sufficiency:

Sufficiency is a metric that measures whether the sample size is sufficient to support reliable inferences from the regression model. In general, larger sample sizes provide more reliable estimates and better model performance.

With small sample sizes, the risk of overfitting increases, and the model may capture noise rather than true relationships between variables, leading to incorrect conclusions about the significance and magnitude of predictor variables' effects. In our example, the sample sizes for each level of the Race variable are considered acceptable by the Fairness Check.

<br>

#### We found bias... now what?

The current example illustrates that simply omitting a variable, in this case Race, does not eliminate potential bias. The underlying data used for training the model would appear to be biased. Action needs to be taken to mitigate the impact of bias.

Where bias is found to exist various pre and post processing techniques are available to minimise the influence of potential sources of bias. These methods, while beyond the scope of the current article, are supported by the Fairmodels package and are detailed in the supporting package [documentation](https://modeloriented.github.io/fairmodels/articles/Advanced_tutorial.html#bias-mitigation-strategies-1).

<br>

# Conclusion

Assessing for algorithmic bias is critical prior to applying ML models to support decision making in all spheres of society. Identifying and understanding if algorithms fail certain groups in society ensures that ML models improve decision making to the betterment of all, as opposed to a few.

The current article illustrates an open-source package, Fairmodels, for assessing, identifying and visualising the presence of bias in ML models. By understanding the metrics for Fairness checks and diagnosing potential violations, researchers can improve the validity and reliability of their models, ultimately leading to more accurate conclusions about the relationships between variables, and thereby mitigating bias toward particular groups. Fair models should lead to the betterment of all!
