{
  "hash": "8e19eaf9927c2b6ebfb34117bf7adb47",
  "result": {
    "markdown": "---\ntitle: 'Assessing Bias in ML Models'\ndate: 2023-04-16\ndescription: \"Assessing the presence of bias against minority groups in Machine Learning models to enhance their ethical strength and equality.\"\nauthor: \n    - name: Adam D McKinnon\ncategories: [Bias, Tidymodels, Fairmodels, R]\nimage: \"ai_models_stage.webp\"\ntitle-block-banner: true\ndraft: true\n---\n\n::: {.cell code_folding='false'}\n::: {.cell-output-display}\n![](ai_models_stage.webp){width=100%}\n:::\n:::\n\n\n<br>\n\n# Introduction\n\nModel explainability is a critical component of machine learning (ML) model building. **Ludek Stehlik**, a great contributor in the people analytics community, recently published an excellent [article](https://blog-about-people-analytics.netlify.app/posts/2023-04-13-interpretable-ml/) on the use of the **ModelStudio** package (R ecosystem) for explaining ML models. Inspired by Ludek's blog, I've created the following blog on the use of the **Fairmodels** package in R.\n\n[![](fairmodels_logo.png){fig-align=\"center\" width=\"200\"}](https://modeloriented.github.io/fairmodels/index.html)\n\nFairmodels, built by the developers of ModelStudio, is intended for assessing, visualising and mitigating the presence of bias in ML models.\n\nWe'll begin by loading the required libraries, loading the data, and preparing the data for ML modelling.\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fairmodels)\nlibrary(DALEX)\nlibrary(DALEXtra)\n\n\nwage_tbl <- readr::read_csv(file = \"dataset-37830.csv\") |> \n            janitor::clean_names()\n```\n:::\n\n\n<br><br>\n\n## Model Preparation\n\nWe'll remove the \"logwage\" column as we're going to predict \"wage\" directly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove unnecessary columns\nwage_cleaned_tbl <- wage_tbl %>%\n    mutate_if(is.character, ~as.factor(.)) |> \n    select(-c(logwage, sex))\n\n# Feature Engineering\n# For this example, we'll keep the features as they are. However, depending on the dataset and the domain, you may need to engineer new features or transform existing ones.\n\n# Split the dataset into training and testing sets\nset.seed(123)\ndata_split <- initial_split(wage_cleaned_tbl, prop = 0.8)\nwage_train_tbl <- training(data_split)\nwage_test_tbl <- testing(data_split)\n\n# Create a recipe for preprocessing\nwage_recipe <- recipe(wage ~ ., data = wage_train_tbl) |> \n    recipes::update_role(race, new_role = \"id\")\n\n\n\nprotected <- bake(wage_recipe |> prep(), new_data = wage_train_tbl) |> select(race) |> pull()\n```\n:::\n\n\n<br><br>\n\n## Model Specification\n\nFrom here we'll set up the random forest model\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the model specification\nrf_spec <- rand_forest(trees = 1000) |> \n    set_engine(\"ranger\") |> \n    set_mode(\"regression\") \n    \n\n# Create a workflow\nwage_workflow <- workflow() |> \n    add_recipe(wage_recipe) |> \n    add_model(rf_spec)\n\n# Train the model\nwage_fit <- fit(wage_workflow, wage_train_tbl)\n\n# Make predictions on the test set\nwage_test_pred <- predict(wage_fit, wage_test_tbl) |> \n    bind_cols(wage_test_tbl)\n\n# Evaluate the model performance\nwage_metrics <- metric_set(rmse)\nwage_results <- wage_metrics(wage_test_pred, truth = wage, estimate = .pred)\n\nprint(wage_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        33.6\n```\n:::\n:::\n\n\n<br><br>\n\n## Model Explainer & Fairness Check\n\nFirst we create the DALEX-based explainer object, which draws upon the fitted tidymodels model, the training data and our predicted variable, in this instance, Wage. We then use the explainer object to perform a fairness check of the Wage prediction. As the prediction is a value, and not a probability of classification, we use the fairness_check_regression function from Fairmodels.\n\nPrinting the fairness check object provides a summary. Plotting the fairness check object provides a detailed and self-explanatory visualisation of the summary. The regression-based fairness check utilises three metrics to assess the presence of bias. The respective assessment metrics are applied to each of the factor levels of the variable of interest, in our case Race. The visualisation clearly delineates those factor levels that pass the fairness check (i.e., values fall within the green zone), as opposed to those that don't (i.e., values falling within the red zone).\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwage_explainer <- DALEXtra::explain_tidymodels(\n    wage_fit,\n    data    = wage_train_tbl |> select(-wage),\n    y       = wage_train_tbl$wage,\n    verbose = FALSE\n\n)\n\n\nmodel_performance(wage_explainer)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMeasures for:  regression\nmse        : 946.0564 \nrmse       : 30.75803 \nr2         : 0.4612749 \nmad        : 14.55176\n\nResiduals:\n          0%          10%          20%          30%          40%          50% \n-102.6445597  -29.0820629  -20.6263656  -13.8813996   -8.8483464   -4.0898662 \n         60%          70%          80%          90%         100% \n   0.9815948    7.1601028   15.4004204   29.9078928  190.8156705 \n```\n:::\n\n```{.r .cell-code}\nfairness_object <- fairness_check_regression(wage_explainer,\n                          protected  = protected,\n                          privileged = \"1. White\",\n                          colorize = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreating fairness regression object\n-> Privileged subgroup\t\t: character (\u001b[32m Ok \u001b[39m )\n-> Protected variable\t\t: factor (\u001b[32m Ok \u001b[39m ) \n-> Fairness objects\t\t: 0 objects \n-> Checking explainers\t\t: 1 in total ( \u001b[32m compatible \u001b[39m )\n-> Metric calculation\t\t: 3/3 metrics calculated for all models\n\u001b[32m Fairness regression object created succesfully \u001b[39m \n```\n:::\n\n```{.r .cell-code}\nprint(fairness_object)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFairness check regression for models: workflow \n\n\u001b[31mworkflow passes 1/3 metrics\n\u001b[39mTotal loss:  1.61128 \n```\n:::\n\n```{.r .cell-code}\nplot(fairness_object)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/explainer-1.png){width=672}\n:::\n:::\n\n\nIndependence, Separation, and Sufficiency are essential metrics when assessing regression-based machine learning models for bias. These metrics help identify potential sources of bias and ensure the reliability and validity of the model's results.\n\n#### Independence:\n\nIndependence is a metric that measures whether the residuals (the differences between predicted and actual values) are independent of one another. In a well-specified regression model, the residuals should not exhibit any patterns or correlations among themselves.\n\nViolations of independence can lead to biased estimates. For example, if residuals are correlated with one of the predictor variables, this indicates that the model has not adequately accounted for the relationship between that predictor and the outcome variable. This can lead to biased estimates of the predictor's effect on the outcome. Incorrect standard errors can also arise from violations of independence, which can subsequently lead to incorrect conclusions about the significance of predictor variables.\n\n#### Separation:\n\nSeparation is a metric that measures whether there is complete separation between predictor variables and the outcome variable. In a logistic regression context, complete separation occurs when a predictor variable perfectly predicts the outcome. This can lead to infinite parameter estimates, as the maximum likelihood estimation fails to converge. Violations of separation can result in infinite parameter estimates or incorrect standard errors. When a predictor perfectly separates the outcome variable, the model can become overfit, resulting in unstable estimates and misleading conclusions about the predictor's effect. To diagnose separation issues, one can use statistical techniques like Firth's penalized likelihood method or the application of ridge regression.\n\n#### Sufficiency:\n\nSufficiency is a metric that measures whether the sample size is sufficient to support reliable inference from the regression model. In general, larger sample sizes provide more reliable estimates and better model performance.\n\nWith small sample sizes, the risk of overfitting increases, and the model may capture noise rather than true relationships between variables, leading to incorrect conclusions about the significance and magnitude of predictor variables' effects. As a rule of thumb, a larger sample size is preferred when working with multiple predictors or complex relationships.\n\n# Conclusion\n\nIn conclusion, Independence, Separation, and Sufficiency are crucial metrics for assessing bias in regression models. By understanding these metrics and diagnosing potential violations, researchers can improve the validity and reliability of their models, ultimately leading to more accurate conclusions about the relationships between variables.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}