---
title: 'Creating Synthetic People Analytics Data'
date: 2023-08-10
description: "Some simple methods for creating people analytics data."
author: 
    - name: Adam D McKinnon
categories: [Synthetic Data, People Analytics]
image: "maxim-berg-Ac02zYZs22Y-unsplash.jpg"
title-block-banner: true
draft: false
---

```{r header, echo=FALSE, code_folding = FALSE, fig.cap="[Photo by Maxim Berg on Unsplash](https://unsplash.com/@maxberg).",out.width = '100%'}
knitr::include_graphics("maxim-berg-Ac02zYZs22Y-unsplash.jpg")
  
```

<br>

```{r libraries}


library(dplyr)
library(purrr)
library(lubridate)
library(ggplot2)

theme_set(theme_linedraw() + theme(legend.position = "bottom"))

```

\## Rationale


Synthetic datasets can be useful for testing and explaining analytical methods, especially in the people analytics field where real-world data is not able to be shared. While there are a number of example datasets out there for use (eg. IBM Employee Attrition & Performance (canonical link?)) in many situations there is a need for data with a specific set of characteristics. In this case, it may be easier to generate the data yourself to fit the specific need.

\## Generating a basic dataset


When the requirements are relatively simple we can get most of the way using R's inbuilt random generators, either sampling from a statistical distribution (eg. \`rnorm\` for a random value from a Normal distribution) or sampling from a defined list of options and probabilities with \`sample\`.

Using a combination of these functions, we can generate some synthetic data that approximates what we might expect from a people analytics dataset.


```{r basic_data}


generate_basic_data <- function(num_rows) {
  # Going to be doing a lot of repeated sampling of a fixed size with 
  # replacement, so customise the sample function with the right defaults
  sample_replace <- function(x, prob = NULL) {
    sample(x = x, prob = prob, size = num_rows, replace = T)
  }
  
  data_frame(
    ID = 1:num_rows,
    age = rnorm(num_rows, mean = 40, sd = 5),
    hire_date = sample_replace(seq.Date(from = dmy("01/01/1990"), to = today(), by = "1 day")),
    job_family = sample_replace(c("Engineering", "Sales", "Administration"), prob = c(0.6,0.25, 0.15)),
    engagement = sample_replace(c("Full Time", "Part Time"), prob = c(0.9, 0.1)),
    employment = sample_replace(c("Permanent", "Contract"), prob = c(0.7,0.3)),
    state = sample_replace(c("VIC","NSW","QLD"), prob = c(0.5, 0.3, 0.2))
  )
}

generate_basic_data(num_rows = 10)



```


## Related variables

To make a list behave realistically however, we shouldn't allow all the data points to be randomly generated independently of each other. There are often correlations and dependencies between variables and it may be important to include these in your synthetic data set.

If you are trying to create something that has similar properties to a real dataset, perform an exploratory analysis to understand what correlations are present in the existing data and decide which are relevant for your purpose. Then, we will need to adjust the data generation code to take these differences into account. It is a bit more work because we will now need to have a separate data generation function for each correlated variable, but it is necessary if we want these observed interactions to be in the final data.

For example, say the main office for our example company was in Victoria. We would expect the administrative staff to be more likely to work there rather than being randomly distributed across the states. To make this work we will need to change how we randomly select a state to be dependent on the job family.


```{r dependency_example}


sample_state <- function(job) {
  # Different State probabilities depending on job family
  prob_for_family <- list(
    "Engineering" = c(0.5, 0.3, 0.2),
    "Sales" = c(0.5, 0.3, 0.2),
    "Administration" = c(0.8, 0.1, 0.1)
  )

  # Randomly select the state using the per-job probabilities
  map_chr(
    job, 
    ~sample(c("VIC","NSW","QLD"), size = 1, prob = prob_for_family[[.]])
    )
}



```

The first part of this process is to create a way to adjust the state probability weightings based on a job family. We can keep things simple in this example by using a named list where the names are the three job families and the values are the probabilities to select each state for that job family. Here, we have left the Engineering and Sales families at 50% VIC, 30% NSW, and 20% QLD from the original example but changed the Administration family to be 80%/10%/10% respectively.

Next, we randomly generate the state data. In order to change the probability weightings each time we will use \`map_chr\` to apply our sample function over the job families, randomly selecting one state each time, but using the probabilities we defined above.

## More complex interactions

This same approach can be extended as the interactions become more complex. For example, we could imagine a scenario in which employment type is dependent on \*both\* the hire date and the job family. How would we generate a data set in which more recent hires (\\\<5 years) are more likely to be on short-term contracts, as are Sales staff.


```{r dual_dependency_example}


sample_employment <- function(hired, job) {
    # Probability map of Permanent/Contract employment for combinations of 
    # new/old hires and job family
    prob_map <- list(
      new_hire = list(
        "Engineering" = c(0.7, 0.3),
        "Sales" = c(0.4, 0.6),
        "Administration" = c(0.7, 0.3)
      ),
      old_hire = list(
        "Engineering" = c(0.9, 0.1),
        "Sales" = c(0.7, 0.3),
        "Administration" = c(0.9, 0.1)
      )
    )
 
    hire_status = ifelse(
      time_length(hired %--% today(), "years") < 5, 
      "new_hire", 
      "old_hire"
    )
    
    map2_chr(
      hire_status, job, 
      ~sample(c("Permanent", "Contract"), size = 1, prob = prob_map[[.x]][[.y]])
    )
  }


```



While a bit more involved to determine the probability weightings than the previous example, the overall structure remains the same.



\## Making use of it


To bring it all together, we can now swap out how we randomly selected state and employment type in the first basic example with our two functions above that randomly generate data dependent on other data columns. These other data columns are generated earlier in the process and then passed along to our new functions.


```{r complex_data}


generate_complex_data <- function(num_rows) {
  sample_replace <- function(x, prob = NULL) {
    sample(x = x, prob = prob, size = num_rows, replace = T)
  }
  
data_frame(
    ID = 1:num_rows,
    age = rnorm(num_rows, mean = 40, sd = 5),
    hire_date = sample_replace(seq.Date(from = dmy("01/01/1990"), to = today(), by = "1 day")),
    job_family = sample_replace(c("Engineering", "Sales", "Administration"), prob = c(0.6,0.25, 0.15)),
    engagement = sample_replace(c("Full Time", "Part Time"), prob = c(0.9, 0.1)),
    employment = sample_employment(hire_date, job_family),
    state = sample_state(job_family) 
  )
}

generate_complex_data(num_rows = 10)


```


This produces a randomised data set with our desired characteristics that can then be saved and reused later.



\## Other suggestions


The approach shown here works well enough when there are a relatively small number of variables and interactions that you need to account for. It is capable of being expanded by creating extra data generating functions however at some point things will become challenging to maintain. Once your needs for synthetic data have outgrown this approach, there are alternatives that may be worth exploring. The \[fabricatr\] (https://declaredesign.org/r/fabricatr/) package provides a method to generate synthetic datasets with quite sophisticated interactions between variables. Or if you have a specific real dataset whose characteristics you are looking to create, the \[synthpop\](https://www.synthpop.org.uk) package may be appropriate.
