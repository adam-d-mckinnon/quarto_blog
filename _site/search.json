[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "People Analytics Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nExpediting Exploratory Data Analysis\n\n\n\n\n\n\n\nPPSR\n\n\nCorrelation Funnel\n\n\nDataExplorer\n\n\nR\n\n\n\n\nExperimenting with different methods to rapidly explore relationships within datasets prior to performing other analytic activities.\n\n\n\n\n\n\nJan 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPredicting Promotions Through Machine Learning\n\n\n\n\n\n\n\nTidymodels\n\n\nXGBoost\n\n\nR\n\n\nMachine Learning\n\n\nEmployee Promotions\n\n\n\n\nBuilding an XGBoost model in the Tidymodels ecosystem that predicts whether an employee should be promoted.\n\n\n\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAutomated assessment of employee data quality using machine learning\n\n\n\n\n\n\n\nPython\n\n\nH2o\n\n\nIsolation Forests\n\n\nMachine Learning\n\n\nData Quality\n\n\n\n\nAn automated, multi-dimensional and scalable approach to monitoring and improving employee data quality using unsupervised machine learning.\n\n\n\n\n\n\nJul 21, 2022\n\n\nMartha Curioni, Adam D McKinnon\n\n\n\n\n\n\n  \n\n\n\n\nMeasuring Impact in HR: A Practical Demonstration\n\n\n\n\n\n\n\nImpact Measurement\n\n\nt-test\n\n\nrstatix\n\n\nggpubr\n\n\nPlotly\n\n\nR\n\n\n\n\nA ‘How To’ article on the measurment of differences between groups in HR settings.\n\n\n\n\n\n\nJun 6, 2021\n\n\nAdam D McKinnon, Sambit Das\n\n\n\n\n\n\n  \n\n\n\n\nGoing the Distance!\n\n\n\n\n\n\n\nGoogle Maps\n\n\ngoogleway\n\n\necharts4r\n\n\nR\n\n\n\n\nA practical guide to measuring distance using Google Maps in R.\n\n\n\n\n\n\nJun 6, 2021\n\n\nAdam D McKinnon\n\n\n\n\n\n\n  \n\n\n\n\nThe Value of Measuring Employee Skill Data\n\n\n\n\n\n\n\nEmployee Skills\n\n\nSkill Management\n\n\nData\n\n\nHR\n\n\n\n\nThis article explores the two fundamental considerations when it comes to skills management. First, how do you acquire your employees’ skill data? Second, what practical value can you then generate from this wealth of information?\n\n\n\n\n\n\nDec 9, 2020\n\n\nAdam D McKinnon, Mikaël Wornoo\n\n\n\n\n\n\n  \n\n\n\n\nHow HR can Apply Network Analysis to Open Data\n\n\n\n\n\n\n\nNetwork Analysis\n\n\nOpen Data\n\n\nHR\n\n\n\n\nThis article explores how Network Analysis can be applied to a variety of Open data sources to inform people-related decisions inside organisations. Practical use cases applying network analysis to open data are provided.\n\n\n\n\n\n\nAug 27, 2020\n\n\nAdam D McKinnon, André Vermeij\n\n\n\n\n\n\n  \n\n\n\n\nA Beginner’s Guide to Machine Learning for HR Practitioners\n\n\n\n\n\n\n\nArtificial Intelligence\n\n\nMachine Learning\n\n\nHR\n\n\n\n\nAn introductory article intended to demistify Machine Learning (ML)–an important subset of AI–for HR Professionals.\n\n\n\n\n\n\nJun 19, 2020\n\n\nAdam D McKinnon, Monica Ashton\n\n\n\n\n\n\n  \n\n\n\n\n4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019\n\n\n\n\n\n\n\nHR Tech\n\n\nNetwork Analysis\n\n\nKenelyze\n\n\n\n\nIn this article we take a data-driven look at HR Technology, using network analysis. Specifically, we objectively examine the question ‘which technologies and vendors are influential in the HR Tech landscape?’\n\n\n\n\n\n\nMar 16, 2020\n\n\nAdam D McKinnon, Leandra Griep\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dmckinnon2023,\n  author = {Adam D McKinnon and Harlow Malloc},\n  title = {Post {With} {Code}},\n  date = {2023-01-08},\n  url = {https://www.adam-d-mckinnon.com//posts/post-with-code},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAdam D McKinnon, and Harlow Malloc. 2023. “Post With Code.”\nJanuary 8, 2023. https://www.adam-d-mckinnon.com//posts/post-with-code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my personal blog, coming to you from Melbourne, Australia. I write about the evolving field of People Analytics, mostly through the application of the R language. On this blog site you can find my posts which provide both a philosophical perspective regarding advancements in people analytics, along with practical “How To” projects applying different methods (please learn from my mistakes!)."
  },
  {
    "objectID": "posts/2023-01-03-exploratory_analysis/index.html",
    "href": "posts/2023-01-03-exploratory_analysis/index.html",
    "title": "Expediting Exploratory Data Analysis",
    "section": "",
    "text": "Start with the required libraries, and then load some data.\n\n\nCode\n# data manipulation\nlibrary(tidyverse)\nlibrary(janitor)\n\n# Data Exploration\nlibrary(ppsr)\nlibrary(correlationfunnel)\nlibrary(DataExplorer)\n\n# Visualisation addition\nlibrary(plotly)\n\n\n# Load Data ----\npromotions_tbl <- readr::read_csv(file = \"train.csv\") %>% \n    janitor::clean_names()\n\n# reduce the dataset size\ncleaned_promotions_tbl <-\npromotions_tbl %>%\n    tidyr::drop_na() %>% \n    dplyr::mutate(\n        is_promoted = as.character(is_promoted),\n        is_promoted = if_else(is_promoted==1, \"Yes\", \"No\") %>% as.factor()\n    )\n\n\n\n\n1. Predictive Power Score\n\n\nCode\ncleaned_promotions_tbl %>%\n    select(-employee_id) %>%\n    visualize_pps(y = 'is_promoted', do_parallel = TRUE)\n\n\n\n\n\n\n\n\n\n\n2. Correlation Funnel\n\n\nCode\ncleaned_promotions_tbl %>% \n    select(-employee_id) %>% \n    binarize() %>% \n    correlate(target = is_promoted__Yes) %>% \n    plot_correlation_funnel(interactive = TRUE) %>% \n    plotly::config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n3. DataExplorer\n\n\n\n\n\n\n\nDefault Heatmap\n\n\nCode\ncorr_plot <- cleaned_promotions_tbl %>% \n    select(-employee_id) %>% \n    DataExplorer::plot_correlation(\n        theme_config = list(\n            legend.position = \"none\",\n            axis.text.x     = element_text(angle = 90)\n                )\n    )\n\n\n\n\n\n\n\nInteractive Heatmap\n\n\nCode\ncorr_plot$data$value <- round(corr_plot$data$value, digits = 2)\n\nplotly::plotly_build(corr_plot) %>% \n    plotly::layout(width = 700, height = 700) %>% \n    plotly::config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{2023,\n  author = {},\n  title = {Expediting {Exploratory} {Data} {Analysis}},\n  date = {2023-01-03},\n  url = {https://www.adam-d-mckinnon.com//posts/2023-01-03-exploratory_analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Expediting Exploratory Data Analysis.” 2023. January 3,\n2023. https://www.adam-d-mckinnon.com//posts/2023-01-03-exploratory_analysis."
  },
  {
    "objectID": "posts/2022-12-30-promotion_prediction/index.html",
    "href": "posts/2022-12-30-promotion_prediction/index.html",
    "title": "Predicting Promotions Through Machine Learning",
    "section": "",
    "text": "Photo by Possessed Photography on Unsplash.\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{2022,\n  author = {},\n  title = {Predicting {Promotions} {Through} {Machine} {Learning}},\n  date = {2022-12-30},\n  url = {https://www.adam-d-mckinnon.com//posts/2022-12-30-promotion_prediction},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Predicting Promotions Through Machine Learning.” 2022.\nDecember 30, 2022. https://www.adam-d-mckinnon.com//posts/2022-12-30-promotion_prediction."
  },
  {
    "objectID": "posts/2022-07-21-isolation_forests/index.html",
    "href": "posts/2022-07-21-isolation_forests/index.html",
    "title": "Automated assessment of employee data quality using machine learning",
    "section": "",
    "text": "Photo by Mika Baumeister on Unsplash.\n\n\n\n\n\nIntroduction\nThe topic of data quality is like that of I.T. services generally… you only ever hear about it when there’s a problem! There’s an implicit assumption among stakeholders receiving the findings of people analytics initiatives that your data is “good”. Failure to observe this assumption of high data quality can significantly (and rapidly) undermine the credibility of findings, irrespective of how small the data quality deviation!\nFrom an analytical perspective, the quality of your insights can potentially be limited by flawed data. As the saying goes “garbage in, garbage out”. Coupled with stakeholder expectations, it becomes increasingly important that organisations invest time and energy in the ongoing assessment and curation of quality data to maximise the influence of data driven decision making in organisations.\nAt the same time, monitoring data quality in HR can be immensely time consuming, expensive, simplistic in execution (i.e., simple evaluation criteria such as age ranges), highly repetitive, and utterly devoid of ANY professional enjoyment! To overcome these shortcomings, we have developed an automated, multi-dimensional and scalable approach to data quality evaluation using unsupervised machine learning—Isolation Forests.\n\n\nThe Process\nWe used Isolation Forests, an unsupervised machine learning approach to scan employee data for potentially anomalous[1] records.\nOur code can be viewed at this GitHub account, which followed the following process steps:\n\nIngested personnel data and loaded the appropriate libraries. In our example, we used the IBM HR Attrition dataset, then installed the H2o and Pandas libraries.\nDefined the dataset columns to include in the data quality assessment. Trained an Isolation Forest algorithm on the data. The Isolation Forest algorithm partitioned data through a forest of decision trees. Each split of the data was made randomly and the number of splits required to isolate a record indicated whether or not the record was considered anomalous. When a forest of random trees collectively produced shorter path lengths for particular records, they were highly likely to be considered anomalies.\nUsing the trained Isolation Forest model, we then predicted which records we believed to be anomalous. Of those records predicted to have data quality issues we then narrowed down to those with the highest likelihood of being anomalous using characteristics from the trained model.\nThe model has two levels of output interpretation:\n\nGlobal – which dataset columns are most often related to data quality errors. At best this is interesting in identifying potentially systemic errors, which may be a catalyst for more controlled data entry parameters.\nLocal – for any given record why is it deemed an anomaly. It is this level that is most valuable in directly resolving data quality issues, and therefore, was the focus of our approach.\n\nFor those records deemed highly anomalous (our subset), we then trained a simple Random Forest model to predict the anomaly flag(s). This process was repeated multiple times for each record to increase our certainty in identifying those variables contributing to its anomaly status. Note: We kept the model simple to make identification (and interpretation) of the combination of variables contributing to each records anomaly status easy to perform. In short, we wanted the output interpretable by a wide audience, not just those familiar with machine learning outputs.\nThe generated output (see excerpt of output in image below) has two fields: 1. A unique identifier, and 2. A list of variables contributing to each records anomaly status. The unique identifier, in our case the unique Employee Number taken from the original dataset, is critical to enabling identification and access to the original record that requires review. The second column has a ranked list (ranked from most important to least) showing the variables “collectively” contributing to the anomaly status. For example, when inspecting EmployeeNumber 58’s record we see a Monthly Income only four times their DailyRate (possibly incorrect), and that the individual has been with the company 2 years, had their last promotion 2 years ago, and has 2 years in their current role, which collectively cannot all be correct–the algorithm suggests reviewing YearsInCurrentRole.\n\n\n\n\n\n\nExample Output.\n\n\n\n\n\n\nConclusion\nUnsupervised Machine Learning algorithms such as Isolation Forests can be an excellent way of automating and scaling the review of data to monitor for quality concerns. The approach lends itself to those datasets that have a reasonable level of data quality and that are looking to make further improvements.\nThe major advantage of this approach is that the Isolation Forest can identify a record as anomalous, despite no one variable in the record being out of acceptable limits. Instead, the algorithm assesses the combination of multiple variables to determine if the combination makes it seem anomalous. This is like having human intelligence review each record quickly, at scale, and without destroying anyone’s job satisfaction!\nWe propose that this approach has the potential to significantly reduce time spent on direct data quality evaluations, which has considerable direct benefits (i.e., better quality data, more representative analyses and interpretation, etc.) as well as indirect benefits (i.e., time can be spent on other value-add initiatives that can only be done by humans!). This is particularly true for smaller teams, where the need to do more with less is greatest.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{curioni2022,\n  author = {Martha Curioni and Adam D McKinnon},\n  title = {Automated Assessment of Employee Data Quality Using Machine\n    Learning},\n  date = {2022-07-21},\n  url = {https://www.adam-d-mckinnon.com//posts/2022-07-21-isolation_forests},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMartha Curioni, and Adam D McKinnon. 2022. “Automated Assessment\nof Employee Data Quality Using Machine Learning.” July 21, 2022.\nhttps://www.adam-d-mckinnon.com//posts/2022-07-21-isolation_forests."
  },
  {
    "objectID": "posts/2020--8-13-network_analysis/index.html",
    "href": "posts/2020--8-13-network_analysis/index.html",
    "title": "Expediting Exploratory Data Analysis",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{dmckinnon2023,\n  author = {Adam D McKinnon and Adam D McKinnon and André Vermeij},\n  title = {Expediting {Exploratory} {Data} {Analysis}},\n  date = {2023-01-03},\n  url = {https://www.adam-d-mckinnon.com//posts/2020--8-13-network_analysis},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAdam D McKinnon, Adam D McKinnon, and André Vermeij. 2023.\n“Expediting Exploratory Data Analysis.” January 3, 2023. https://www.adam-d-mckinnon.com//posts/2020--8-13-network_analysis."
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html",
    "href": "posts/2020-08-27-network_analysis/index.html",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "",
    "text": "Photo by Matt Palmer on Unsplash.\nThis article explores:"
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html#combining-datasets-to-add-even-more-value",
    "href": "posts/2020-08-27-network_analysis/index.html#combining-datasets-to-add-even-more-value",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "Combining datasets to add even more value",
    "text": "Combining datasets to add even more value\nAnalyses based on open data can become particularly powerful when datasets are combined in a meaningful way. If you’re working with various types of documents (let’s say patents and journal articles), you can use metadata which is common to both datasets to link them together and generate combined network overviews. Document summaries/abstracts and author information are good examples of columns which can be used to link datasets."
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html#acknowledgments",
    "href": "posts/2020-08-27-network_analysis/index.html#acknowledgments",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the myHRfuture website under the title “How HR can Apply Network Analysis to Open Data” on August 13, 2020."
  },
  {
    "objectID": "posts/2020-08-27-network_analysis/index.html#why-network-analysis-could-be-more-valuable-than-organisational-network-analysis",
    "href": "posts/2020-08-27-network_analysis/index.html#why-network-analysis-could-be-more-valuable-than-organisational-network-analysis",
    "title": "How HR can Apply Network Analysis to Open Data",
    "section": "Why “Network Analysis” could be more valuable than “Organisational Network Analysis”",
    "text": "Why “Network Analysis” could be more valuable than “Organisational Network Analysis”\nOrganisational Network Analysis (ONA) has received considerable attention in recent years within People Analytics circles globally, as illustrated by the public discussion and proliferation of companies specialising in this method. There is an appetite to learn and do more, which also speaks to the quality and actionability of insights generated. Typical ONA benefits include, for example, improved collaboration through a better view on intra- and inter-team collaboration and the identification of key individuals acting as hubs and connectors for efficient message broadcasting during change management initiatives.\nONA traditionally denotes the analysis of social interactions internally to inform organisational decision making. We have seen exceptional examples of this work published in the public domain from people like Michael Arena, Rob Cross, Manish Goel and many others. These are important examples; however, we could be imposing a false barrier upon practitioners by focusing too much on the ‘O’ for Organisational. While the term ONA is directionaly accurate, it limits our thinking and actions regarding the breadth of possible applications for network analysis. In other words, we are limiting the data sources we could use and the questions we could answer by only looking internally, within the organisation.\nIn academic circles, this work is often referred to as “Social Network Analysis” (SNA) and can be found in a body of literature that spans decades. SNA broadens the perspective of the discussion by using network analysis to understand social interactions. In doing so, it has generated a wealth of insights around social relationships in the fields of sociology, psychology and anthropology. There are however, once again, limitations. The field is limited to solely represent social connectedness (connections between individuals, teams, or entire companies), as opposed to simply connectedness (connections between any other type of data point – for example documents, projects, skills, knowledge areas, etc.). In a society of ever-increasing digitisation and data generation, insights into this broader concept of connectedness are of increasing value.\nNetwork analysis can be applied to a variety of data types and data sources in a bid to understand this kind of broad connectedness between data, as opposed to just connections between people. Since the early 2000s, the broader scientific fields of Network Science and Complex Networks have been focusing on applying large-scale network analysis in a wide variety of other domains as well, including, for example, biology (cellular networks), neurology (brain networks), economics (corporate governance, trade networks) and finance (transaction networks, fraud detection).\nThe People Analytics field should therefore embrace the name “Network Analysis” as opposed to “Organisational Network Analysis” in order to shed the limitations imposed by the domain specific names that have previously dominated public discussion in the field, and subsequently broaden the opportunity for People Analytics practitioners to generate unique insights."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "",
    "text": "Image by Pietro Jeng on Unsplash.\nWhen you hear Artificial Intelligence (AI) the first thing that comes to mind are robots; in particular, the Steven Spielberg movie titled A.I. where a robot child is built that can love and behave just like a real human. This idea appears to be closer to a dream than reality. Truth is, AI is more ubiquitous than we might think. It ranges from self-driving cars, movie recommendations on Netflix, e-mail spam detection to voice-controlled assistants such as Apple’s SIRI. The fact is that AI is already present across many businesses and various industries, as is shown in the figure below (Note the low adoption rate in Human Resources).\nStill, evidence suggests that HR departments remain unable to seize the multitude of opportunities associated with AI. In part, what may be required to accelerate the adoption of AI is educational content directed at HR Professionals, not data scientists. Thus, we offer this brief guide to Machine Learning (ML), an important subset of AI, with the intent to demystify ML and make it tangible."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-reinforcement-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-reinforcement-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "1. What is Reinforcement Learning?",
    "text": "1. What is Reinforcement Learning?\nReinforcement Learning is probably best known through IBM’s Deep Blue computer, a “robot” that learned how to play chess and beat the human world champion.\nReinforcement Learning is a type of technique that enables an algorithm to learn by trial and error, using feedback from its own actions and experiences. Much like Pavlov and his dog, Reinforcement Learning involves rewarding decisions that lead to success and penalizing decisions that lead to anything other than success—ultimately making the algorithm more intelligent in the process.\nExamples of reinforcement learning applied in HR are a bit lean, though are most prevalent in areas such as education (i.e., applying content based on the progress of the student), finance and investment (i.e., advanced forecasting), supply chain operations (i.e., robots fulfilling orders in a warehouse), traffic flow optimization, and healthcare (i.e., accurate classification of biopsy images)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-supervised-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-supervised-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "2. What is Supervised Learning?",
    "text": "2. What is Supervised Learning?\nThe most common forms of ML across industries, and specifically the HR domain, are Supervised Learning, followed by Unsupervised Learning.\nIn Supervised Learning, we try to predict an outcome, such as whether an employee will leave the company, the risk of an employee being injured, or the ideal starting salary of a new employee.\nTo make predictions we need different input variables (i.e., variables are called “Features” by data scientists). Our input features are only limited to our imagination (i.e., what we think will be important), what data we can get our hands-on, or what data we can create (e.g., by knowing where someone works and where they live we can create a variable focused on employee commute distance).  ### An example: Supervised Learning informing Employee Turnover\nLet’s examine a more detailed example of Supervised Learning—predicting who will leave an organization. Imagine that 1 in 5 new recruits leaves an organization in their first 12 months of tenure. To prevent such turnover, we could build a supervised learning model that predicts the likelihood of new starters leaving, so that our HR and managerial colleagues could intervene.\nIn this example, the model outcome being predicted is turnover risk, and the features used to predict turnover risk could include an employees’ demographic and employment characteristics (e.g., age, education level, role level, pay relative to market, month of employment, presence of development plans, and so on.).\nAssuming such a model was highly accurate, it would enable us to understand turnover among our new starter population from three angles.\n\nFirstly, what are the factors most influential in predicting turnover among our population. An example of such a model output is presented in the figure below, which illustrates whether a feature prevents turnover (green bars) or promotes turnover (red lines), and the relative importance of each feature in predicting turnover (i.e. longer lines denote more importance).\nSecondly, the model also rates the likelihood of each new starter leaving the company, enabling focused intervention (i.e. the risk that Adam will leave in his first 12 months).\nThirdly, the model identifies the features preventing or promoting turnover risk for each employee. This individualized output can enable HR professionals to take informed and personalized action, regardless of whether they personally know each employee.\n\n\n\n\n\n\nExample output from a model that predicts employee turnover risk. The plot describes the relative importance and directional influence of features in the model. Red bars represent features that contribute to employee turnover risk, while green bars represent features preventing turnover risk.\n\n\n\n\n\nA supervised learning model used to predict employee turnover among new starters has the potential to reduce notable costs, including financial (e.g., separation, vacancy, recruitment, training, and replacement) reputational (e.g., eroding an EVP and/or reducing candidate appeal) and productivity-related (e.g., on average organizations invest between four weeks and three months training new employees). Some of these costs can be readily quantified so that we can identify organizational savings based on prevented turnover (e.g., preventing 2 in 10 resignations saves $xxx)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-unsupervised-learning",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#what-is-unsupervised-learning",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "3. What is Unsupervised Learning?",
    "text": "3. What is Unsupervised Learning?\nUnlike Supervised Learning where we are trying to predict an outcome, Unsupervised Learning analyzes many variables simultaneously to identify similarities, patterns or relationships in the data. Unsupervised Learning is more about understanding what’s in the data. The two most common uses of unsupervised learning are focused on:\n\nClustering: automatically splitting the dataset into groups based on similarities among the features analyzed. Classically applied to consumers, but equally relevant to organizations, whereby we understand our employee segments (i.e., clusters) and determine whether our HR policies serve the segments.\nAssociation mining: identifies sets of variables that often occur together in your dataset. For example, identifying injury patterns among workers at specific sites.  ### An example: Unsupervised Learning informing Employee Turnover\n\nCluster Analysis, the most famous form of unsupervised learning, can also help us better understand employee attrition. This approach can help group employees based on similar features (e.g., location, tenure, nationality, education level, age, performance level, etc.).\nThe figure below depicts the results of an analysis of the employee’s demographic features. Multiple demographic features are first reduced to two dimensions using a method called Manifold Learning (another non-supervised method), and these two new dimensions are then clustered using a method called T-SNE. The figure below shows us how the employees can be grouped together, in this case, twelve clusters, based on their demographic features.\n\n\n\n\n\nAn example cluster analysis output when applied to employee features.\n\n\n\n\n Once grouped into clusters, the next step is to determine the risk of turnover for each group. Moreover, it is interesting to identify if there are some shared risk factors, practically indicating that employees within a cluster are experiencing the workplace in a similar way.\nThis last insight is of considerable practical significance, as it may help us tailor interventions that target specific employee clusters, thereby delivering maximum impact (i.e., retaining employees and reducing turnover costs) and return on our investment (i.e. for every $ spent we generated $xxx in savings from reduced turnover)."
  },
  {
    "objectID": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#acknowledgments",
    "href": "posts/2020-06-19-a-beginners-guide-to-machine-learning-for-hr-practitioners/index.html#acknowledgments",
    "title": "A Beginner’s Guide to Machine Learning for HR Practitioners",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the Analytics In HR (AIHR) website under the title “A Beginner’s Guide to Machine Learning for HR Practitioners” on June 8th, 2020."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html",
    "href": "posts/2020-03-16-hrtechanalysis/index.html",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "",
    "text": "Photo by JJ Ying on Unsplash."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#data-collection",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#data-collection",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Data collection",
    "text": "Data collection\nWe obtained data from the 2019 Unleash Conference in London, specifically the 35 start-ups and 64 sponsors that were present at the conference. Conference attendees were provided a list of company names and their “tags”, which were descriptors of the technology or HR domain that characterize the respective companies."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#network-analyses-how-we-did-what-we-did",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#network-analyses-how-we-did-what-we-did",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Network Analyses — How we did what we did?",
    "text": "Network Analyses — How we did what we did?\nWe examined the tags using network analysis, in a technology called Kenelyze (kudos to our partner André Vermeij).\nThe current network examines the “influence” of HR tech among those vendors present at the 2019 Unleash Conference in London. Network Nodes were the company Tags, and company name was included as a node attribute. The network was undirected and displayed using the Force Atlas 2 layout. Nodes (i.e. Tags) were sized according to the number of times the Tag was used to describe an organization, and coloured according to the PageRank score (i.e. influence measure). The network is presented below.\n \n\nNetwork Output of Analysis of HR Tech at 2019 Unleash Conference in London"
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#data-interpretation-what-did-we-discover-what-are-main-insights",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#data-interpretation-what-did-we-discover-what-are-main-insights",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Data interpretation: What did we discover? / What are main insights?",
    "text": "Data interpretation: What did we discover? / What are main insights?"
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#next-steps",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#next-steps",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "Next Steps",
    "text": "Next Steps\nThe analysis provides a data driven exploration of the HR Tech landscape present at Unleash London in 2019. While the dataset does not claim to capture the universe of HR Tech, it does provide an interesting perspective of available technologies and how these technologies relate to one another. We encourage further exploration of the interactive network—reflect on your own hypotheses, look at the edges for new trends, examine the network structure, and consider what it might mean for the future. To make the article digestible we intentionally did not address many relationships in the network.\nWe welcome your comments, particularly additional interpretation of the network, that may generate insights and practical value for other readers. Stay tuned! We may look to squeeze further insights from this analysis (e.g. different algorithms, node configurations, etc.)."
  },
  {
    "objectID": "posts/2020-03-16-hrtechanalysis/index.html#what-are-prominent-technologies-in-the-hr-tech-landscape",
    "href": "posts/2020-03-16-hrtechanalysis/index.html#what-are-prominent-technologies-in-the-hr-tech-landscape",
    "title": "4 Key Insights on the HR Tech Landscape… Analysis from London Unleash 2019",
    "section": "What are prominent technologies in the HR Tech landscape?",
    "text": "What are prominent technologies in the HR Tech landscape?\n\n\n1. Talent acquisition is a main influencer\nThe most frequent Tag, most connected to other Tags, and most influential was “Talent Acquisition (incl. Referrals)”. It is perhaps unsurprising that Talent Acquisition is both a common and influential domain in HR Technology. This may in part reflect the discrete nature of the recruiting process, characterized by a definitive start and end point, and a clear, binary success measure—hired or not hired. In addition, Talent Acquisition also lends itself to performance measurement, evidenced by metrics such as source of hire, time to hire, applicants per hire, cost per hire, offer acceptance per hire, etc. All discrete metrics that the technology domain has progressively digitized. Good news for Talent Acquisition functions, what about the rest of us?\n\n\n\n2. People Analytics and Employee Experience are “up and coming”\nFollowing Talent Acquisition the next most frequent and influential Tags included Employee Engagement, Learning & Development, People Analytics, Talent Management, and Employee Experience (EX). These Tags reflect what we interpret as a mix of established HR domains (e.g. Employee Engagement, Learning & Development, Talent Management, HR Transformation), and “up and coming” (e.g. People Analytics and EX). The latter two topics have experienced a meteoric rise in prominence in recent years. However, both People Analytics and EX we anecdotally know to be technology related, and both benefit from a historical connection to data collection network nodes such as “Employee Engagement” and “Surveys (incl. Employee Listening)”.\nWe note two points about People Analytics and EX—first, their collective network metrics and second, the various companies that identify themselves with these two capabilities (i.e. click on nodes to see company listings). To us the network metrics and network position suggest that People Analytics is currently more influential (e.g. frequency, degree, PageRank), more central to the network (i.e. position), and consequently likely to play a more significant role in connecting technologies in the HR Tech landscape in the immediate future. Longer term, only time and repeated assessment of the market will tell.\nSecondly, the companies identifying themselves with these two capabilities are varied! Our interpretation of this variation is that the terms People Analytics and EX (also applicable to “Workforce Planning”) mean many things to different people, perhaps driven by maturity of practice in the consumer population. Alternatively, it may also represent future use cases that aren’t currently mainstream, or the potential for partnership or acquisition among technology vendors. Finally, it may represent opportunistic thinking by marketing professionals!\n\n\n\n3. Skills are the new currency\nA special call out to the topic of Skills Matching, which we at Merck KGaA have invested in during 2019, and will further explore in 2020. As evidenced in the network, we believe there is real world connectivity between Skills Matching and domains such as People Analytics (our team currently owns the Skills Matching topic at Merck KGaA), Strategic Workforce Planning (i.e. bridging the current with the future), Talent Management, Learning & Development (i.e. to get from the present to the future what training interventions are required), Talent Sourcing (incl. Referrals), and Performance Management (i.e. among other things a useful source of data to inform Skills Matching!). As referenced in recent predictions for 2020 by a variety of authors (e.g. David Green, Visier, Bersin, etc.), we believe that “Skills are the new currency”! Kudos to Ian Bailie for one of our favourite stakeholder pitch lines.\n\n\n\n4. Social Networks appear niche\nTech Tags that appeared less frequently than expected, at least considering public hype, included Digital HR and Social Networks. This may simply reflect the companies present at Unleash 2019. However, it may also be suggestive of the niche nature of this technology / approach, and the slow rate of adoption among companies due to various reasons (e.g. data privacy implications, prevalence of skills, lack of available use cases, etc.)."
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html",
    "href": "posts/2021-06-06-going_the_distance/index.html",
    "title": "Going the Distance!",
    "section": "",
    "text": "Photo by Émile Séguin on Unsplash."
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html#calling-google-maps",
    "href": "posts/2021-06-06-going_the_distance/index.html#calling-google-maps",
    "title": "Going the Distance!",
    "section": "1. Calling Google Maps",
    "text": "1. Calling Google Maps\nTo use Google Maps you will need three things:\n\nAddress data. We begin by loading some fictitious address data provided by the Victorian State Government – School addresses from 2015.\nWorkplace addresses, both an old workplace address and a new workplace address. For this example, I am using the following two addresses from Victoria, Australia:\n\n\n\nOld workplace address: 154 High St, Ashburton VIC 3147, Australia (Ashburton Public Library); and\n\n\n\n\nNew workplace address: Spring St, East Melbourne VIC 3002, Australia (Victorian Parliament Building).\n\n\n\nA Google Maps API key, which can be set up on the Google Maps Developer Site. The Google Maps service has a free usage quota. To access Google Maps we will use the googleway library in R.\n\nWith all three pieces ready, we will then call Google Maps using the googleway::google_distance function. We will do this for two modes of transit:\n\nPublic Transport (called “Transit”) &\nCar.\n\n\n\nCode\n# Libraries\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(stringr)\nlibrary(rrapply)\n\nlibrary(googleway)\nlibrary(purrr)\nlibrary(echarts4r)\nlibrary(reactable)\n\n\n\n# # Set API key ----\nkey = my_api_key # enter your API key here\n  \n# Import Data ----\noriginal_locations_tbl <- readr::read_csv(file =\n\"http://www.education.vic.gov.au/Documents/about/research/datavic/dv165-allschoolslocationlist2015.csv\") %>% \n                          janitor::clean_names()\n\n\n\n# limit the address data to schools in the Greater Melbourne local government area's\ncouncils <- c(\"Bayside (C)\", \"Port Phillip (C)\", \"Stonnington (C)\", \"Casey (C)\", \"Melbourne (C)\", \n              \"Frankston (C)\", \"Glen Eira (C)\", \"Monash (C)\", \"Yarra (C)\", \"Moonee Valley (C)\")\n\n\n# create an address dataset\naddresses_tbl <- original_locations_tbl %>% \n  \n  # create and format the home address field, and create the old and new workplace addresses\n  dplyr::mutate(\n    home_address = base::paste0(address_line_1, \", \", address_town, \" \", address_state, \" \",address_postcode),\n    old_work_address = \"154 High St, Ashburton VIC 3147\",\n    new_work_address = \"Spring St, East Melbourne VIC 3002\",\n    employee         = paste0(\"Employee \", row_number())  \n    ) %>% \n  \n  # only include addresses from areas around Melbourne\n  dplyr::filter(lga_name %in% councils) %>% \n  \n  # randomly select 100 records\n  dplyr::sample_n(100) %>% \n  \n  dplyr::select(employee, home_address, old_work_address, new_work_address)\n  \n\n\n# check the dataset\n# head(addresses_tbl)\n\n\n# call Google Maps using googleway to calculate the distance and time for the old and new workplace locations\n# the calculations are repeated for both public transport and car modes of transport\ncommute_tbl <- addresses_tbl %>%\n\n  dplyr::mutate(\n        old_transit_calculations = purrr::map2(.x = home_address,\n                                               .y = old_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"transit\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        new_transit_calculations = purrr::map2(.x = home_address,\n                                               .y = new_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"transit\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        old_car_calculations     = purrr::map2(.x = home_address,\n                                               .y = old_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"driving\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE)),\n\n        new_car_calculations     = purrr::map2(.x = home_address,\n                                               .y = new_work_address,\n                                               .f = ~ googleway::google_distance(origins      = .x,\n                                                                                 destinations = .y,\n                                                                                 mode         = \"driving\",\n                                                                                 key          = key,\n                                                                                 simplify     = TRUE))\n\n    )"
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html",
    "href": "posts/2021-02-05-group_impact/index.html",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "",
    "text": "Photo by patricia serna on Unsplash."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html#ingesting-data",
    "href": "posts/2021-02-05-group_impact/index.html#ingesting-data",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "1. Ingesting Data",
    "text": "1. Ingesting Data\nThe first code chunk is fairly simple. We load our packages, read in our assessment results, clean the variable names, and finally take a simple look at the structure of the data to see what we are working with.\n\n\nCode\n# load the packages we will use for this workflow\nlibrary(tidyverse) # workhorse\nlibrary(readxl) # ingesting our data\nlibrary(janitor) # cleaning variable names\nlibrary(rstatix) # performing t-tests\nlibrary(ggpubr) # simple boxplots\n\n\n# Ingest our training assessment results\nassessment_results_tbl <- readxl::read_excel(path = \"training_data.xlsx\") %>% \n                          janitor::clean_names()\n\n\n# Get a feel for the data and its structure\ndplyr::glimpse(assessment_results_tbl)\n\n\nRows: 27\nColumns: 2\n$ training <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ result   <dbl> 56, 50, 52, 44, 52, 47, 47, 53, 45, 48, 42, 51, 42, 43, 44, 5…\n\n\nA key point to note is that in the “training” variable a response of “A” equates to the traditional face-to-face delivery mode, while “B” equates to the new digital learning format."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html#analysing-data",
    "href": "posts/2021-02-05-group_impact/index.html#analysing-data",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "2. Analysing Data",
    "text": "2. Analysing Data\nThe analysis comprised the following steps:\n\nFirstly, we performed a quick exploration of the data by getting some summary statistics.\nSecondly, we performed our t-test to determine if statistically significant differences existed in assessment outcome scores achieved by the two training formats.\n\nThese two steps are performed sequentially.\n\nStep 1.\n\n\nCode\n# 2a. get some summary stats\ndescriptive_stats_tbl <- assessment_results_tbl %>% \n  dplyr::group_by(training) %>% \n  rstatix::get_summary_stats(result, type = \"mean_sd\")\n\n\ndescriptive_stats_tbl\n\n\n# A tibble: 2 × 5\n  training variable     n  mean    sd\n  <chr>    <chr>    <dbl> <dbl> <dbl>\n1 A        result      15  47.7  4.42\n2 B        result      12  56.5  4.28\n\n\n\n\nInterpretation\nOur descriptive statistics indicate an interesting outcome. It appears a notable difference exists between training formats in the mean score achieved on the assessment outcome, Training B (digital) seems to achieve on average a higher score (56.5) than Training A (face-to-face). However, the question remains–is this difference statistically significant?\n\n\n\nStep 2.\nBefore we can perform the t-test we must check several assumptions to ensure a t-test is appropriate. The assumptions are as follows:\n\n\n\n\n\n\n\nAssumption\nOutcome\n\n\n\n\n1. The data are continuous (not discrete).\nOur assessment outcomes scores are continuous - CHECK\n\n\n2. The data follow the normal probability distribution.\nWe will test for normality using a Q-Q plot (Quantile-Quantile plot) and the Shapiro-Wilk’s test.\n\n\n3. The variances of the two populations are equal.\nWe will test for equal variance using the Levene Test.\n\n\n4. The two samples are independent.\nOur training participants could only participate in one type of training (i.e., face-to-face or digital) so our samples are independent - CHECK\n\n\n5. Both samples are simple random samples from their respective populations.\nOur participant results were randomly selected for analysis - CHECK!\n\n\n\nIf the data is normally distributed and the variance across the two groups (e.g. Training Groups A & B) are equal then a t-test can be used effectively. Let’s check the last two assumptions and then analyse the data!\n\n\nCode\n# 2b. Normality Tests\nggpubr::ggqqplot(assessment_results_tbl$result,\n                 title = \"Normality Assessment\") %>% \n  plotly::plotly_build()\n\n\n\n\n\n\nCode\nrstatix::shapiro_test(assessment_results_tbl$result)\n\n\n# A tibble: 1 × 3\n  variable                      statistic p.value\n  <chr>                             <dbl>   <dbl>\n1 assessment_results_tbl$result     0.958   0.338\n\n\nCode\n# 2c. Equal Variance - levene's test\nlevene_test(result ~ training, data = assessment_results_tbl)\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     1    25     0.242 0.627\n\n\nCode\n# 2d. perform the t-test\n(t_test_tbl <- assessment_results_tbl %>% \n  rstatix::t_test(formula = result ~ training, var.equal = TRUE) %>% \n  rstatix::add_significance())\n\n\n# A tibble: 1 × 9\n  .y.    group1 group2    n1    n2 statistic    df         p p.signif\n  <chr>  <chr>  <chr>  <int> <int>     <dbl> <dbl>     <dbl> <chr>   \n1 result A      B         15    12     -5.20    25 0.0000223 ****    \n\n\n\n\n\nInterpretation\nNormality Our QQ plot suggests that we can assume normality. The distribution of results are falling approximately along the reference line and within the bounds. However, the QQ plot can be sensitive to small sample sizes. To further validate normality we performed the Shapiro Wilk’s test. This test again suggests that the data are normally distributed, as the p-value (0.338) is greater than 0.05. We can assume normality!\nEqual Variance We use the levene’s test to determine whether both the training methods have an approximately equal variance. Since our p-value (0.627) is greater than 0.05 we can assume that the variance in outcome assessments scores is not statistically significantly different for the two training methods. We can assume equal variance! All our assumptions are met and we can perform the t-test.\nt-test The t-test indicates that the difference is highly significant (p-value = 0.0002); well below a cut-off of 0.01. This difference is extremely unlikely to be by chance alone and suggests that the digital training yields a better outcome, at least as defined by the assessment outcomes."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html#visualising-results",
    "href": "posts/2021-02-05-group_impact/index.html#visualising-results",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "3. Visualising Results",
    "text": "3. Visualising Results\nOur next step is to visualise the differences achieved by the two training formats. The intent here is to ensure our stakeholders can clearly appreciate that a notable difference exists, and subsequently take an investment decision with confidence. To illustrate the difference observed, we opted to use a Box-Plot, which displays the distribution of scores in each group, coupled with some descriptive statistics. As with our QQ plot, we have used the Plotly package to make the plot interactive.\n\n\nCode\n# 3. visualise the results from the two training methods\n\ntraining_plot <- assessment_results_tbl %>%  \n                 ggpubr::ggboxplot(x     = \"training\", \n                                   y     = \"result\",\n                                   fill  = \"training\",\n                                   xlab  = \"Training Delivery Format\",\n                                   ylab  = \"Assessment Result\") \nt <- list(\n  family = \"arial\",\n  size = 12)\n\n\n# make the graph interactive\nplotly::plotly_build(training_plot) %>% \n  plotly::layout(title = \"<b>Difference in Assessment Results by Training Delivery Format</b>\",\n                 font = t,\n                 showlegend = FALSE) \n\n\n\n\n\n\n\n\nInterpretation\nOur visualisation seeks to clearly illustrate that employees who completed the digital training performed better on the end-of-course assessment. This may suggest that the digital training results in better knowledge retention (i.e., they are learning more).\nThe line crossing each box on the boxplot indicates the median score of each training format. The median is the middle value achieved on the assessment results for each training format. There is a clear difference between the two groups when comparing these lines. This is worth calling out when delivering the results to stakeholders."
  },
  {
    "objectID": "posts/2021-02-05-group_impact/index.html#quantifying-benefit",
    "href": "posts/2021-02-05-group_impact/index.html#quantifying-benefit",
    "title": "Measuring Impact in HR: A Practical Demonstration",
    "section": "4. Quantifying Benefit",
    "text": "4. Quantifying Benefit\nThere are two key ways in which we can further quantify the differences of the training formats:\n\nThe percentage change in performance. We have identified the gain is statistically significant and visualised the difference above. We can make this increase in performance more accessible to a broader audience by indicating the performance gain as a percentage.\nThe cost difference of the delivery formats. By calculating the cost difference of delivering the training over a year or more for the two formats.\n\nOn the basis of these two metrics we are in a better position to recommend a future course of action.\nPerformance Efficiency\nUsing the mean score from each training group we can calculate the % gain in performance. The code is presented in the code chunk below.\n\n\nCode\n# 4. quantify the benefit  \n# mean assessment score for f2f training\ntrain_A_mean <- descriptive_stats_tbl %>% \n  dplyr::filter(training == \"A\") %>% \n  pull(mean)\n\n\n# mean assessment score for digital training\ntrain_B_mean <- descriptive_stats_tbl %>% \n  dplyr::filter(training == \"B\") %>% \n  pull(mean)\n\n\n# calculate % increase in performance\n(pct_increase = ((train_B_mean - train_A_mean)/ train_A_mean) %>% scales::percent())\n\n\n[1] \"18%\"\n\n\nCost Efficiency\nBelow is an except from an excel spreadsheet in which we have detailed costs associated with both training formats.\n\n\nCode\nknitr::include_graphics(\"training_costs_3.png\")\n\n\n\n\n\nCost of training offerings.\n\n\n\n\n\n\nInterpretation\nOur performance gains are straightforward–digital training results in end of course assessment scores that are 18% higher than traditional face-to-face delivery. This kind of result is accessible to all audiences, and is compelling! In addition, we are confident that the result is a function of the training delivery format based on the results of our t-test.\nOur simple cost assessment also creates a compelling picture of the financial savings associated with digital training. In one year we stand to create $8,400 in savings if we move to a digital format, and this increases to $21,800 at the end of two years."
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html#unpacking-the-data",
    "href": "posts/2021-06-06-going_the_distance/index.html#unpacking-the-data",
    "title": "Going the Distance!",
    "section": "2. Unpacking the Data",
    "text": "2. Unpacking the Data\nThe data returned from the google_distance function is complicated! Consequently, it required some fiddling to unpack and format in a usable fashion. While the approach developed works, I strongly suspect it could be better. Searching online has yielded several alternatives. However, I decided to stay with my own and welcome suggested improvements.\n\n\nCode\n# cleaning function for results\nresults_cleaner <- function(data, old_or_new, car_or_transit){\n    \n    # for renaming multiple variables from the API call\n    var_string <- paste0(old_or_new, \"_\", car_or_transit, \"_\")\n    # for renaming the status field\n    new_name   <- paste0(var_string, \"status\")\n    \n    # receive the data\n    data |> \n        rrapply::rrapply(how = \"bind\") |> \n        janitor::clean_names() |> \n        dplyr::select(-1, -2, -7) |> # remove the last two columns\n        dplyr::rename_at(vars(starts_with(\"rows_elements_1_\")), ~ str_replace_all(., \n                                                                              pattern     = \"rows_elements_1_\",\n                                                                              replacement = var_string) \n                                                                              ) |> \n        dplyr::rename( !! quo_name(new_name) := status)\n        \n}\n\n\n# clean the final results for the old commute by public transport\nold_transit_commute_tbl <- commute_tbl %>%\n    dplyr::select(old_transit_calculations) %>%\n    results_cleaner(old_or_new = \"old\", car_or_transit = \"transit\")\n\n\n# clean the final results for the new commute by public transport\nnew_transit_commute_tbl <- commute_tbl %>%\n    dplyr::select(new_transit_calculations) %>%\n    results_cleaner(old_or_new = \"new\", car_or_transit = \"transit\")\n\n# clean the final results for the old commute by car\nold_car_commute_tbl <- commute_tbl %>%\n    dplyr::select(old_car_calculations) %>%\n    results_cleaner(old_or_new = \"old\", car_or_transit = \"car\")\n\n\n# clean the final results for the old commute by car\nnew_car_commute_tbl <- commute_tbl %>%\n    dplyr::select(new_car_calculations) %>%\n    results_cleaner(old_or_new = \"new\", car_or_transit = \"car\")\n\n\n\n\n# connect the original data with the new clean distance and time results\ntotal_commute_tbl <- addresses_tbl %>%\n    dplyr::bind_cols(old_transit_commute_tbl) %>% # add the columns of the old commute by public transport\n    dplyr::bind_cols(new_transit_commute_tbl) %>% # add the columns of the new commute by public transport\n    dplyr::bind_cols(old_car_commute_tbl) %>% # add the columns of the old commute by car\n    dplyr::bind_cols(new_car_commute_tbl) %>% # add the columns of the new commute by car\n\n    # filter out any results that were not OK\n    dplyr::filter(old_transit_status == \"OK\" | \n                  new_transit_status == \"OK\" | \n                  old_car_status     == \"OK\" | \n                  new_car_status     == \"OK\")"
  },
  {
    "objectID": "posts/2021-06-06-going_the_distance/index.html#creating-insights",
    "href": "posts/2021-06-06-going_the_distance/index.html#creating-insights",
    "title": "Going the Distance!",
    "section": "3. Creating Insights",
    "text": "3. Creating Insights\nThe data returned from googleway provides both the travel distance between the two points, and time required to complete the trip for the selected mode of transit. From here it is important to gauge the following:\n\n\n1. Are people generally better off travelling to the new office location?\nThis can be examined by both distance and time. However, in a city a short trip can still take considerable time. Therefore, examining the difference in time taken for the shortest option (i.e., car or public transport) appears more realistic / accurate. If the time decreased by more than 5 minutes, an improvement in commute is assumed.\n\n\nCode\n# calculate value in Minutes (default results are in seconds)\nscale_mins <- function(x){\n  x <- x / 60 \n  round(x, digits = 0)\n}\n\n\n# calculate value in Kilometres (default results are in metres)\nscale_kms <- function(x){\n  x <- x / 1000\n  round(x, digits = 1)\n} \n\n\n\n\nfinal_commute_details_tbl <- total_commute_tbl |> \n    \n    # covert all duration variable values to minutes\n    dplyr::mutate_at(vars(ends_with(\"duration_value\")), scale_mins) %>% \n    \n      \n    # covert all distance variable values to kilometres\n    dplyr::mutate_at(vars(ends_with(\"distance_value\")), scale_kms) %>% \n  \n    # calculate the difference in distance and time between the new and old locations\n    # NOTE: calculations are based on new - old. Therefore, negative numbers indicate less distance and time, \n    # whereas positive numbers indicate an increase in distance and time\n    dplyr::mutate(\n        diff_transit_commute_distance = new_transit_distance_value - old_transit_distance_value,\n        diff_transit_commute_time     = new_transit_duration_value - old_transit_duration_value,\n        diff_car_commute_distance     = new_car_distance_value - old_car_distance_value,\n        diff_car_commute_time         = new_car_duration_value - old_car_duration_value,\n\n        # find the shortest time (i.e., transit or car) for both the old and new offices\n        old_office_min_mins = base::ifelse(old_transit_duration_value < old_car_duration_value, \n                                           old_transit_duration_value, old_car_duration_value),\n        new_office_min_mins = base::ifelse(new_transit_duration_value < new_car_duration_value, \n                                           new_transit_duration_value, new_car_duration_value),\n        \n        # find the differences in the best commute times between the two locations\n        new_office_best_commute_diff = new_office_min_mins - old_office_min_mins,\n        \n        # find which is the better commute for each person (new office, old office, or similar). This assumes \n        # that a commute is better if the commute time is reduced by more than 5 minutes\n        preferred_commute_location   = dplyr::case_when((old_office_min_mins - new_office_min_mins) < -5 ~ \"Old Office\",\n                                                        (new_office_min_mins - old_office_min_mins) < -5 ~ \"New Office\",\n                                                        TRUE ~ \"Similar Commute\"),\n        \n        # determine the best mode of transport for the new location\n        preferred_mode_transport     = base::ifelse(new_transit_duration_value <= new_car_duration_value, \"Public Transport\", \"Car\")\n                                        \n        )\n\n\n# visually represent the impact of the new location to travel time\nfinal_commute_details_tbl %>% \n  dplyr::count(preferred_commute_location) %>% \n  echarts4r::e_chart(preferred_commute_location) %>% \n  echarts4r::e_bar(n) %>% \n  echarts4r::e_labels(position = \"insideTop\") %>% \n  echarts4r::e_legend(show = FALSE) %>% \n  echarts4r::e_title(text = \"Preferred Office Location Based on Travel Time\",\n                     \"Preferred Location Is Defined by A Reduction In Travel Time Of More Than 5 Minutes\",\n                     left = \"10%\") %>% \n  echarts4r::e_tooltip() \n\n\n\n\n\n\nThe visualisation above depicts how the location change will impact employee travel, and it appears the majority of individuals will be worse off when commuting to the new location.\n\n\n\n2. Which individuals will have shorter or longer travel times?\n\n\nCode\nfinal_commute_details_tbl %>% \n  \n  # select the preferred variables\n  dplyr::select(employee, home_address, preferred_commute_location, new_office_best_commute_diff,preferred_mode_transport, \n                new_transit_duration_value, diff_transit_commute_time, new_car_duration_value, \n                diff_car_commute_time) %>% \n  \n    dplyr::arrange(new_office_best_commute_diff) %>% \n  \n  # rename the variables for more readable output\n  dplyr::rename(\n                Employee = employee,\n                `Home Address`= home_address,\n                `Preferred Office` = preferred_commute_location,\n                `Commute Change (Mins)` = new_office_best_commute_diff,\n                `Preferred Transport for New Office` = preferred_mode_transport,\n                `Time via Public Transport (Mins)` = new_transit_duration_value,\n                `Public Transport Time Saving (Mins)` = diff_transit_commute_time,\n                `Time via Car (Mins)` = new_car_duration_value,\n                `Car Time Saving (Mins)` = diff_car_commute_time) %>% \n  \n  reactable::reactable(\n    # pagination and searching options\n    showPageSizeOptions = TRUE,\n    pageSizeOptions = c(5, 10, 25, 50, 100),\n    defaultPageSize = 5,\n    \n    # table formatting options\n    outlined = TRUE,\n    borderless = TRUE,\n    striped = TRUE,\n    resizable = TRUE,\n\n    # column width\n    columns = list(\n      Employee = reactable::colDef(minWidth = 150),\n      `Home Address` = reactable::colDef(minWidth = 175),\n      `Preferred Office` = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Commute Change (Mins)` = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Preferred Transport for New Office`  = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Time via Public Transport (Mins)`  = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Public Transport Time Saving (Mins)`  = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Time via Car (Mins)` = reactable::colDef(minWidth = 150, align = \"center\"),\n      `Car Time Saving (Mins)`= reactable::colDef(minWidth = 150, align = \"center\")\n    ),\n    \n    # theming changes    \n    theme = reactable::reactableTheme(\n      highlightColor = \"#f0f5f9\",\n      cellPadding = \"8px 12px\",\n      style = list(fontFamily = \"-apple-system, BlinkMacSystemFont, Segoe UI, Helvetica, Arial, sans-serif\"))\n  )\n\n\n\n\n\n\n\n\nThe above table could readily be exported to Excel and delivered to Leaders, Managers, Transition Teams, HRBP’s, etc., to help inform the delivery of personalised discussions with staff regarding proposed or upcoming changes. In addition, it could also help to inform the identification of voluntary flight risks, based on negative travel experiences associated with the move. Moreover, additional analyses could be performed on this data to help inform communication efforts (e.g., clustering to develop employee personas)."
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html",
    "href": "posts/2020-12-09-employee_skills_data/index.html",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "",
    "text": "Lachlan, a people analyst at a global company, sits down with the organisation’s Chief Strategy Officer (CSO). The CSO quickly gets to the point, stating: “Lachlan, I need your help finding a specialist to head up a new business. The opportunity is ripe—so we need to get started as soon as possible. By the end of the week, I need you to have identified a leader with the following skill set: experience in front- and back-end development, working knowledge of Amazon Web Services (AWS), knowledge of the South East Asian IT market, and fluent in Mandarin. We’ve got roughly 60,000 global employees, so surely there must be someone who fits the bill—right?”.\nLachlan pauses for a second before responding: “I honestly don’t know. I can certainly reach out to some contacts and look at the different datasets that we have internally, but I’m not 100% certain that I can find someone with the precise skill set that you’re looking for”.\nThe CSO frowns and purses her lips. “Okay, well how about externally? Can you find me someone from outside the company?”. Upon hearing this, Lachlan immediately looks relieved, replying: “Definitely—I can give you a detailed shortlist of specialists by close-of-business tomorrow. Would that be okay?”. The CSO smiles and nods, before adding: “How is it though that you can’t find me someone from our global internal population of 60,000, but you can provide me a list of people in 24 hours among a global population of approximately 8 billion people?”\nLachlan pauses, considering the problem at hand. After a few seconds, he responds simply: “Well, it’s all down to the quality of data at my disposal”.\nThis anecdote illustrates a common problem among companies of all sizes—most organisations are unaware of the skills residing within their employee population. The notion of employee skill data has garnered considerable interest of late, with interested parties ranging all the way from:\nSo what unites these disparate parties? Well, it’s ultimately the realisation that old skills management practices—specifically, an inability to catalogue the “gold” within the organisation—is untenable.\nThis article explores the two fundamental considerations when it comes to skills management. First, how do you acquire your employees’ relevant skill data? Second, what practical value can you then generate from this wealth of information?"
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html#how-to-acquire-employees-skill-data",
    "href": "posts/2020-12-09-employee_skills_data/index.html#how-to-acquire-employees-skill-data",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "How to Acquire Employees’ Skill Data",
    "text": "How to Acquire Employees’ Skill Data\nThe introductory anecdote highlights one key issue: an employee skills catalogue entirely depends upon quality data. So how about asking individual employees to complete and maintain their own employee skills profile (i.e. a company-specific profile similar to LinkedIn)?\nFor starters, this is unsustainable. Employees might forget to update their profile, feel like they’re too busy, or simply not see the benefits of this activity.\nAs a result, organisations must:\n\nMinimise the amount of effort needed from each individual employee.\nFully exploit existing data as intelligently as possible.\nGenerate clear-cut benefits for both individuals and wider organisations alike.\n\nAnd it appears that advancements in AI—specifically Natural Language Processing (NLP)—might hold the key. NLP has already been used widely within the HR industry, especially in the field of Talent Acquisition. This technology, when used correctly, connects and distils numerous data sources to make previously unknown connections.\nFor example, the following data sources—while often scattered across the HR / IT landscape and laying dormant—already contain a wealth of valuable information about individual employees.\n\nRecruitment data: Resumes, cover letters, assessments, etc.\nCareer history data: Roles previously held by individuals within an organisation, role titles, job descriptions, project descriptions, leadership roles, awards won, etc.\nLearning & Development data: Courses undertaken and completed, development objectives, mentoring, certifications, 360-degree feedback, psychometric assessments, etc.\nPerformance data: Performance objectives, performance appraisals, written feedback by peers and stakeholders, etc.\nExternal data: Professional networking sites, professional publications, patents, blogs, etc.\n\nAll of these sources (well, all aside from external data) have long been standard data collection mechanisms within the HR industry. If finance is considered the home of numbers-based insights, then HR is undoubtedly the home of text-based insights. Organisations can therefore automate the engineering of skill data by combining:\nThe various forms of employee data currently held in disparate organisational HR/IT systems.\nNext-generation NLP technologies that can perform realistic interpretations of text-based data.\n\n\n\n\n\nVisual representation of the process of automating 1. Ingestion of employee data currently in organisations, and 2. Interpretation of content via NLP to develop realistic skill profiles.\n\n\n\n\nThe above image visually demonstrates this process. By analysing and applying relevant skill data, organisations can achieve evidence-based decision-making when it comes to personnel matters. Needless to say, this brings considerable value to both individual employees and the broader organisation."
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html#the-practical-value-you-can-generate-with-skill-data-across-the-employee-lifecycle",
    "href": "posts/2020-12-09-employee_skills_data/index.html#the-practical-value-you-can-generate-with-skill-data-across-the-employee-lifecycle",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "The practical value you can generate with skill data across the employee lifecycle",
    "text": "The practical value you can generate with skill data across the employee lifecycle\nIt’s all well and good acquiring relevant skill data, but how can it be put to good use? If used correctly, skill data can guide a wide variety of employee touchpoints within an organisation. For instance, the very process of identifying the right person for a specific role (as per the introductory example) is a foundational application of employee skill data.\n\n\nFigure: Overview of where skills data can be used for both employee and organisation\nThe following diagram gives an overview of the areas where skills data can be put to good use for the advantage of the organisation and the advantage of the employee.\n\n\n\n\n\nOverview of the areas where skills data can be put to good use for the advantage of both the employee and organisation.\n\n\n\n\n\n\nTable: The benefits of skills data for both employee and organisation\nThe following table lists several other potential applications, both from the perspective of the employee and the organisation.\n\n\n\n\n\n\n\n\nEmployee Lifecycle Stage\nEmployee Value\nOrganisation Value\n\n\n\n\nRecruitment\nObjective recruiting decisions that provide personalised feedback: skills are inferred from application data sources (e.g. CV, LinkedIn, etc.) and skill-based matches are provided to the recruiter as an additional lens. As a result, objective and tailored feedback can be provided to unsuccessful candidates (i.e., no more generic “unsuccessful” emails from organisations), which can help them take future action, e.g. upskilling or tailoring their job search. For successful candidates, the inferred skills captured can be used to personalise the onboarding process.\nObjective skill-based recruiting decisions: skills can be inferred from a previous role incumbent and/or data sources that describe the role (e.g., job description, performance objectives, etc.). The vacancy is then more accurately described in terms of objectives and required skills. This provides greater accuracy for candidates and the organisation, thereby improving recruitment outcomes, saving time and money. Furthermore, the influx of skills into the organisation can be captured and subsequently analysed for trends and insights. Are critical skills being recruited at an adequate pace? Which skills are difficult to source and should be built?\n\n\nOnboarding\nExpediting employee integration: new employees are immediately included in the employee skill catalogue. This enables the immediate personalisation of learning recommendations, mentoring opportunities, and inclusion in organisational projects. Moreover, because expertise is immediately captured, employees can play an immediate role as SME’s where applicable. This enables value contribution and subsequently engagement and satisfaction, which may otherwise take months to establish.\nExpediting employee productivity: including employee skill details in the organisational catalogue expedites the speed with which new employees can be included in work deliverables, improving employee time to productivity. In addition, skill gaps between the vacancy and candidate are assessed automatically, thereby enabling faster closure of skill gaps through training and mentor recommendations.\n\n\nDevelopment\nPersonalised employability assessment and development recommendations: skills data helps to compare each employee to both the internal and external environment to determine their employability. Employees can undertand how desirable their skillset is, which in turn becomes a great motivator for learning. Organisations are better equipped to provide personalised learning recommendations to employees, based on their current skills and market skill trends. Finally, by tying learning recommendations to opportunities within the company (also known as talent markeplace), employees can explore new avenues for career growth.\nOptimising training offerings: Organisations can make more objective and intelligent decisions regarding their Learning System investments, by answering questions such as: How closely does a system mirror the skills an organisation currently has, and will need in future? Does a system mirror skill trends in the different geographic markets, or just one market? How are new training offerings prioritised by the provider and will this mirror future skill requirements of the organisation? Secondly, organisations can make more effective Strategic Workforce Planning decisions regarding which skills are “built” vs “bought”. Thirdly, organisations can compare employee skills to external market trends. This transparency, when shared with the workforce, can help to facilitate a culture of learning.\n\n\nTalent Identification\nObjective visibility and identification of talent (i.e., non-political) that enables better employee experiences: skill based identification surfaces better employee opportunities for awards and recognition, funding opportunities, project sponsorship, project involvement, skill and career growth, and learning programs (e.g., high performer investment, university course sponsorship, etc.).\nOptimising staffing decisions: rapid identification of resident skills facilitates more effective project staffing, team configuration and ultimately organisational design. An employee skills catalogue enables decision making at scale. Better staffing decisions can directly impact innovation and productivity outcomes. The employee skills catalogue enables greater ease and precision of talent (e.g., who has skills of greatest organisational value), internal job mobility and succession.\n\n\nCareer Growth\nGreater career transition opportunities: A skills-based talent marketplace enables employees to determine how transferable their skills are to other roles in their local or global organization. Which roles are currently open that an employee could transition into? Which roles are realistic career growth opportunities?\nLeveraging internal talent to maximum benefit: many organisations find it easier to identify and source talent externally than internally. By creating a skills-based talent marketplace, organisations can leverage the employee skills catalogue to identify and source talent with corporate knowledge, expediting onboarding. Moreover, enabling employees to easily identify opportunities internally may facilitate longer careers within organisations, thereby reducing turnover and the costs associated with recruitment, onboarding, etc.\n\n\nManaging Change\nIncreasingly objective, and transparent decisions with personalised feedback. Skill based decisions provide all employees with visibility during change, not just those who are well-networked. Decisions can be made in a way that systematically consider current state, SWP, employee skill utility, and thereby provide individual feedback in a transparent and valuable fashion.\nObjective and intelligent management of skill capital during change. A skills catalogue may inform the due diligence process (i.e., what are we acquiring), retention (i.e., which skill sets represent a unique value-add), and the post-sale integration process (i.e., configuring new organisations or project teams). Organisations can systematically determine the skills each employee provides the workplace as well as understanding, during times of restructure, which skills are being lost or gained during various change scenarios. This affords organisations an opportunity to remove some of the uncertainty associated with change.\n\n\nStrategic Workforce Planning\n\nConnecting the future skill state with the current skill state. It is impossible to effectively work toward a future state skill target, without knowing which skills are currently resident within the organisation. Such understanding will inform build or buy skill decisions and learning investments.\n\n\nOperational Efficiency / Knowledge Management\nFinding help (i.e., skills, corporate knowledge, resources, etc.) through an internal talent marketplace. Who has the skills related to my project(s) who may not be within my current network of contacts? Has something similar been done before? Having a transparent view of the skills across the workforce means that employees can connect with others who have expertise more effectively thus proactively catalyzing innovation and productivity. Employees no longer have to wait to “hear” about new communities of practice in an information-overload environment\nAn indirect approach to knowledge management and talent management can boost inclusivity across the workforce as more informal and formal network connections are facilitated. In turn, the impact of a more transparent, inclusive approach to the workforce could have significant impacts on developing the right culture at your organisation.\n\n\nOffboarding\nA transferable skills record that promotes employability: Leaving an organisation with a systematic, objective and comprehensive assessment of skills enables individual employability. Moreover, this benefit would ideally engender goodwill between ex-employee and organisation, encouraging advocacy and loyalty, and provide a skill passport that could readily enable future re-employment.\nIdentifying skills gaps before they are felt: The outflow of skills in the organisation can be captured and subsequently analysed for trends and insights. Are any critical skills leaving the company? How are departing skills being addressed by Talent Acquisition, Career Mobility and L&D activities?\n\n\n\nThe above list, while not exhaustive, illustrates the various ways in which skill data can underpin many major employee touch points within an organisation. The greatest benefit of skill based personnel decision making is that it provides an objective, comprehensive, and close to real time perspective of the organisation - an outcome previously beyond the reach of organisations."
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html#why-skills-could-be-the-new-business-currency",
    "href": "posts/2020-12-09-employee_skills_data/index.html#why-skills-could-be-the-new-business-currency",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "Why skills could be the new business currency",
    "text": "Why skills could be the new business currency\nSkills management has been around since the eighties. However, it has historically been complicated, highly resource-intensive, and, even worse, yielded questionable tangible value to the organisation as a whole.\nBut that’s all about to change. Technological advancements mean that organisations can identify and manage their employees’ skill sets more easily than ever before. The above discussion detailing prospective applications of skill data demonstrates the breadth of utility, and potential for objective enhancement of personnel decision making across the employee lifecycle. Moreover, skill data provides a globally universal currency for organisational decision making, thereby enabling personalised employee decision making at scale that transcends geographic boundaries.\nBy maintaining a continuous overview of the capabilities that lie within their organisation, they’ll be able to meet the future head-on—no matter what challenges they face."
  },
  {
    "objectID": "posts/2020-12-09-employee_skills_data/index.html#acknowledgments",
    "href": "posts/2020-12-09-employee_skills_data/index.html#acknowledgments",
    "title": "The Value of Measuring Employee Skill Data",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis article was first published on the myHRfuture website under the title “The Value of Measuring Employee Skill Data” on November 25th, 2020."
  }
]