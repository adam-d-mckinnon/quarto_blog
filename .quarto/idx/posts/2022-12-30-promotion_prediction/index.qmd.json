{"title":"Predicting Promotions Through Machine Learning","markdown":{"yaml":{"title":"Predicting Promotions Through Machine Learning","date":"2022-12-30","description":"Building an XGBoost model in the Tidymodels ecosystem that predicts whether an employee should be promoted.","author":[{"name":"Adam D McKinnon"}],"categories":["Tidymodels","XGBoost","R","Machine Learning","Employee Promotions"],"image":"markus-spiske-QozzJpFZ2lg-unsplash.jpg","title-block-banner":true},"headingText":"Libraries","containsRefs":false,"markdown":"\n\n```{r header, echo=FALSE, code_folding = FALSE, fig.cap=\"Photo by [Possessed Photography](https://unsplash.com/@markusspiske) on [Unsplash](https://unsplash.com/).\", out.width = '100%'}\nknitr::include_graphics(\"markus-spiske-QozzJpFZ2lg-unsplash.jpg\")\n\n```\n\n<br>\n\nThe current article is focussed on tuning an xgboost model in the Tidymodels ecosystem to predict promotions using a fictitious employee dataset. The article is intended to complement and earlier [article](https://www.adam-d-mckinnon.com/posts/2023-04-05-using_ai_for_promotions/) focused on the theoretical utility of promotion models.\n\n\n```{r loading_libraries}\n\n# data manipulation\nlibrary(readxl)\nlibrary(tidyverse)\n\n# modelling\nlibrary(tidymodels)\nlibrary(themis)\nlibrary(finetune)\nlibrary(bundle)\nlibrary(cvms)\n\n# Processing power\nlibrary(doParallel)\nlibrary(parallelly)\n\n# Visualisation\nlibrary(plotly)\n\ntidymodels_prefer()\n\n```\n\n<br>\n\n# Data\n\n```{r loading_data}\n\n# Load Data ----\npromotions_tbl <- readxl::read_excel(path = \"2022_12_15_promotions.xlsx\")\n\n\npromotions_tbl <- promotions_tbl |> \n    mutate(promoted  = forcats::as_factor(promoted) %>% forcats::fct_relevel(\"promoted\", \"not promoted\")) |>  \n    mutate_at(.vars = c(\"gender\", \"work_site\", \"management_level\"), .funs = ~ forcats::as_factor(.))\n\n\n\n```\n\n<br>\n\n# Building an ML Model\n\n### 1. Splitting the data\n\n```{r splitting_data}\n\n# Spending the dataset ----\n\nset.seed(836)\npromotion_split     <- initial_split(promotions_tbl, strata = promoted)\npromotion_train_tbl <- training(promotion_split)\npromotion_test_tbl  <- testing(promotion_split)\n\n\nset.seed(234)\npromotion_folds <- bootstraps(promotion_train_tbl, \n                              times = 75, # default is 25 - inflated to accommodate racing method of tuning \n                              strata = promoted)\n\n# check the promotion_folds \n# promotion_folds\n\n\n\n```\n\n<br> \n### 2. Pre-processing the data\n\n```{r data_preprocessing}\n\n# Data Pre-processing ----\nxgboost_recipe <- \n    recipe(formula = promoted ~ ., data = promotion_train_tbl) |>  \n    recipes::update_role(employee_id, new_role = \"id\") |>  \n    step_dummy(all_nominal_predictors(), one_hot = TRUE) |>  \n    step_zv(all_predictors()) |> \n    step_rose(promoted)\n\n\n\n\n# check the recipe\n# xgboost_recipe\n\n```\n\n<br>\n\n### 3. Create a model specification\n\nThe model specification is created below. With the exception of the number of trees, all other standard parameters are tuned to find the best combination.\n\n<br>\n\n```{r model_spec}\n\n# Model Set-up ----\nxgboost_spec <- \n    boost_tree(trees = 1000, \n               tree_depth = tune(),\n               min_n = tune(),\n               mtry = tune(),\n               learn_rate = tune()) |>  \n    set_engine(\"xgboost\") |>  \n    set_mode(\"classification\")\n\n\n# check the model specification\n# xgboost_spec\n\n```\n\n<br>\n\n### 4. Workflow setup\n\nBelow we simply create the workflow and add the recipe and model specification.\n\n<br>\n\n```{r workflow_setup}\n\n\n# Workflow setup\nxgboost_workflow <- \n    workflow() |> \n    add_recipe(xgboost_recipe) |>  \n    add_model(xgboost_spec) \n\n# Check the workflow\n# xgboost_workflow\n\n\n```\n\n<br>\n\n### 5. Tuning the model\n\nThree key activities occur when tuning the model. These are:\n\n1.  Specifying the metrics that will be used to assess the model. In practice only one metric is used in the 'tune_race_anova' function, which is always the first specified in the 'metric_set' function. If a metric is not defined the defaults are either accuracy of RMSE, depending upon the model type.\n\n2.  The next step is to enable parallel processing, using the 'availableCores' function, to expedit the tuning process.\n\n3.  Finally, we define the 'tune_race_anova' function, specifying the workflow, resamples, and metrics. It is important to save the predictions from the tuning process.\n\n<br>\n\n```{r model_tuning}\n\n# specify the metrics of interest\n# NOTE: The first metric listed will be used for tuning\npromotion_metrics <- metric_set(\n                            roc_auc, \n                            accuracy, \n                            sensitivity, \n                            specificity\n                            )\n\n\n# enable parallel processing based on the number of available cores\ndoParallel::registerDoParallel(cores = parallelly::availableCores())\n\n\nset.seed(826)\nracing_resamples <- finetune::tune_race_anova(\n    xgboost_workflow,\n    resamples = promotion_folds,\n    grid = 100, # cast a wide grid to optimise the results -\n                # works best with many resamples - set earlier to 75\n    metrics = promotion_metrics,\n    control = control_race(\n        verbose_elim = TRUE,\n        save_pred    = TRUE\n        )\n)\n\n\n# racing_resamples\n\n\n\n```\n\n<br>\n\n### 6. Assess model performance\n\nHere we can look at the results of the model tuning process in two ways:\n\n1.  The model metrics for the combination that \"won\" the anova race; and\n2.  The plot of the tuning process. The plot shows the number of model combinations that were dropped early in the process (i.e., a considerable time saving) to reach the combination that won the process. This can be an efficient way of testing multiple model parameters efficiently and effectively!\n\n```{r assess_model}\n\nfirst_model_metrics_tbl <- collect_metrics(racing_resamples)\ntuning_plot <- plotly_build(plot_race(racing_resamples))\n\nxaringanExtra::use_panelset()\n\n```\n\n::: panelset\n::: panel\n[Promotion Metrics]{.panel-name}\n\n```{r echo=FALSE, code_folding = FALSE}\n\nfirst_model_metrics_tbl |>  gt::gt()\n\n```\n:::\n\n::: panel\n[Model Tuning Visualisation]{.panel-name}\n\n```{r echo=FALSE, code_folding = FALSE}\n\ntuning_plot\n\n```\n:::\n:::\n\n<br>\n\n### 7. Finalise the workflow\n\n```{r finalise_workflow}\n\n# last_fit_xgboost_workflow\nlast_fit_xgboost_workflow <- xgboost_workflow |> \n    finalize_workflow(select_best(racing_resamples, \"roc_auc\")) |> \n    last_fit(promotion_split)\n\n# test the fit\n(final_model_workflow_metrics <- collect_metrics(last_fit_xgboost_workflow) |> gt::gt())\n\n# extract the model workflow for further testing & saving\nfinal_model_workflow <- last_fit_xgboost_workflow |> \n    extract_workflow()\n\n\n```\n\n<br>\n\n### 8. Re-assess model performance\nThe confusion matrix provides an overview of our success with the model.\nInterestingly, when the process was run without addressing the class imbalance in the recipe the ability to predict the minority class (i.e., True Positive) decreased by ~12%. However, this came at the expense of the ability to predict the True Negative (bottom right of confusion matrix), which decreased by ~9%. \n\n\n```{r predictions}\n\n# test the model\npred_test <- final_model_workflow |> \n    predict(promotion_test_tbl) |> \n    bind_cols(promotion_test_tbl)\n\n# Visualise the performance using a confusion matrix\npred_test |> \n    # retrieve the relevant variables\n    select(.pred_class, promoted) |> \n    # convert the text to numeric format\n    mutate_all(~if_else(.x == \"promoted\", 1, 0)) |> \n    # aggregate into a table and then convert the table to tibble\n    table() |> \n    tibble::as_tibble() |> \n    # plot the confusion matrix\n    cvms::plot_confusion_matrix(\n        target_col     = \"promoted\",\n        prediction_col = \".pred_class\",\n        counts_col     = \"n\",\n        palette        = \"Greens\",\n        add_normalized = FALSE # this removes the normalised count % from the middle of each square - easier to reading the interpretation\n    )\n    \n\n```\n\n<br>\n\n# Save the model\n\n```{r save_model}\n\n# save the model for future use \nmodel_bundle <- bundle::bundle(final_model_workflow)\nreadr::write_rds(model_bundle, file = \"model_bundle.rds\")\n\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc-depth":3,"toc":true,"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.313","editor":"visual","theme":["lux","../../theme.scss"],"title-block-banner":true,"title-block-banner-color":"white","license":"CC BY","toc-title":"Table of contents","toc-location":"left","citation":true,"title":"Predicting Promotions Through Machine Learning","date":"2022-12-30","description":"Building an XGBoost model in the Tidymodels ecosystem that predicts whether an employee should be promoted.","author":[{"name":"Adam D McKinnon"}],"categories":["Tidymodels","XGBoost","R","Machine Learning","Employee Promotions"],"image":"markus-spiske-QozzJpFZ2lg-unsplash.jpg"},"extensions":{"book":{"multiFile":true}}}}}